{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import imageio\n",
    "import skimage\n",
    "import scipy # \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from IPython.display import Image\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import smtplib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset_name, img_res=(128, 128)):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.img_res = img_res\n",
    "\n",
    "    def load_data(self, batch_size=1, is_testing=False):\n",
    "        data_type = \"train\" if not is_testing else \"test\"\n",
    "        path = glob('%s/%s/*' % (self.dataset_name, data_type))\n",
    "        \n",
    "        batch_images = np.random.choice(path, size=batch_size)\n",
    "\n",
    "        imgs_A = []\n",
    "        imgs_B = []\n",
    "        for img_path in batch_images:\n",
    "            img = self.imread(img_path)\n",
    "\n",
    "            h, w, _ = img.shape\n",
    "            _w = int(w/2)\n",
    "            img_A, img_B = img[:, :_w, :], img[:, _w:, :]\n",
    "\n",
    "            img_A = scipy.misc.imresize(img_A, self.img_res)\n",
    "            img_B = scipy.misc.imresize(img_B, self.img_res)\n",
    "\n",
    "            # If training => do random flip\n",
    "            if not is_testing and np.random.random() < 0.5:\n",
    "                img_A = np.fliplr(img_A)\n",
    "                img_B = np.fliplr(img_B)\n",
    "\n",
    "            imgs_A.append(img_A)\n",
    "            imgs_B.append(img_B)\n",
    "\n",
    "        imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "        imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "        return imgs_A, imgs_B\n",
    "\n",
    "    def load_batch(self, batch_size=1, is_testing=False):\n",
    "        data_type = \"train\" if not is_testing else \"val\"\n",
    "        path = glob('%s/%s/*' % (self.dataset_name, data_type))\n",
    "\n",
    "        self.n_batches = int(len(path) / batch_size)\n",
    "\n",
    "        for i in range(self.n_batches-1):\n",
    "            batch = path[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img in batch:\n",
    "                img = self.imread(img)\n",
    "                h, w, _ = img.shape\n",
    "                half_w = int(w/2)\n",
    "                img_A = img[:, :half_w, :]\n",
    "                img_B = img[:, half_w:, :]\n",
    "\n",
    "                img_A = scipy.misc.imresize(img_A, self.img_res)\n",
    "                img_B = scipy.misc.imresize(img_B, self.img_res)\n",
    "\n",
    "                if not is_testing and np.random.random() > 0.5:\n",
    "                        img_A = np.fliplr(img_A)\n",
    "                        img_B = np.fliplr(img_B)\n",
    "\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "\n",
    "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "            yield imgs_A, imgs_B\n",
    "\n",
    "\n",
    "    def imread(self, path):\n",
    "        return imageio.imread(path).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2Pix():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'dataset'\n",
    "        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
    "                                      img_res=(self.img_rows, self.img_cols))\n",
    "\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(0.00002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        img_A = tf.keras.layers.Input(shape=self.img_shape)\n",
    "        img_B = tf.keras.layers.Input(shape=self.img_shape)\n",
    "\n",
    "        # By conditioning on B generate a fake version of A\n",
    "        fake_A = self.generator(img_B)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        valid = self.discriminator([fake_A, img_B])\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "\n",
    "        self.combined = tf.keras.models.Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "        self.combined.compile(loss=['mse', 'mae'],\n",
    "                              loss_weights=[1, 100],\n",
    "                              optimizer=optimizer)\n",
    "        \n",
    "        #-------------------------\n",
    "        #     TensorBoard Log\n",
    "        #-------------------------\n",
    "        \n",
    "        log_path = 'logs'\n",
    "        self.callback = TensorBoard(log_path)\n",
    "        self.callback.set_model(self.combined)\n",
    "        self.train_names = ['train_loss', 'train_mae']\n",
    "        self.val_names = ['val_loss', 'val_mae']\n",
    "    \n",
    "    def write_log(self, callback, names, logs, batch_no):\n",
    "        for name, value in zip(names, logs):\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value\n",
    "            summary_value.tag = name\n",
    "            callback.writer.add_summary(summary, batch_no)\n",
    "            callback.writer.flush()\n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = tf.keras.layers.BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = tf.keras.layers.UpSampling2D(size=2)(layer_input)\n",
    "            u = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = tf.keras.layers.Dropout(dropout_rate)(u)\n",
    "            u = tf.keras.layers.BatchNormalization(momentum=0.8)(u)\n",
    "            u = tf.keras.layers.Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = tf.keras.layers.Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf, bn=False)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        d5 = conv2d(d4, self.gf*8)\n",
    "        d6 = conv2d(d5, self.gf*8)\n",
    "        d7 = conv2d(d6, self.gf*8)\n",
    "        print(d7.shape)\n",
    "        print(d6.shape)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, self.gf*8)\n",
    "        u2 = deconv2d(u1, d5, self.gf*8)\n",
    "        u3 = deconv2d(u2, d4, self.gf*8)\n",
    "        u4 = deconv2d(u3, d3, self.gf*4)\n",
    "        u5 = deconv2d(u4, d2, self.gf*2)\n",
    "        u6 = deconv2d(u5, d1, self.gf)\n",
    "\n",
    "        u7 = tf.keras.layers.UpSampling2D(size=2)(u6)\n",
    "        output_img = tf.keras.layers.Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "        return tf.keras.models.Model(d0, output_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = tf.keras.layers.BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = tf.keras.layers.Input(shape=self.img_shape)\n",
    "        img_B = tf.keras.layers.Input(shape=self.img_shape)\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = tf.keras.layers.Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "        d1 = d_layer(combined_imgs, self.df, bn=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "        #print(d4.shape)\n",
    "\n",
    "        validity = tf.keras.layers.Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return tf.keras.models.Model([img_A, img_B], validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "\n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                # Train the generators\n",
    "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                # Plot the progress\n",
    "                print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
    "                                                                        batch_i, self.data_loader.n_batches,\n",
    "                                                                        d_loss[0], 100*d_loss[1],\n",
    "                                                                        g_loss[0]/batch_size,\n",
    "                                                                        elapsed_time))\n",
    "                \n",
    "                self.write_log(self.callback, self.val_names, g_loss, batch_i)\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.sample_images(epoch, batch_i)\n",
    "\n",
    "    def sample_images(self, epoch, batch_i):\n",
    "        os.makedirs('./images/%s' % self.dataset_name, exist_ok=True)\n",
    "        r, c = 3, 3\n",
    "\n",
    "        imgs_A, imgs_B = self.data_loader.load_data(batch_size=3, is_testing=True)\n",
    "        fake_A = self.generator.predict(imgs_B)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Condition', 'Generated', 'Original']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[i])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"./images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 1, 512)\n",
      "(?, 2, 2, 512)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 64, 64, 64)   3136        input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)      (None, 64, 64, 64)   0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 32, 32, 128)  131200      leaky_re_lu_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)      (None, 32, 32, 128)  0           conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 32, 32, 128)  512         leaky_re_lu_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 16, 16, 256)  524544      batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)      (None, 16, 16, 256)  0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 16, 16, 256)  1024        leaky_re_lu_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 512)    2097664     batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)      (None, 8, 8, 512)    0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 8, 8, 512)    2048        leaky_re_lu_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 4, 4, 512)    4194816     batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)      (None, 4, 4, 512)    0           conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 4, 4, 512)    2048        leaky_re_lu_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 2, 2, 512)    4194816     batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)      (None, 2, 2, 512)    0           conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 2, 2, 512)    2048        leaky_re_lu_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 1, 1, 512)    4194816     batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)      (None, 1, 1, 512)    0           conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1, 1, 512)    2048        leaky_re_lu_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling2D) (None, 2, 2, 512)    0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 2, 2, 512)    4194816     up_sampling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 2, 2, 512)    2048        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 2, 2, 1024)   0           batch_normalization_v1_64[0][0]  \n",
      "                                                                 batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling2D) (None, 4, 4, 1024)   0           concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 4, 4, 512)    8389120     up_sampling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 4, 4, 512)    2048        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 4, 4, 1024)   0           batch_normalization_v1_65[0][0]  \n",
      "                                                                 batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling2D) (None, 8, 8, 1024)   0           concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 512)    8389120     up_sampling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 8, 8, 512)    2048        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 8, 8, 1024)   0           batch_normalization_v1_66[0][0]  \n",
      "                                                                 batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling2D) (None, 16, 16, 1024) 0           concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 16, 16, 256)  4194560     up_sampling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 16, 16, 256)  1024        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 512)  0           batch_normalization_v1_67[0][0]  \n",
      "                                                                 batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_15 (UpSampling2D) (None, 32, 32, 512)  0           concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 32, 32, 128)  1048704     up_sampling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 32, 32, 128)  512         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 32, 32, 256)  0           batch_normalization_v1_68[0][0]  \n",
      "                                                                 batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_16 (UpSampling2D) (None, 64, 64, 256)  0           concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 64, 64, 64)   262208      up_sampling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 64, 64, 64)   256         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 64, 64, 128)  0           batch_normalization_v1_69[0][0]  \n",
      "                                                                 leaky_re_lu_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_17 (UpSampling2D) (None, 128, 128, 128 0           concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 128, 128, 3)  6147        up_sampling2d_17[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 41,843,331\n",
      "Trainable params: 41,834,499\n",
      "Non-trainable params: 8,832\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan = Pix2Pix()\n",
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\amm65\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:53: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "D:\\Users\\amm65\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:54: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/10000] [Batch 0/36] [D loss: 0.060739, acc:  99%] [G loss: 8.163401] time: 0:00:02.337290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\amm65\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "D:\\Users\\amm65\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/10000] [Batch 1/36] [D loss: 0.041331, acc:  99%] [G loss: 7.342183] time: 0:00:05.151136\n",
      "[Epoch 0/10000] [Batch 2/36] [D loss: 0.058731, acc:  99%] [G loss: 6.190771] time: 0:00:08.361073\n",
      "[Epoch 0/10000] [Batch 3/36] [D loss: 0.085033, acc:  92%] [G loss: 7.058984] time: 0:00:11.011579\n",
      "[Epoch 0/10000] [Batch 4/36] [D loss: 0.055301, acc:  96%] [G loss: 6.244478] time: 0:00:13.460945\n",
      "[Epoch 0/10000] [Batch 5/36] [D loss: 0.048944, acc:  99%] [G loss: 6.487414] time: 0:00:16.004885\n",
      "[Epoch 0/10000] [Batch 6/36] [D loss: 0.040115, acc:  99%] [G loss: 6.698098] time: 0:00:18.488662\n",
      "[Epoch 0/10000] [Batch 7/36] [D loss: 0.035481, acc:  99%] [G loss: 7.343241] time: 0:00:20.947166\n",
      "[Epoch 0/10000] [Batch 8/36] [D loss: 0.048071, acc:  99%] [G loss: 6.089492] time: 0:00:23.408010\n",
      "[Epoch 0/10000] [Batch 9/36] [D loss: 0.050750, acc:  99%] [G loss: 6.378377] time: 0:00:25.907959\n",
      "[Epoch 0/10000] [Batch 10/36] [D loss: 0.033004, acc:  99%] [G loss: 7.010770] time: 0:00:28.355035\n",
      "[Epoch 0/10000] [Batch 11/36] [D loss: 0.051283, acc:  98%] [G loss: 6.954537] time: 0:00:30.829956\n",
      "[Epoch 0/10000] [Batch 12/36] [D loss: 0.083088, acc:  96%] [G loss: 6.478955] time: 0:00:33.266105\n",
      "[Epoch 0/10000] [Batch 13/36] [D loss: 0.026700, acc:  99%] [G loss: 7.883713] time: 0:00:35.728390\n",
      "[Epoch 0/10000] [Batch 14/36] [D loss: 0.035567, acc:  99%] [G loss: 6.399754] time: 0:00:38.135200\n",
      "[Epoch 0/10000] [Batch 15/36] [D loss: 0.034831, acc:  99%] [G loss: 6.612410] time: 0:00:40.566965\n",
      "[Epoch 0/10000] [Batch 16/36] [D loss: 0.084150, acc:  97%] [G loss: 6.814900] time: 0:00:42.974205\n",
      "[Epoch 0/10000] [Batch 17/36] [D loss: 0.042869, acc:  99%] [G loss: 6.579473] time: 0:00:45.347898\n",
      "[Epoch 0/10000] [Batch 18/36] [D loss: 0.031464, acc:  99%] [G loss: 6.920635] time: 0:00:47.785918\n",
      "[Epoch 0/10000] [Batch 19/36] [D loss: 0.021968, acc:  99%] [G loss: 7.397094] time: 0:00:50.305077\n",
      "[Epoch 0/10000] [Batch 20/36] [D loss: 0.060672, acc:  97%] [G loss: 6.316098] time: 0:00:52.847725\n",
      "[Epoch 0/10000] [Batch 21/36] [D loss: 0.067016, acc:  97%] [G loss: 6.119076] time: 0:00:55.344455\n",
      "[Epoch 0/10000] [Batch 22/36] [D loss: 0.022351, acc:  99%] [G loss: 7.209464] time: 0:00:57.812747\n",
      "[Epoch 0/10000] [Batch 23/36] [D loss: 0.049715, acc:  98%] [G loss: 6.989428] time: 0:01:00.311617\n",
      "[Epoch 0/10000] [Batch 24/36] [D loss: 0.051721, acc:  99%] [G loss: 7.271102] time: 0:01:02.749883\n",
      "[Epoch 0/10000] [Batch 25/36] [D loss: 0.045973, acc:  99%] [G loss: 7.043881] time: 0:01:05.218258\n",
      "[Epoch 0/10000] [Batch 26/36] [D loss: 0.067135, acc:  97%] [G loss: 6.052084] time: 0:01:07.646176\n",
      "[Epoch 0/10000] [Batch 27/36] [D loss: 0.072703, acc:  97%] [G loss: 6.136726] time: 0:01:10.037119\n",
      "[Epoch 0/10000] [Batch 28/36] [D loss: 0.059374, acc:  97%] [G loss: 6.651697] time: 0:01:12.433559\n",
      "[Epoch 0/10000] [Batch 29/36] [D loss: 0.034088, acc:  99%] [G loss: 6.760292] time: 0:01:14.919721\n",
      "[Epoch 0/10000] [Batch 30/36] [D loss: 0.057290, acc:  99%] [G loss: 6.218474] time: 0:01:17.338425\n",
      "[Epoch 0/10000] [Batch 31/36] [D loss: 0.070867, acc:  94%] [G loss: 6.726966] time: 0:01:19.754989\n",
      "[Epoch 0/10000] [Batch 32/36] [D loss: 0.037400, acc:  99%] [G loss: 6.393255] time: 0:01:22.172849\n",
      "[Epoch 0/10000] [Batch 33/36] [D loss: 0.057493, acc:  98%] [G loss: 6.039250] time: 0:01:24.854902\n",
      "[Epoch 0/10000] [Batch 34/36] [D loss: 0.035181, acc:  99%] [G loss: 7.855569] time: 0:01:27.333350\n",
      "[Epoch 1/10000] [Batch 0/36] [D loss: 0.035690, acc:  99%] [G loss: 6.632715] time: 0:01:30.486583\n",
      "[Epoch 1/10000] [Batch 1/36] [D loss: 0.048328, acc:  99%] [G loss: 7.079411] time: 0:01:33.246542\n",
      "[Epoch 1/10000] [Batch 2/36] [D loss: 0.063921, acc:  99%] [G loss: 5.499433] time: 0:01:35.724432\n",
      "[Epoch 1/10000] [Batch 3/36] [D loss: 0.095867, acc:  88%] [G loss: 7.033790] time: 0:01:38.741416\n",
      "[Epoch 1/10000] [Batch 4/36] [D loss: 0.050779, acc:  97%] [G loss: 5.965372] time: 0:01:41.174060\n",
      "[Epoch 1/10000] [Batch 5/36] [D loss: 0.055064, acc:  99%] [G loss: 6.612360] time: 0:01:43.668591\n",
      "[Epoch 1/10000] [Batch 6/36] [D loss: 0.067457, acc:  94%] [G loss: 6.313850] time: 0:01:46.100057\n",
      "[Epoch 1/10000] [Batch 7/36] [D loss: 0.058465, acc:  97%] [G loss: 7.007438] time: 0:01:48.635844\n",
      "[Epoch 1/10000] [Batch 8/36] [D loss: 0.050850, acc:  98%] [G loss: 6.020799] time: 0:01:51.074192\n",
      "[Epoch 1/10000] [Batch 9/36] [D loss: 0.084855, acc:  91%] [G loss: 5.948967] time: 0:01:53.526327\n",
      "[Epoch 1/10000] [Batch 10/36] [D loss: 0.023359, acc:  99%] [G loss: 6.776721] time: 0:01:55.930065\n",
      "[Epoch 1/10000] [Batch 11/36] [D loss: 0.056963, acc:  98%] [G loss: 6.512861] time: 0:01:58.439212\n",
      "[Epoch 1/10000] [Batch 12/36] [D loss: 0.061342, acc:  94%] [G loss: 6.275041] time: 0:02:00.938714\n",
      "[Epoch 1/10000] [Batch 13/36] [D loss: 0.022314, acc:  99%] [G loss: 7.837869] time: 0:02:03.416842\n",
      "[Epoch 1/10000] [Batch 14/36] [D loss: 0.041902, acc:  99%] [G loss: 6.553185] time: 0:02:05.824291\n",
      "[Epoch 1/10000] [Batch 15/36] [D loss: 0.053216, acc:  98%] [G loss: 6.576719] time: 0:02:08.237249\n",
      "[Epoch 1/10000] [Batch 16/36] [D loss: 0.047452, acc:  99%] [G loss: 6.925374] time: 0:02:10.624286\n",
      "[Epoch 1/10000] [Batch 17/36] [D loss: 0.034314, acc:  99%] [G loss: 6.497047] time: 0:02:13.091983\n",
      "[Epoch 1/10000] [Batch 18/36] [D loss: 0.029551, acc:  99%] [G loss: 7.031912] time: 0:02:15.526066\n",
      "[Epoch 1/10000] [Batch 19/36] [D loss: 0.022383, acc:  99%] [G loss: 7.970587] time: 0:02:17.913259\n",
      "[Epoch 1/10000] [Batch 20/36] [D loss: 0.047159, acc:  99%] [G loss: 6.359595] time: 0:02:20.381776\n",
      "[Epoch 1/10000] [Batch 21/36] [D loss: 0.046126, acc:  99%] [G loss: 6.396938] time: 0:02:22.840280\n",
      "[Epoch 1/10000] [Batch 22/36] [D loss: 0.019038, acc:  99%] [G loss: 7.288292] time: 0:02:25.308695\n",
      "[Epoch 1/10000] [Batch 23/36] [D loss: 0.028736, acc:  99%] [G loss: 6.608392] time: 0:02:27.807933\n",
      "[Epoch 1/10000] [Batch 24/36] [D loss: 0.053760, acc:  98%] [G loss: 7.367122] time: 0:02:30.240702\n",
      "[Epoch 1/10000] [Batch 25/36] [D loss: 0.051601, acc:  99%] [G loss: 7.198235] time: 0:02:32.694026\n",
      "[Epoch 1/10000] [Batch 26/36] [D loss: 0.116666, acc:  88%] [G loss: 6.213237] time: 0:02:35.145852\n",
      "[Epoch 1/10000] [Batch 27/36] [D loss: 0.083146, acc:  93%] [G loss: 6.049496] time: 0:02:37.641153\n",
      "[Epoch 1/10000] [Batch 28/36] [D loss: 0.044172, acc:  98%] [G loss: 6.560799] time: 0:02:40.145439\n",
      "[Epoch 1/10000] [Batch 29/36] [D loss: 0.026265, acc:  99%] [G loss: 6.676219] time: 0:02:42.550881\n",
      "[Epoch 1/10000] [Batch 30/36] [D loss: 0.024266, acc:  99%] [G loss: 6.483543] time: 0:02:44.945187\n",
      "[Epoch 1/10000] [Batch 31/36] [D loss: 0.061808, acc:  95%] [G loss: 6.851314] time: 0:02:47.356594\n",
      "[Epoch 1/10000] [Batch 32/36] [D loss: 0.048021, acc:  98%] [G loss: 6.432376] time: 0:02:49.861900\n",
      "[Epoch 1/10000] [Batch 33/36] [D loss: 0.079539, acc:  97%] [G loss: 6.133034] time: 0:02:52.605145\n",
      "[Epoch 1/10000] [Batch 34/36] [D loss: 0.023154, acc:  99%] [G loss: 7.592367] time: 0:02:55.737343\n",
      "[Epoch 2/10000] [Batch 0/36] [D loss: 0.029538, acc: 100%] [G loss: 6.318460] time: 0:02:58.205093\n",
      "[Epoch 2/10000] [Batch 1/36] [D loss: 0.039883, acc:  99%] [G loss: 6.745565] time: 0:03:01.088269\n",
      "[Epoch 2/10000] [Batch 2/36] [D loss: 0.046870, acc:  99%] [G loss: 5.375753] time: 0:03:03.976207\n",
      "[Epoch 2/10000] [Batch 3/36] [D loss: 0.061249, acc:  96%] [G loss: 6.191702] time: 0:03:06.466284\n",
      "[Epoch 2/10000] [Batch 4/36] [D loss: 0.037715, acc:  99%] [G loss: 5.789842] time: 0:03:09.032720\n",
      "[Epoch 2/10000] [Batch 5/36] [D loss: 0.057495, acc:  98%] [G loss: 6.178289] time: 0:03:11.621152\n",
      "[Epoch 2/10000] [Batch 6/36] [D loss: 0.041580, acc:  98%] [G loss: 6.590900] time: 0:03:14.091674\n",
      "[Epoch 2/10000] [Batch 7/36] [D loss: 0.031190, acc:  99%] [G loss: 6.880641] time: 0:03:16.648480\n",
      "[Epoch 2/10000] [Batch 8/36] [D loss: 0.036586, acc:  99%] [G loss: 5.831201] time: 0:03:19.015697\n",
      "[Epoch 2/10000] [Batch 9/36] [D loss: 0.070543, acc:  95%] [G loss: 5.850593] time: 0:03:21.412904\n",
      "[Epoch 2/10000] [Batch 10/36] [D loss: 0.020990, acc:  99%] [G loss: 6.785881] time: 0:03:23.795015\n",
      "[Epoch 2/10000] [Batch 11/36] [D loss: 0.045244, acc:  99%] [G loss: 6.259964] time: 0:03:26.333582\n",
      "[Epoch 2/10000] [Batch 12/36] [D loss: 0.051490, acc:  98%] [G loss: 6.209305] time: 0:03:28.778605\n",
      "[Epoch 2/10000] [Batch 13/36] [D loss: 0.029050, acc:  99%] [G loss: 7.950414] time: 0:03:31.307427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/10000] [Batch 14/36] [D loss: 0.035042, acc:  99%] [G loss: 6.253051] time: 0:03:33.848896\n",
      "[Epoch 2/10000] [Batch 15/36] [D loss: 0.034731, acc:  99%] [G loss: 6.540001] time: 0:03:36.329642\n",
      "[Epoch 2/10000] [Batch 16/36] [D loss: 0.111741, acc:  91%] [G loss: 6.654017] time: 0:03:38.835039\n",
      "[Epoch 2/10000] [Batch 17/36] [D loss: 0.046341, acc:  99%] [G loss: 6.297898] time: 0:03:41.303443\n",
      "[Epoch 2/10000] [Batch 18/36] [D loss: 0.035787, acc:  99%] [G loss: 6.576979] time: 0:03:43.822834\n",
      "[Epoch 2/10000] [Batch 19/36] [D loss: 0.020724, acc:  99%] [G loss: 7.497158] time: 0:03:46.291100\n",
      "[Epoch 2/10000] [Batch 20/36] [D loss: 0.051787, acc:  98%] [G loss: 5.943864] time: 0:03:48.694744\n",
      "[Epoch 2/10000] [Batch 21/36] [D loss: 0.066568, acc:  97%] [G loss: 6.255686] time: 0:03:50.986039\n",
      "[Epoch 2/10000] [Batch 22/36] [D loss: 0.028330, acc:  99%] [G loss: 6.704976] time: 0:03:53.277009\n",
      "[Epoch 2/10000] [Batch 23/36] [D loss: 0.037506, acc:  99%] [G loss: 6.644901] time: 0:03:55.561116\n",
      "[Epoch 2/10000] [Batch 24/36] [D loss: 0.038495, acc:  99%] [G loss: 7.331054] time: 0:03:57.822511\n",
      "[Epoch 2/10000] [Batch 25/36] [D loss: 0.046272, acc:  99%] [G loss: 7.165963] time: 0:04:00.116980\n",
      "[Epoch 2/10000] [Batch 26/36] [D loss: 0.059990, acc:  98%] [G loss: 6.003572] time: 0:04:02.430604\n",
      "[Epoch 2/10000] [Batch 27/36] [D loss: 0.053418, acc:  98%] [G loss: 6.377291] time: 0:04:04.685741\n",
      "[Epoch 2/10000] [Batch 28/36] [D loss: 0.050028, acc:  98%] [G loss: 6.752590] time: 0:04:06.994595\n",
      "[Epoch 2/10000] [Batch 29/36] [D loss: 0.027836, acc:  99%] [G loss: 6.415081] time: 0:04:09.521222\n",
      "[Epoch 2/10000] [Batch 30/36] [D loss: 0.024368, acc:  99%] [G loss: 6.165312] time: 0:04:12.042076\n",
      "[Epoch 2/10000] [Batch 31/36] [D loss: 0.051113, acc:  99%] [G loss: 6.781218] time: 0:04:14.551222\n",
      "[Epoch 2/10000] [Batch 32/36] [D loss: 0.030478, acc:  99%] [G loss: 6.073390] time: 0:04:17.155765\n",
      "[Epoch 2/10000] [Batch 33/36] [D loss: 0.056877, acc:  99%] [G loss: 6.340088] time: 0:04:19.908620\n",
      "[Epoch 2/10000] [Batch 34/36] [D loss: 0.036812, acc:  99%] [G loss: 7.782394] time: 0:04:22.910271\n",
      "[Epoch 3/10000] [Batch 0/36] [D loss: 0.040353, acc: 100%] [G loss: 6.333354] time: 0:04:25.409439\n",
      "[Epoch 3/10000] [Batch 1/36] [D loss: 0.053610, acc:  97%] [G loss: 6.698440] time: 0:04:28.101470\n",
      "[Epoch 3/10000] [Batch 2/36] [D loss: 0.048599, acc:  99%] [G loss: 5.397779] time: 0:04:30.545162\n",
      "[Epoch 3/10000] [Batch 3/36] [D loss: 0.085806, acc:  92%] [G loss: 6.159638] time: 0:04:33.270972\n",
      "[Epoch 3/10000] [Batch 4/36] [D loss: 0.038137, acc:  98%] [G loss: 5.816193] time: 0:04:35.810368\n",
      "[Epoch 3/10000] [Batch 5/36] [D loss: 0.041201, acc:  99%] [G loss: 6.203506] time: 0:04:38.254882\n",
      "[Epoch 3/10000] [Batch 6/36] [D loss: 0.059545, acc:  95%] [G loss: 6.284569] time: 0:04:40.759147\n",
      "[Epoch 3/10000] [Batch 7/36] [D loss: 0.041073, acc:  99%] [G loss: 7.074966] time: 0:04:43.304020\n",
      "[Epoch 3/10000] [Batch 8/36] [D loss: 0.054866, acc:  98%] [G loss: 5.837141] time: 0:04:45.762467\n",
      "[Epoch 3/10000] [Batch 9/36] [D loss: 0.038927, acc:  99%] [G loss: 5.988300] time: 0:04:48.200068\n",
      "[Epoch 3/10000] [Batch 10/36] [D loss: 0.022632, acc:  99%] [G loss: 6.522817] time: 0:04:50.631205\n",
      "[Epoch 3/10000] [Batch 11/36] [D loss: 0.039971, acc:  99%] [G loss: 6.405733] time: 0:04:53.188003\n",
      "[Epoch 3/10000] [Batch 12/36] [D loss: 0.077825, acc:  97%] [G loss: 6.125858] time: 0:04:55.630040\n",
      "[Epoch 3/10000] [Batch 13/36] [D loss: 0.031722, acc:  99%] [G loss: 7.571598] time: 0:04:58.043988\n",
      "[Epoch 3/10000] [Batch 14/36] [D loss: 0.078555, acc:  95%] [G loss: 6.434689] time: 0:05:00.441430\n",
      "[Epoch 3/10000] [Batch 15/36] [D loss: 0.063438, acc:  96%] [G loss: 6.313330] time: 0:05:02.940489\n",
      "[Epoch 3/10000] [Batch 16/36] [D loss: 0.060059, acc:  95%] [G loss: 6.814457] time: 0:05:05.388415\n",
      "[Epoch 3/10000] [Batch 17/36] [D loss: 0.070022, acc:  99%] [G loss: 6.259964] time: 0:05:07.898066\n",
      "[Epoch 3/10000] [Batch 18/36] [D loss: 0.033127, acc:  99%] [G loss: 6.735139] time: 0:05:10.366641\n",
      "[Epoch 3/10000] [Batch 19/36] [D loss: 0.038791, acc:  98%] [G loss: 7.296629] time: 0:05:12.855521\n",
      "[Epoch 3/10000] [Batch 20/36] [D loss: 0.059635, acc:  97%] [G loss: 5.877345] time: 0:05:15.323589\n",
      "[Epoch 3/10000] [Batch 21/36] [D loss: 0.084849, acc:  93%] [G loss: 5.818314] time: 0:05:17.731596\n",
      "[Epoch 3/10000] [Batch 22/36] [D loss: 0.045091, acc:  98%] [G loss: 6.347989] time: 0:05:20.159518\n",
      "[Epoch 3/10000] [Batch 23/36] [D loss: 0.047603, acc:  98%] [G loss: 6.418525] time: 0:05:22.617580\n",
      "[Epoch 3/10000] [Batch 24/36] [D loss: 0.042541, acc:  99%] [G loss: 7.247113] time: 0:05:25.116808\n",
      "[Epoch 3/10000] [Batch 25/36] [D loss: 0.027186, acc:  99%] [G loss: 7.296665] time: 0:05:27.574886\n",
      "[Epoch 3/10000] [Batch 26/36] [D loss: 0.118849, acc:  91%] [G loss: 5.862789] time: 0:05:30.023393\n",
      "[Epoch 3/10000] [Batch 27/36] [D loss: 0.070023, acc:  96%] [G loss: 6.245332] time: 0:05:32.481904\n",
      "[Epoch 3/10000] [Batch 28/36] [D loss: 0.051993, acc:  98%] [G loss: 6.449387] time: 0:05:34.909455\n",
      "[Epoch 3/10000] [Batch 29/36] [D loss: 0.018388, acc:  99%] [G loss: 6.695667] time: 0:05:37.281683\n",
      "[Epoch 3/10000] [Batch 30/36] [D loss: 0.028387, acc:  99%] [G loss: 6.175115] time: 0:05:39.714688\n",
      "[Epoch 3/10000] [Batch 31/36] [D loss: 0.068994, acc:  95%] [G loss: 6.474466] time: 0:05:42.126955\n",
      "[Epoch 3/10000] [Batch 32/36] [D loss: 0.032781, acc:  99%] [G loss: 6.232300] time: 0:05:44.580520\n",
      "[Epoch 3/10000] [Batch 33/36] [D loss: 0.055447, acc:  99%] [G loss: 6.194641] time: 0:05:47.343630\n",
      "[Epoch 3/10000] [Batch 34/36] [D loss: 0.018205, acc:  99%] [G loss: 7.422801] time: 0:05:49.794729\n",
      "[Epoch 4/10000] [Batch 0/36] [D loss: 0.032675, acc:  99%] [G loss: 6.137574] time: 0:05:52.230221\n",
      "[Epoch 4/10000] [Batch 1/36] [D loss: 0.028383, acc:  99%] [G loss: 7.167775] time: 0:05:54.957494\n",
      "[Epoch 4/10000] [Batch 2/36] [D loss: 0.041995, acc:  99%] [G loss: 5.874382] time: 0:05:57.421402\n",
      "[Epoch 4/10000] [Batch 3/36] [D loss: 0.067194, acc:  97%] [G loss: 6.329376] time: 0:05:59.769910\n",
      "[Epoch 4/10000] [Batch 4/36] [D loss: 0.046571, acc:  97%] [G loss: 6.113854] time: 0:06:02.212500\n",
      "[Epoch 4/10000] [Batch 5/36] [D loss: 0.071787, acc:  97%] [G loss: 6.087754] time: 0:06:05.477059\n",
      "[Epoch 4/10000] [Batch 6/36] [D loss: 0.072541, acc:  93%] [G loss: 6.329646] time: 0:06:07.916712\n",
      "[Epoch 4/10000] [Batch 7/36] [D loss: 0.067182, acc:  97%] [G loss: 6.625891] time: 0:06:10.675548\n",
      "[Epoch 4/10000] [Batch 8/36] [D loss: 0.058255, acc:  98%] [G loss: 5.696691] time: 0:06:13.150813\n",
      "[Epoch 4/10000] [Batch 9/36] [D loss: 0.050413, acc:  98%] [G loss: 5.950283] time: 0:06:15.790171\n",
      "[Epoch 4/10000] [Batch 10/36] [D loss: 0.023276, acc:  99%] [G loss: 6.486128] time: 0:06:18.302943\n",
      "[Epoch 4/10000] [Batch 11/36] [D loss: 0.047268, acc:  98%] [G loss: 6.647604] time: 0:06:20.801479\n",
      "[Epoch 4/10000] [Batch 12/36] [D loss: 0.075070, acc:  97%] [G loss: 5.932874] time: 0:06:23.226384\n",
      "[Epoch 4/10000] [Batch 13/36] [D loss: 0.031952, acc:  99%] [G loss: 7.257125] time: 0:06:25.694806\n",
      "[Epoch 4/10000] [Batch 14/36] [D loss: 0.063264, acc:  97%] [G loss: 6.162864] time: 0:06:28.163522\n",
      "[Epoch 4/10000] [Batch 15/36] [D loss: 0.062222, acc:  95%] [G loss: 6.123935] time: 0:06:30.631972\n",
      "[Epoch 4/10000] [Batch 16/36] [D loss: 0.088278, acc:  93%] [G loss: 6.926361] time: 0:06:33.103844\n",
      "[Epoch 4/10000] [Batch 17/36] [D loss: 0.060252, acc:  99%] [G loss: 6.238757] time: 0:06:35.569030\n",
      "[Epoch 4/10000] [Batch 18/36] [D loss: 0.035471, acc:  99%] [G loss: 6.500380] time: 0:06:38.017242\n",
      "[Epoch 4/10000] [Batch 19/36] [D loss: 0.032942, acc:  98%] [G loss: 7.601927] time: 0:06:40.455385\n",
      "[Epoch 4/10000] [Batch 20/36] [D loss: 0.042813, acc:  99%] [G loss: 5.952023] time: 0:06:42.882998\n",
      "[Epoch 4/10000] [Batch 21/36] [D loss: 0.092730, acc:  89%] [G loss: 5.750651] time: 0:06:45.294213\n",
      "[Epoch 4/10000] [Batch 22/36] [D loss: 0.061742, acc:  96%] [G loss: 6.585584] time: 0:06:47.739128\n",
      "[Epoch 4/10000] [Batch 23/36] [D loss: 0.046125, acc:  98%] [G loss: 6.726335] time: 0:06:50.217716\n",
      "[Epoch 4/10000] [Batch 24/36] [D loss: 0.053299, acc:  99%] [G loss: 6.814373] time: 0:06:52.772665\n",
      "[Epoch 4/10000] [Batch 25/36] [D loss: 0.036463, acc: 100%] [G loss: 7.030573] time: 0:06:55.534848\n",
      "[Epoch 4/10000] [Batch 26/36] [D loss: 0.075697, acc:  97%] [G loss: 5.853140] time: 0:06:57.857300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/10000] [Batch 27/36] [D loss: 0.064003, acc:  98%] [G loss: 6.046564] time: 0:07:00.163314\n",
      "[Epoch 4/10000] [Batch 28/36] [D loss: 0.054196, acc:  97%] [G loss: 6.804151] time: 0:07:02.518903\n",
      "[Epoch 4/10000] [Batch 29/36] [D loss: 0.020709, acc:  99%] [G loss: 6.503419] time: 0:07:05.171680\n",
      "[Epoch 4/10000] [Batch 30/36] [D loss: 0.024857, acc:  99%] [G loss: 5.761157] time: 0:07:07.794523\n",
      "[Epoch 4/10000] [Batch 31/36] [D loss: 0.058364, acc:  97%] [G loss: 6.856254] time: 0:07:10.364565\n",
      "[Epoch 4/10000] [Batch 32/36] [D loss: 0.031602, acc:  99%] [G loss: 6.494107] time: 0:07:12.844222\n",
      "[Epoch 4/10000] [Batch 33/36] [D loss: 0.049883, acc:  99%] [G loss: 6.249963] time: 0:07:15.586111\n",
      "[Epoch 4/10000] [Batch 34/36] [D loss: 0.032161, acc:  99%] [G loss: 7.812950] time: 0:07:18.085054\n",
      "[Epoch 5/10000] [Batch 0/36] [D loss: 0.045897, acc:  99%] [G loss: 6.235411] time: 0:07:20.972462\n",
      "[Epoch 5/10000] [Batch 1/36] [D loss: 0.027132, acc:  99%] [G loss: 6.756101] time: 0:07:23.743598\n",
      "[Epoch 5/10000] [Batch 2/36] [D loss: 0.067526, acc:  96%] [G loss: 5.631074] time: 0:07:26.150999\n",
      "[Epoch 5/10000] [Batch 3/36] [D loss: 0.088098, acc:  91%] [G loss: 7.326560] time: 0:07:28.721395\n",
      "[Epoch 5/10000] [Batch 4/36] [D loss: 0.062189, acc:  95%] [G loss: 6.073421] time: 0:07:31.139097\n",
      "[Epoch 5/10000] [Batch 5/36] [D loss: 0.058735, acc:  98%] [G loss: 6.260242] time: 0:07:33.567056\n",
      "[Epoch 5/10000] [Batch 6/36] [D loss: 0.062031, acc:  95%] [G loss: 6.659115] time: 0:07:36.055937\n",
      "[Epoch 5/10000] [Batch 7/36] [D loss: 0.041750, acc:  99%] [G loss: 6.784703] time: 0:07:38.534514\n",
      "[Epoch 5/10000] [Batch 8/36] [D loss: 0.062464, acc:  97%] [G loss: 5.987636] time: 0:07:41.520997\n",
      "[Epoch 5/10000] [Batch 9/36] [D loss: 0.040341, acc:  99%] [G loss: 6.028927] time: 0:07:43.969233\n",
      "[Epoch 5/10000] [Batch 10/36] [D loss: 0.022279, acc:  99%] [G loss: 6.379245] time: 0:07:46.437708\n",
      "[Epoch 5/10000] [Batch 11/36] [D loss: 0.040634, acc:  99%] [G loss: 6.374368] time: 0:07:48.997957\n",
      "[Epoch 5/10000] [Batch 12/36] [D loss: 0.050573, acc:  99%] [G loss: 6.079248] time: 0:07:51.496859\n",
      "[Epoch 5/10000] [Batch 13/36] [D loss: 0.024214, acc:  99%] [G loss: 7.937113] time: 0:07:53.917930\n",
      "[Epoch 5/10000] [Batch 14/36] [D loss: 0.036129, acc:  99%] [G loss: 6.016966] time: 0:07:56.397621\n",
      "[Epoch 5/10000] [Batch 15/36] [D loss: 0.051067, acc:  98%] [G loss: 6.421518] time: 0:07:58.778737\n",
      "[Epoch 5/10000] [Batch 16/36] [D loss: 0.111719, acc:  91%] [G loss: 6.623877] time: 0:08:01.253278\n",
      "[Epoch 5/10000] [Batch 17/36] [D loss: 0.047225, acc:  99%] [G loss: 6.098423] time: 0:08:03.885007\n",
      "[Epoch 5/10000] [Batch 18/36] [D loss: 0.039028, acc:  99%] [G loss: 6.982758] time: 0:08:06.441752\n",
      "[Epoch 5/10000] [Batch 19/36] [D loss: 0.023044, acc:  99%] [G loss: 7.483027] time: 0:08:09.005258\n",
      "[Epoch 5/10000] [Batch 20/36] [D loss: 0.044719, acc:  99%] [G loss: 6.378644] time: 0:08:11.439894\n",
      "[Epoch 5/10000] [Batch 21/36] [D loss: 0.058528, acc:  98%] [G loss: 6.092153] time: 0:08:13.898060\n",
      "[Epoch 5/10000] [Batch 22/36] [D loss: 0.027592, acc:  99%] [G loss: 6.488531] time: 0:08:16.326138\n",
      "[Epoch 5/10000] [Batch 23/36] [D loss: 0.038821, acc:  99%] [G loss: 6.882429] time: 0:08:18.738820\n",
      "[Epoch 5/10000] [Batch 24/36] [D loss: 0.043911, acc:  99%] [G loss: 7.440382] time: 0:08:21.202270\n",
      "[Epoch 5/10000] [Batch 25/36] [D loss: 0.057926, acc:  99%] [G loss: 6.803248] time: 0:08:23.620162\n",
      "[Epoch 5/10000] [Batch 26/36] [D loss: 0.087135, acc:  93%] [G loss: 6.254067] time: 0:08:26.071351\n",
      "[Epoch 5/10000] [Batch 27/36] [D loss: 0.066381, acc:  97%] [G loss: 5.968873] time: 0:08:28.581140\n",
      "[Epoch 5/10000] [Batch 28/36] [D loss: 0.041505, acc:  99%] [G loss: 6.617625] time: 0:08:30.986632\n",
      "[Epoch 5/10000] [Batch 29/36] [D loss: 0.030038, acc:  99%] [G loss: 6.793085] time: 0:08:33.483962\n",
      "[Epoch 5/10000] [Batch 30/36] [D loss: 0.025892, acc:  99%] [G loss: 5.910614] time: 0:08:35.912027\n",
      "[Epoch 5/10000] [Batch 31/36] [D loss: 0.072777, acc:  97%] [G loss: 6.525555] time: 0:08:38.359931\n",
      "[Epoch 5/10000] [Batch 32/36] [D loss: 0.033965, acc:  99%] [G loss: 6.408647] time: 0:08:40.827238\n",
      "[Epoch 5/10000] [Batch 33/36] [D loss: 0.068314, acc:  98%] [G loss: 6.136395] time: 0:08:43.571522\n",
      "[Epoch 5/10000] [Batch 34/36] [D loss: 0.020203, acc:  99%] [G loss: 7.245187] time: 0:08:46.060486\n",
      "[Epoch 6/10000] [Batch 0/36] [D loss: 0.047685, acc: 100%] [G loss: 6.501649] time: 0:08:48.624273\n",
      "[Epoch 6/10000] [Batch 1/36] [D loss: 0.028115, acc:  99%] [G loss: 6.573205] time: 0:08:51.363335\n",
      "[Epoch 6/10000] [Batch 2/36] [D loss: 0.032914, acc:  99%] [G loss: 5.479153] time: 0:08:53.770948\n",
      "[Epoch 6/10000] [Batch 3/36] [D loss: 0.044950, acc:  99%] [G loss: 5.958343] time: 0:08:56.249276\n",
      "[Epoch 6/10000] [Batch 4/36] [D loss: 0.046523, acc:  97%] [G loss: 5.853257] time: 0:08:58.678102\n",
      "[Epoch 6/10000] [Batch 5/36] [D loss: 0.044988, acc:  99%] [G loss: 6.050207] time: 0:09:01.105102\n",
      "[Epoch 6/10000] [Batch 6/36] [D loss: 0.045337, acc:  99%] [G loss: 6.311404] time: 0:09:03.613233\n",
      "[Epoch 6/10000] [Batch 7/36] [D loss: 0.034632, acc:  99%] [G loss: 6.909910] time: 0:09:06.096670\n",
      "[Epoch 6/10000] [Batch 8/36] [D loss: 0.068670, acc:  96%] [G loss: 6.023267] time: 0:09:08.535103\n",
      "[Epoch 6/10000] [Batch 9/36] [D loss: 0.052785, acc:  97%] [G loss: 6.025901] time: 0:09:10.983560\n",
      "[Epoch 6/10000] [Batch 10/36] [D loss: 0.026451, acc:  99%] [G loss: 6.559176] time: 0:09:13.493729\n",
      "[Epoch 6/10000] [Batch 11/36] [D loss: 0.045307, acc:  99%] [G loss: 6.451443] time: 0:09:15.896484\n",
      "[Epoch 6/10000] [Batch 12/36] [D loss: 0.049623, acc:  99%] [G loss: 5.896470] time: 0:09:18.354498\n",
      "[Epoch 6/10000] [Batch 13/36] [D loss: 0.017662, acc:  99%] [G loss: 7.458918] time: 0:09:20.847160\n",
      "[Epoch 6/10000] [Batch 14/36] [D loss: 0.035199, acc:  99%] [G loss: 6.266048] time: 0:09:23.261394\n",
      "[Epoch 6/10000] [Batch 15/36] [D loss: 0.045097, acc:  99%] [G loss: 6.309947] time: 0:09:25.844588\n",
      "[Epoch 6/10000] [Batch 16/36] [D loss: 0.109129, acc:  91%] [G loss: 6.521142] time: 0:09:28.553842\n",
      "[Epoch 6/10000] [Batch 17/36] [D loss: 0.028951, acc:  99%] [G loss: 6.120157] time: 0:09:31.139263\n",
      "[Epoch 6/10000] [Batch 18/36] [D loss: 0.028618, acc:  99%] [G loss: 6.525874] time: 0:09:33.673952\n",
      "[Epoch 6/10000] [Batch 19/36] [D loss: 0.034063, acc:  99%] [G loss: 7.456960] time: 0:09:36.184090\n",
      "[Epoch 6/10000] [Batch 20/36] [D loss: 0.046710, acc:  99%] [G loss: 5.906110] time: 0:09:38.741849\n",
      "[Epoch 6/10000] [Batch 21/36] [D loss: 0.086334, acc:  93%] [G loss: 6.009001] time: 0:09:41.380760\n",
      "[Epoch 6/10000] [Batch 22/36] [D loss: 0.052730, acc:  98%] [G loss: 6.587553] time: 0:09:44.084542\n",
      "[Epoch 6/10000] [Batch 23/36] [D loss: 0.034899, acc:  99%] [G loss: 6.475126] time: 0:09:46.695327\n",
      "[Epoch 6/10000] [Batch 24/36] [D loss: 0.059081, acc:  98%] [G loss: 7.060821] time: 0:09:49.147394\n",
      "[Epoch 6/10000] [Batch 25/36] [D loss: 0.053604, acc: 100%] [G loss: 6.958033] time: 0:09:51.500563\n",
      "[Epoch 6/10000] [Batch 26/36] [D loss: 0.081876, acc:  93%] [G loss: 6.173810] time: 0:09:53.994291\n",
      "[Epoch 6/10000] [Batch 27/36] [D loss: 0.074057, acc:  97%] [G loss: 6.028364] time: 0:09:56.529222\n",
      "[Epoch 6/10000] [Batch 28/36] [D loss: 0.039960, acc:  99%] [G loss: 6.569840] time: 0:09:58.906227\n",
      "[Epoch 6/10000] [Batch 29/36] [D loss: 0.015797, acc:  99%] [G loss: 6.788959] time: 0:10:01.327395\n",
      "[Epoch 6/10000] [Batch 30/36] [D loss: 0.020014, acc:  99%] [G loss: 6.154585] time: 0:10:03.746559\n",
      "[Epoch 6/10000] [Batch 31/36] [D loss: 0.063482, acc:  96%] [G loss: 6.454359] time: 0:10:06.233799\n",
      "[Epoch 6/10000] [Batch 32/36] [D loss: 0.039791, acc:  99%] [G loss: 6.362211] time: 0:10:08.723127\n",
      "[Epoch 6/10000] [Batch 33/36] [D loss: 0.067900, acc:  98%] [G loss: 6.232237] time: 0:10:11.414959\n",
      "[Epoch 6/10000] [Batch 34/36] [D loss: 0.033589, acc:  99%] [G loss: 7.181236] time: 0:10:13.961321\n",
      "[Epoch 7/10000] [Batch 0/36] [D loss: 0.052178, acc: 100%] [G loss: 6.365659] time: 0:10:16.754571\n",
      "[Epoch 7/10000] [Batch 1/36] [D loss: 0.039363, acc:  98%] [G loss: 6.972831] time: 0:10:19.589075\n",
      "[Epoch 7/10000] [Batch 2/36] [D loss: 0.024643, acc:  99%] [G loss: 5.613678] time: 0:10:21.996703\n",
      "[Epoch 7/10000] [Batch 3/36] [D loss: 0.088446, acc:  92%] [G loss: 6.114301] time: 0:10:24.394987\n",
      "[Epoch 7/10000] [Batch 4/36] [D loss: 0.031468, acc:  99%] [G loss: 5.892928] time: 0:10:26.842372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/10000] [Batch 5/36] [D loss: 0.057448, acc:  99%] [G loss: 5.985649] time: 0:10:29.239540\n",
      "[Epoch 7/10000] [Batch 6/36] [D loss: 0.060710, acc:  96%] [G loss: 6.202170] time: 0:10:31.804789\n",
      "[Epoch 7/10000] [Batch 7/36] [D loss: 0.057981, acc:  98%] [G loss: 7.207861] time: 0:10:34.261570\n",
      "[Epoch 7/10000] [Batch 8/36] [D loss: 0.087397, acc:  95%] [G loss: 6.329824] time: 0:10:37.356536\n",
      "[Epoch 7/10000] [Batch 9/36] [D loss: 0.043954, acc:  99%] [G loss: 6.169283] time: 0:10:39.856032\n",
      "[Epoch 7/10000] [Batch 10/36] [D loss: 0.026241, acc:  99%] [G loss: 6.874887] time: 0:10:42.298782\n",
      "[Epoch 7/10000] [Batch 11/36] [D loss: 0.043374, acc:  99%] [G loss: 6.402205] time: 0:10:44.914471\n",
      "[Epoch 7/10000] [Batch 12/36] [D loss: 0.035119, acc:  99%] [G loss: 5.716847] time: 0:10:47.384306\n",
      "[Epoch 7/10000] [Batch 13/36] [D loss: 0.024784, acc:  99%] [G loss: 7.504488] time: 0:10:49.861973\n",
      "[Epoch 7/10000] [Batch 14/36] [D loss: 0.059685, acc:  98%] [G loss: 6.469845] time: 0:10:52.383093\n",
      "[Epoch 7/10000] [Batch 15/36] [D loss: 0.028682, acc:  99%] [G loss: 6.769557] time: 0:10:54.809025\n",
      "[Epoch 7/10000] [Batch 16/36] [D loss: 0.086062, acc:  96%] [G loss: 6.760119] time: 0:10:57.273318\n",
      "[Epoch 7/10000] [Batch 17/36] [D loss: 0.070294, acc:  99%] [G loss: 6.443084] time: 0:10:59.766290\n",
      "[Epoch 7/10000] [Batch 18/36] [D loss: 0.058835, acc:  97%] [G loss: 6.796949] time: 0:11:02.180364\n",
      "[Epoch 7/10000] [Batch 19/36] [D loss: 0.028146, acc:  99%] [G loss: 7.489431] time: 0:11:04.550844\n",
      "[Epoch 7/10000] [Batch 20/36] [D loss: 0.068533, acc:  96%] [G loss: 6.000635] time: 0:11:06.983729\n",
      "[Epoch 7/10000] [Batch 21/36] [D loss: 0.061891, acc:  96%] [G loss: 6.042089] time: 0:11:09.349621\n",
      "[Epoch 7/10000] [Batch 22/36] [D loss: 0.031473, acc:  99%] [G loss: 6.698327] time: 0:11:11.753393\n",
      "[Epoch 7/10000] [Batch 23/36] [D loss: 0.036847, acc:  99%] [G loss: 6.389580] time: 0:11:14.215598\n",
      "[Epoch 7/10000] [Batch 24/36] [D loss: 0.048676, acc:  99%] [G loss: 7.145671] time: 0:11:16.650209\n",
      "[Epoch 7/10000] [Batch 25/36] [D loss: 0.058772, acc:  99%] [G loss: 6.961213] time: 0:11:19.079243\n",
      "[Epoch 7/10000] [Batch 26/36] [D loss: 0.151675, acc:  79%] [G loss: 6.317119] time: 0:11:21.526046\n",
      "[Epoch 7/10000] [Batch 27/36] [D loss: 0.116237, acc:  88%] [G loss: 6.634588] time: 0:11:23.893903\n",
      "[Epoch 7/10000] [Batch 28/36] [D loss: 0.029781, acc:  99%] [G loss: 6.477160] time: 0:11:26.345042\n",
      "[Epoch 7/10000] [Batch 29/36] [D loss: 0.023826, acc:  99%] [G loss: 6.730799] time: 0:11:28.772670\n",
      "[Epoch 7/10000] [Batch 30/36] [D loss: 0.031942, acc:  99%] [G loss: 6.098568] time: 0:11:31.180229\n",
      "[Epoch 7/10000] [Batch 31/36] [D loss: 0.072007, acc:  95%] [G loss: 6.649533] time: 0:11:33.648756\n",
      "[Epoch 7/10000] [Batch 32/36] [D loss: 0.040166, acc:  99%] [G loss: 6.398783] time: 0:11:36.134289\n",
      "[Epoch 7/10000] [Batch 33/36] [D loss: 0.080232, acc:  98%] [G loss: 6.446526] time: 0:11:38.846479\n",
      "[Epoch 7/10000] [Batch 34/36] [D loss: 0.027042, acc:  99%] [G loss: 7.740852] time: 0:11:41.234347\n",
      "[Epoch 8/10000] [Batch 0/36] [D loss: 0.017410, acc: 100%] [G loss: 6.184957] time: 0:11:43.615982\n",
      "[Epoch 8/10000] [Batch 1/36] [D loss: 0.038017, acc:  99%] [G loss: 7.177192] time: 0:11:46.354320\n",
      "[Epoch 8/10000] [Batch 2/36] [D loss: 0.039042, acc:  99%] [G loss: 5.421397] time: 0:11:48.996097\n",
      "[Epoch 8/10000] [Batch 3/36] [D loss: 0.072371, acc:  93%] [G loss: 6.811482] time: 0:11:51.508594\n",
      "[Epoch 8/10000] [Batch 4/36] [D loss: 0.027826, acc:  99%] [G loss: 5.838223] time: 0:11:53.957017\n",
      "[Epoch 8/10000] [Batch 5/36] [D loss: 0.059455, acc:  99%] [G loss: 6.983020] time: 0:11:56.502112\n",
      "[Epoch 8/10000] [Batch 6/36] [D loss: 0.032052, acc:  99%] [G loss: 6.332291] time: 0:11:58.940440\n",
      "[Epoch 8/10000] [Batch 7/36] [D loss: 0.028243, acc:  99%] [G loss: 7.105736] time: 0:12:01.423961\n",
      "[Epoch 8/10000] [Batch 8/36] [D loss: 0.037166, acc:  99%] [G loss: 6.116757] time: 0:12:03.892766\n",
      "[Epoch 8/10000] [Batch 9/36] [D loss: 0.060849, acc:  97%] [G loss: 6.082054] time: 0:12:06.335826\n",
      "[Epoch 8/10000] [Batch 10/36] [D loss: 0.023519, acc:  99%] [G loss: 6.650385] time: 0:12:08.726870\n",
      "[Epoch 8/10000] [Batch 11/36] [D loss: 0.061460, acc:  97%] [G loss: 6.476976] time: 0:12:11.140575\n",
      "[Epoch 8/10000] [Batch 12/36] [D loss: 0.066223, acc:  97%] [G loss: 5.882996] time: 0:12:13.836348\n",
      "[Epoch 8/10000] [Batch 13/36] [D loss: 0.019848, acc:  99%] [G loss: 7.070318] time: 0:12:16.284756\n",
      "[Epoch 8/10000] [Batch 14/36] [D loss: 0.027244, acc:  99%] [G loss: 7.411183] time: 0:12:18.658203\n",
      "[Epoch 8/10000] [Batch 15/36] [D loss: 0.061662, acc:  97%] [G loss: 6.305639] time: 0:12:21.055512\n",
      "[Epoch 8/10000] [Batch 16/36] [D loss: 0.078792, acc:  98%] [G loss: 6.837228] time: 0:12:23.526773\n",
      "[Epoch 8/10000] [Batch 17/36] [D loss: 0.049959, acc:  99%] [G loss: 6.164778] time: 0:12:25.901267\n",
      "[Epoch 8/10000] [Batch 18/36] [D loss: 0.038511, acc:  99%] [G loss: 6.357739] time: 0:12:28.364787\n",
      "[Epoch 8/10000] [Batch 19/36] [D loss: 0.033891, acc:  99%] [G loss: 7.428810] time: 0:12:30.777870\n",
      "[Epoch 8/10000] [Batch 20/36] [D loss: 0.051894, acc:  98%] [G loss: 5.970895] time: 0:12:33.195242\n",
      "[Epoch 8/10000] [Batch 21/36] [D loss: 0.101674, acc:  86%] [G loss: 5.988758] time: 0:12:35.622961\n",
      "[Epoch 8/10000] [Batch 22/36] [D loss: 0.036363, acc:  99%] [G loss: 6.572498] time: 0:12:38.409836\n",
      "[Epoch 8/10000] [Batch 23/36] [D loss: 0.030036, acc:  99%] [G loss: 6.335812] time: 0:12:41.177939\n",
      "[Epoch 8/10000] [Batch 24/36] [D loss: 0.049006, acc:  99%] [G loss: 7.161891] time: 0:12:43.722989\n",
      "[Epoch 8/10000] [Batch 25/36] [D loss: 0.025024, acc:  99%] [G loss: 6.814422] time: 0:12:46.269372\n",
      "[Epoch 8/10000] [Batch 26/36] [D loss: 0.073585, acc:  96%] [G loss: 6.220575] time: 0:12:48.641095\n",
      "[Epoch 8/10000] [Batch 27/36] [D loss: 0.108210, acc:  87%] [G loss: 6.222810] time: 0:12:51.034471\n",
      "[Epoch 8/10000] [Batch 28/36] [D loss: 0.067531, acc:  94%] [G loss: 6.654761] time: 0:12:53.404375\n",
      "[Epoch 8/10000] [Batch 29/36] [D loss: 0.027852, acc:  99%] [G loss: 6.766695] time: 0:12:55.818151\n",
      "[Epoch 8/10000] [Batch 30/36] [D loss: 0.055896, acc:  99%] [G loss: 5.739614] time: 0:12:58.260165\n",
      "[Epoch 8/10000] [Batch 31/36] [D loss: 0.065460, acc:  96%] [G loss: 6.316998] time: 0:13:00.736508\n",
      "[Epoch 8/10000] [Batch 32/36] [D loss: 0.032524, acc:  99%] [G loss: 6.229418] time: 0:13:03.183401\n",
      "[Epoch 8/10000] [Batch 33/36] [D loss: 0.084422, acc:  95%] [G loss: 5.874346] time: 0:13:05.909735\n",
      "[Epoch 8/10000] [Batch 34/36] [D loss: 0.024993, acc:  99%] [G loss: 7.783063] time: 0:13:08.378208\n",
      "[Epoch 9/10000] [Batch 0/36] [D loss: 0.027734, acc: 100%] [G loss: 6.338334] time: 0:13:11.462746\n",
      "[Epoch 9/10000] [Batch 1/36] [D loss: 0.031300, acc:  99%] [G loss: 6.709200] time: 0:13:14.256278\n",
      "[Epoch 9/10000] [Batch 2/36] [D loss: 0.056401, acc:  98%] [G loss: 5.308653] time: 0:13:16.718408\n",
      "[Epoch 9/10000] [Batch 3/36] [D loss: 0.075557, acc:  92%] [G loss: 6.285332] time: 0:13:19.213770\n",
      "[Epoch 9/10000] [Batch 4/36] [D loss: 0.060846, acc:  96%] [G loss: 6.009770] time: 0:13:21.651710\n",
      "[Epoch 9/10000] [Batch 5/36] [D loss: 0.054792, acc:  99%] [G loss: 6.348901] time: 0:13:24.089600\n",
      "[Epoch 9/10000] [Batch 6/36] [D loss: 0.041930, acc:  98%] [G loss: 6.396765] time: 0:13:26.497396\n",
      "[Epoch 9/10000] [Batch 7/36] [D loss: 0.021680, acc:  99%] [G loss: 6.722321] time: 0:13:28.926801\n",
      "[Epoch 9/10000] [Batch 8/36] [D loss: 0.051068, acc:  99%] [G loss: 6.060131] time: 0:13:31.367111\n",
      "[Epoch 9/10000] [Batch 9/36] [D loss: 0.065815, acc:  96%] [G loss: 5.818848] time: 0:13:33.933215\n",
      "[Epoch 9/10000] [Batch 10/36] [D loss: 0.046137, acc:  98%] [G loss: 6.584261] time: 0:13:36.384466\n",
      "[Epoch 9/10000] [Batch 11/36] [D loss: 0.064325, acc:  97%] [G loss: 6.586667] time: 0:13:38.789220\n",
      "[Epoch 9/10000] [Batch 12/36] [D loss: 0.044812, acc:  99%] [G loss: 5.918934] time: 0:13:41.179667\n",
      "[Epoch 9/10000] [Batch 13/36] [D loss: 0.022736, acc:  99%] [G loss: 7.120217] time: 0:13:43.588890\n",
      "[Epoch 9/10000] [Batch 14/36] [D loss: 0.039891, acc:  99%] [G loss: 6.562675] time: 0:13:46.032506\n",
      "[Epoch 9/10000] [Batch 15/36] [D loss: 0.033991, acc:  99%] [G loss: 6.939437] time: 0:13:48.474408\n",
      "[Epoch 9/10000] [Batch 16/36] [D loss: 0.078584, acc:  97%] [G loss: 6.644972] time: 0:13:50.918760\n",
      "[Epoch 9/10000] [Batch 17/36] [D loss: 0.042445, acc:  99%] [G loss: 6.616380] time: 0:13:53.289341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/10000] [Batch 18/36] [D loss: 0.031878, acc:  99%] [G loss: 6.531971] time: 0:13:55.693229\n",
      "[Epoch 9/10000] [Batch 19/36] [D loss: 0.028992, acc:  99%] [G loss: 7.353600] time: 0:13:58.125517\n",
      "[Epoch 9/10000] [Batch 20/36] [D loss: 0.049026, acc:  98%] [G loss: 6.073250] time: 0:14:00.539094\n",
      "[Epoch 9/10000] [Batch 21/36] [D loss: 0.048925, acc:  98%] [G loss: 5.850881] time: 0:14:02.939840\n",
      "[Epoch 9/10000] [Batch 22/36] [D loss: 0.042044, acc:  99%] [G loss: 6.790715] time: 0:14:05.465622\n",
      "[Epoch 9/10000] [Batch 23/36] [D loss: 0.027611, acc:  99%] [G loss: 6.627184] time: 0:14:07.983516\n",
      "[Epoch 9/10000] [Batch 24/36] [D loss: 0.051694, acc:  99%] [G loss: 7.165666] time: 0:14:10.629986\n",
      "[Epoch 9/10000] [Batch 25/36] [D loss: 0.038513, acc: 100%] [G loss: 7.153037] time: 0:14:13.047672\n",
      "[Epoch 9/10000] [Batch 26/36] [D loss: 0.073122, acc:  97%] [G loss: 6.018962] time: 0:14:15.563275\n",
      "[Epoch 9/10000] [Batch 27/36] [D loss: 0.064167, acc:  98%] [G loss: 5.948407] time: 0:14:18.072798\n",
      "[Epoch 9/10000] [Batch 28/36] [D loss: 0.060355, acc:  96%] [G loss: 6.422382] time: 0:14:20.561649\n",
      "[Epoch 9/10000] [Batch 29/36] [D loss: 0.032971, acc:  99%] [G loss: 6.517156] time: 0:14:22.969133\n",
      "[Epoch 9/10000] [Batch 30/36] [D loss: 0.033661, acc:  99%] [G loss: 5.766596] time: 0:14:25.390469\n",
      "[Epoch 9/10000] [Batch 31/36] [D loss: 0.051010, acc:  98%] [G loss: 6.429225] time: 0:14:27.819777\n",
      "[Epoch 9/10000] [Batch 32/36] [D loss: 0.032016, acc:  99%] [G loss: 6.381795] time: 0:14:30.222199\n",
      "[Epoch 9/10000] [Batch 33/36] [D loss: 0.069333, acc:  98%] [G loss: 5.994383] time: 0:14:32.924630\n",
      "[Epoch 9/10000] [Batch 34/36] [D loss: 0.021184, acc:  99%] [G loss: 7.273535] time: 0:14:35.311737\n",
      "[Epoch 10/10000] [Batch 0/36] [D loss: 0.036197, acc: 100%] [G loss: 6.293197] time: 0:14:37.709310\n",
      "[Epoch 10/10000] [Batch 1/36] [D loss: 0.035626, acc:  99%] [G loss: 6.504508] time: 0:14:40.441874\n",
      "[Epoch 10/10000] [Batch 2/36] [D loss: 0.023360, acc:  99%] [G loss: 5.206969] time: 0:14:43.344444\n",
      "[Epoch 10/10000] [Batch 3/36] [D loss: 0.069046, acc:  98%] [G loss: 6.221905] time: 0:14:45.799138\n",
      "[Epoch 10/10000] [Batch 4/36] [D loss: 0.045065, acc:  97%] [G loss: 5.860250] time: 0:14:48.474065\n",
      "[Epoch 10/10000] [Batch 5/36] [D loss: 0.040150, acc:  99%] [G loss: 6.264784] time: 0:14:50.942000\n",
      "[Epoch 10/10000] [Batch 6/36] [D loss: 0.051485, acc:  97%] [G loss: 6.341360] time: 0:14:53.530329\n",
      "[Epoch 10/10000] [Batch 7/36] [D loss: 0.044221, acc:  99%] [G loss: 6.928934] time: 0:14:56.206441\n",
      "[Epoch 10/10000] [Batch 8/36] [D loss: 0.046552, acc:  99%] [G loss: 6.013063] time: 0:14:58.948895\n",
      "[Epoch 10/10000] [Batch 9/36] [D loss: 0.059204, acc:  96%] [G loss: 6.079501] time: 0:15:01.529405\n",
      "[Epoch 10/10000] [Batch 10/36] [D loss: 0.025466, acc:  99%] [G loss: 6.807944] time: 0:15:03.947105\n",
      "[Epoch 10/10000] [Batch 11/36] [D loss: 0.040058, acc:  99%] [G loss: 6.367674] time: 0:15:06.547721\n",
      "[Epoch 10/10000] [Batch 12/36] [D loss: 0.058590, acc:  98%] [G loss: 6.259323] time: 0:15:09.067044\n",
      "[Epoch 10/10000] [Batch 13/36] [D loss: 0.037572, acc:  99%] [G loss: 7.413906] time: 0:15:11.497936\n",
      "[Epoch 10/10000] [Batch 14/36] [D loss: 0.040684, acc:  99%] [G loss: 6.291525] time: 0:15:13.952940\n",
      "[Epoch 10/10000] [Batch 15/36] [D loss: 0.030355, acc:  99%] [G loss: 6.351958] time: 0:15:16.370836\n",
      "[Epoch 10/10000] [Batch 16/36] [D loss: 0.122616, acc:  87%] [G loss: 6.960894] time: 0:15:18.768402\n",
      "[Epoch 10/10000] [Batch 17/36] [D loss: 0.049293, acc:  99%] [G loss: 6.506033] time: 0:15:21.176115\n",
      "[Epoch 10/10000] [Batch 18/36] [D loss: 0.042411, acc:  99%] [G loss: 7.458849] time: 0:15:23.664698\n",
      "[Epoch 10/10000] [Batch 19/36] [D loss: 0.024976, acc:  99%] [G loss: 7.799088] time: 0:15:26.128981\n",
      "[Epoch 10/10000] [Batch 20/36] [D loss: 0.032492, acc:  99%] [G loss: 6.199152] time: 0:15:28.572047\n",
      "[Epoch 10/10000] [Batch 21/36] [D loss: 0.074126, acc:  95%] [G loss: 6.245258] time: 0:15:30.958796\n",
      "[Epoch 10/10000] [Batch 22/36] [D loss: 0.041146, acc:  99%] [G loss: 6.575273] time: 0:15:33.468669\n",
      "[Epoch 10/10000] [Batch 23/36] [D loss: 0.031743, acc:  99%] [G loss: 6.640914] time: 0:15:35.888560\n",
      "[Epoch 10/10000] [Batch 24/36] [D loss: 0.055273, acc:  99%] [G loss: 6.969617] time: 0:15:38.392814\n",
      "[Epoch 10/10000] [Batch 25/36] [D loss: 0.035246, acc: 100%] [G loss: 6.814021] time: 0:15:40.622519\n",
      "[Epoch 10/10000] [Batch 26/36] [D loss: 0.118449, acc:  87%] [G loss: 6.128060] time: 0:15:42.990933\n",
      "[Epoch 10/10000] [Batch 27/36] [D loss: 0.090462, acc:  91%] [G loss: 6.226303] time: 0:15:45.444281\n",
      "[Epoch 10/10000] [Batch 28/36] [D loss: 0.091350, acc:  89%] [G loss: 6.518995] time: 0:15:47.948640\n",
      "[Epoch 10/10000] [Batch 29/36] [D loss: 0.036851, acc:  99%] [G loss: 7.037470] time: 0:15:50.320582\n",
      "[Epoch 10/10000] [Batch 30/36] [D loss: 0.050144, acc:  98%] [G loss: 6.123721] time: 0:15:52.723191\n",
      "[Epoch 10/10000] [Batch 31/36] [D loss: 0.034188, acc:  99%] [G loss: 6.676654] time: 0:15:55.136022\n",
      "[Epoch 10/10000] [Batch 32/36] [D loss: 0.033076, acc:  99%] [G loss: 6.514608] time: 0:15:57.528044\n",
      "[Epoch 10/10000] [Batch 33/36] [D loss: 0.042331, acc:  99%] [G loss: 5.787822] time: 0:16:00.253825\n",
      "[Epoch 10/10000] [Batch 34/36] [D loss: 0.040299, acc:  99%] [G loss: 7.495254] time: 0:16:02.861027\n",
      "[Epoch 11/10000] [Batch 0/36] [D loss: 0.031739, acc:  99%] [G loss: 6.343738] time: 0:16:05.614307\n",
      "[Epoch 11/10000] [Batch 1/36] [D loss: 0.048537, acc:  99%] [G loss: 6.475368] time: 0:16:08.529835\n",
      "[Epoch 11/10000] [Batch 2/36] [D loss: 0.026175, acc:  99%] [G loss: 5.585020] time: 0:16:10.977899\n",
      "[Epoch 11/10000] [Batch 3/36] [D loss: 0.046827, acc:  99%] [G loss: 5.980047] time: 0:16:13.416080\n",
      "[Epoch 11/10000] [Batch 4/36] [D loss: 0.057152, acc:  96%] [G loss: 5.763898] time: 0:16:16.030558\n",
      "[Epoch 11/10000] [Batch 5/36] [D loss: 0.056352, acc:  98%] [G loss: 6.553818] time: 0:16:18.474817\n",
      "[Epoch 11/10000] [Batch 6/36] [D loss: 0.059775, acc:  97%] [G loss: 6.486015] time: 0:16:20.873302\n",
      "[Epoch 11/10000] [Batch 7/36] [D loss: 0.028923, acc:  99%] [G loss: 6.655971] time: 0:16:23.279928\n",
      "[Epoch 11/10000] [Batch 8/36] [D loss: 0.056284, acc:  98%] [G loss: 6.187930] time: 0:16:25.693583\n",
      "[Epoch 11/10000] [Batch 9/36] [D loss: 0.038376, acc:  99%] [G loss: 6.148587] time: 0:16:27.967006\n",
      "[Epoch 11/10000] [Batch 10/36] [D loss: 0.030214, acc:  99%] [G loss: 6.576936] time: 0:16:30.238091\n",
      "[Epoch 11/10000] [Batch 11/36] [D loss: 0.041251, acc:  99%] [G loss: 6.491934] time: 0:16:32.579575\n",
      "[Epoch 11/10000] [Batch 12/36] [D loss: 0.048374, acc:  98%] [G loss: 6.172689] time: 0:16:34.971119\n",
      "[Epoch 11/10000] [Batch 13/36] [D loss: 0.025600, acc:  99%] [G loss: 7.122094] time: 0:16:37.321523\n",
      "[Epoch 11/10000] [Batch 14/36] [D loss: 0.035964, acc:  99%] [G loss: 6.006070] time: 0:16:39.635457\n",
      "[Epoch 11/10000] [Batch 15/36] [D loss: 0.059266, acc:  98%] [G loss: 6.684645] time: 0:16:41.928360\n",
      "[Epoch 11/10000] [Batch 16/36] [D loss: 0.086318, acc:  95%] [G loss: 6.461466] time: 0:16:44.316221\n",
      "[Epoch 11/10000] [Batch 17/36] [D loss: 0.027118, acc:  99%] [G loss: 6.066932] time: 0:16:46.646768\n",
      "[Epoch 11/10000] [Batch 18/36] [D loss: 0.033303, acc:  99%] [G loss: 6.605293] time: 0:16:49.011755\n",
      "[Epoch 11/10000] [Batch 19/36] [D loss: 0.031266, acc:  99%] [G loss: 7.192181] time: 0:16:51.314338\n",
      "[Epoch 11/10000] [Batch 20/36] [D loss: 0.029503, acc:  99%] [G loss: 6.339305] time: 0:16:53.576636\n",
      "[Epoch 11/10000] [Batch 21/36] [D loss: 0.060705, acc:  98%] [G loss: 5.974777] time: 0:16:55.947123\n",
      "[Epoch 11/10000] [Batch 22/36] [D loss: 0.039010, acc:  99%] [G loss: 6.509780] time: 0:16:58.292859\n",
      "[Epoch 11/10000] [Batch 23/36] [D loss: 0.068685, acc:  96%] [G loss: 6.687424] time: 0:17:00.633010\n",
      "[Epoch 11/10000] [Batch 24/36] [D loss: 0.055187, acc:  98%] [G loss: 6.997758] time: 0:17:02.935011\n",
      "[Epoch 11/10000] [Batch 25/36] [D loss: 0.069813, acc:  99%] [G loss: 6.629856] time: 0:17:05.320057\n",
      "[Epoch 11/10000] [Batch 26/36] [D loss: 0.083460, acc:  97%] [G loss: 6.027410] time: 0:17:07.732250\n",
      "[Epoch 11/10000] [Batch 27/36] [D loss: 0.105610, acc:  90%] [G loss: 5.956470] time: 0:17:10.148618\n",
      "[Epoch 11/10000] [Batch 28/36] [D loss: 0.045330, acc:  98%] [G loss: 6.246220] time: 0:17:12.579178\n",
      "[Epoch 11/10000] [Batch 29/36] [D loss: 0.034612, acc:  99%] [G loss: 6.510240] time: 0:17:15.082927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/10000] [Batch 30/36] [D loss: 0.028353, acc:  99%] [G loss: 5.868825] time: 0:17:17.535282\n",
      "[Epoch 11/10000] [Batch 31/36] [D loss: 0.040265, acc:  99%] [G loss: 6.395088] time: 0:17:20.114982\n",
      "[Epoch 11/10000] [Batch 32/36] [D loss: 0.038508, acc:  99%] [G loss: 6.133809] time: 0:17:22.563400\n",
      "[Epoch 11/10000] [Batch 33/36] [D loss: 0.049448, acc:  99%] [G loss: 6.005239] time: 0:17:26.108853\n",
      "[Epoch 11/10000] [Batch 34/36] [D loss: 0.026190, acc:  99%] [G loss: 7.280895] time: 0:17:28.749855\n",
      "[Epoch 12/10000] [Batch 0/36] [D loss: 0.034366, acc: 100%] [G loss: 6.108833] time: 0:17:31.182855\n",
      "[Epoch 12/10000] [Batch 1/36] [D loss: 0.026024, acc:  99%] [G loss: 6.983995] time: 0:17:33.910702\n",
      "[Epoch 12/10000] [Batch 2/36] [D loss: 0.030286, acc:  99%] [G loss: 5.219497] time: 0:17:36.419533\n",
      "[Epoch 12/10000] [Batch 3/36] [D loss: 0.054980, acc:  98%] [G loss: 5.968154] time: 0:17:38.840397\n",
      "[Epoch 12/10000] [Batch 4/36] [D loss: 0.050025, acc:  98%] [G loss: 5.782197] time: 0:17:41.285578\n",
      "[Epoch 12/10000] [Batch 5/36] [D loss: 0.053458, acc:  99%] [G loss: 5.900906] time: 0:17:43.724038\n",
      "[Epoch 12/10000] [Batch 6/36] [D loss: 0.062173, acc:  96%] [G loss: 6.126210] time: 0:17:46.151807\n",
      "[Epoch 12/10000] [Batch 7/36] [D loss: 0.048023, acc:  99%] [G loss: 6.624150] time: 0:17:48.559154\n",
      "[Epoch 12/10000] [Batch 8/36] [D loss: 0.051894, acc:  98%] [G loss: 5.763314] time: 0:17:51.093664\n",
      "[Epoch 12/10000] [Batch 9/36] [D loss: 0.041786, acc:  99%] [G loss: 6.218016] time: 0:17:53.577469\n",
      "[Epoch 12/10000] [Batch 10/36] [D loss: 0.021692, acc:  99%] [G loss: 6.596544] time: 0:17:56.025676\n",
      "[Epoch 12/10000] [Batch 11/36] [D loss: 0.054948, acc:  98%] [G loss: 6.189936] time: 0:17:58.467737\n",
      "[Epoch 12/10000] [Batch 12/36] [D loss: 0.063777, acc:  98%] [G loss: 6.135207] time: 0:18:00.891796\n",
      "[Epoch 12/10000] [Batch 13/36] [D loss: 0.047222, acc:  99%] [G loss: 7.250452] time: 0:18:03.360466\n",
      "[Epoch 12/10000] [Batch 14/36] [D loss: 0.051314, acc:  97%] [G loss: 5.893135] time: 0:18:05.832986\n",
      "[Epoch 12/10000] [Batch 15/36] [D loss: 0.040808, acc:  99%] [G loss: 6.387852] time: 0:18:08.270870\n",
      "[Epoch 12/10000] [Batch 16/36] [D loss: 0.078208, acc:  95%] [G loss: 6.597067] time: 0:18:10.715185\n",
      "[Epoch 12/10000] [Batch 17/36] [D loss: 0.027108, acc:  99%] [G loss: 5.793234] time: 0:18:13.173599\n",
      "[Epoch 12/10000] [Batch 18/36] [D loss: 0.037810, acc:  99%] [G loss: 6.768203] time: 0:18:15.642257\n",
      "[Epoch 12/10000] [Batch 19/36] [D loss: 0.026401, acc:  99%] [G loss: 7.417650] time: 0:18:18.043692\n",
      "[Epoch 12/10000] [Batch 20/36] [D loss: 0.041636, acc:  99%] [G loss: 5.771923] time: 0:18:20.481140\n",
      "[Epoch 12/10000] [Batch 21/36] [D loss: 0.056825, acc:  98%] [G loss: 5.963838] time: 0:18:22.878769\n",
      "[Epoch 12/10000] [Batch 22/36] [D loss: 0.031094, acc:  99%] [G loss: 6.365530] time: 0:18:25.282606\n",
      "[Epoch 12/10000] [Batch 23/36] [D loss: 0.034857, acc:  99%] [G loss: 6.339489] time: 0:18:27.731005\n",
      "[Epoch 12/10000] [Batch 24/36] [D loss: 0.068272, acc:  97%] [G loss: 6.776587] time: 0:18:30.128089\n",
      "[Epoch 12/10000] [Batch 25/36] [D loss: 0.028911, acc:  99%] [G loss: 6.658139] time: 0:18:32.566312\n",
      "[Epoch 12/10000] [Batch 26/36] [D loss: 0.116378, acc:  88%] [G loss: 5.793737] time: 0:18:35.017098\n",
      "[Epoch 12/10000] [Batch 27/36] [D loss: 0.062617, acc:  97%] [G loss: 6.159953] time: 0:18:37.422289\n",
      "[Epoch 12/10000] [Batch 28/36] [D loss: 0.028201, acc:  99%] [G loss: 6.311652] time: 0:18:39.840026\n",
      "[Epoch 12/10000] [Batch 29/36] [D loss: 0.026793, acc:  99%] [G loss: 6.640219] time: 0:18:42.277205\n",
      "[Epoch 12/10000] [Batch 30/36] [D loss: 0.042709, acc:  99%] [G loss: 5.907364] time: 0:18:44.776888\n",
      "[Epoch 12/10000] [Batch 31/36] [D loss: 0.063671, acc:  97%] [G loss: 6.566238] time: 0:18:47.187904\n",
      "[Epoch 12/10000] [Batch 32/36] [D loss: 0.036983, acc:  99%] [G loss: 6.085750] time: 0:18:49.628871\n",
      "[Epoch 12/10000] [Batch 33/36] [D loss: 0.068249, acc:  98%] [G loss: 6.089280] time: 0:18:52.335080\n",
      "[Epoch 12/10000] [Batch 34/36] [D loss: 0.027482, acc:  99%] [G loss: 7.573847] time: 0:18:54.788524\n",
      "[Epoch 13/10000] [Batch 0/36] [D loss: 0.017588, acc: 100%] [G loss: 6.022605] time: 0:18:57.190722\n",
      "[Epoch 13/10000] [Batch 1/36] [D loss: 0.060110, acc:  99%] [G loss: 7.027494] time: 0:19:00.678054\n",
      "[Epoch 13/10000] [Batch 2/36] [D loss: 0.035478, acc:  99%] [G loss: 5.577238] time: 0:19:03.153990\n",
      "[Epoch 13/10000] [Batch 3/36] [D loss: 0.051412, acc:  99%] [G loss: 6.026709] time: 0:19:05.565398\n",
      "[Epoch 13/10000] [Batch 4/36] [D loss: 0.044690, acc:  99%] [G loss: 6.023310] time: 0:19:08.146687\n",
      "[Epoch 13/10000] [Batch 5/36] [D loss: 0.047057, acc:  99%] [G loss: 6.255341] time: 0:19:10.620705\n",
      "[Epoch 13/10000] [Batch 6/36] [D loss: 0.056201, acc:  97%] [G loss: 6.377272] time: 0:19:13.053511\n",
      "[Epoch 13/10000] [Batch 7/36] [D loss: 0.061739, acc:  97%] [G loss: 7.005252] time: 0:19:15.476598\n",
      "[Epoch 13/10000] [Batch 8/36] [D loss: 0.056955, acc:  97%] [G loss: 5.802516] time: 0:19:17.914673\n",
      "[Epoch 13/10000] [Batch 9/36] [D loss: 0.121078, acc:  86%] [G loss: 6.130377] time: 0:19:20.322007\n",
      "[Epoch 13/10000] [Batch 10/36] [D loss: 0.033341, acc:  99%] [G loss: 6.635929] time: 0:19:22.719486\n",
      "[Epoch 13/10000] [Batch 11/36] [D loss: 0.092402, acc:  91%] [G loss: 6.749574] time: 0:19:25.147274\n",
      "[Epoch 13/10000] [Batch 12/36] [D loss: 0.090972, acc:  89%] [G loss: 5.667965] time: 0:19:27.483738\n",
      "[Epoch 13/10000] [Batch 13/36] [D loss: 0.036838, acc:  99%] [G loss: 7.688240] time: 0:19:29.912759\n",
      "[Epoch 13/10000] [Batch 14/36] [D loss: 0.063705, acc:  98%] [G loss: 6.171743] time: 0:19:32.425980\n",
      "[Epoch 13/10000] [Batch 15/36] [D loss: 0.041846, acc:  99%] [G loss: 6.435446] time: 0:19:34.823394\n",
      "[Epoch 13/10000] [Batch 16/36] [D loss: 0.038030, acc:  99%] [G loss: 6.544686] time: 0:19:37.327417\n",
      "[Epoch 13/10000] [Batch 17/36] [D loss: 0.032948, acc:  99%] [G loss: 6.114530] time: 0:19:39.755716\n",
      "[Epoch 13/10000] [Batch 18/36] [D loss: 0.060352, acc:  97%] [G loss: 6.453661] time: 0:19:42.276040\n",
      "[Epoch 13/10000] [Batch 19/36] [D loss: 0.023190, acc:  99%] [G loss: 7.258385] time: 0:19:44.794202\n",
      "[Epoch 13/10000] [Batch 20/36] [D loss: 0.056603, acc:  98%] [G loss: 5.832351] time: 0:19:47.220095\n",
      "[Epoch 13/10000] [Batch 21/36] [D loss: 0.053906, acc:  98%] [G loss: 5.605606] time: 0:19:49.554697\n",
      "[Epoch 13/10000] [Batch 22/36] [D loss: 0.028028, acc:  99%] [G loss: 6.673048] time: 0:19:51.840433\n",
      "[Epoch 13/10000] [Batch 23/36] [D loss: 0.029099, acc:  99%] [G loss: 6.247685] time: 0:19:54.358672\n",
      "[Epoch 13/10000] [Batch 24/36] [D loss: 0.075166, acc:  96%] [G loss: 6.778234] time: 0:19:56.913218\n",
      "[Epoch 13/10000] [Batch 25/36] [D loss: 0.037436, acc:  99%] [G loss: 6.770700] time: 0:19:59.430032\n",
      "[Epoch 13/10000] [Batch 26/36] [D loss: 0.168384, acc:  74%] [G loss: 5.885403] time: 0:20:01.989130\n",
      "[Epoch 13/10000] [Batch 27/36] [D loss: 0.096671, acc:  92%] [G loss: 5.854114] time: 0:20:04.488127\n",
      "[Epoch 13/10000] [Batch 28/36] [D loss: 0.028415, acc:  99%] [G loss: 6.546818] time: 0:20:06.875573\n",
      "[Epoch 13/10000] [Batch 29/36] [D loss: 0.025171, acc:  99%] [G loss: 6.342611] time: 0:20:09.376079\n",
      "[Epoch 13/10000] [Batch 30/36] [D loss: 0.019127, acc:  99%] [G loss: 5.679665] time: 0:20:11.796357\n",
      "[Epoch 13/10000] [Batch 31/36] [D loss: 0.057346, acc:  98%] [G loss: 6.159582] time: 0:20:14.270979\n",
      "[Epoch 13/10000] [Batch 32/36] [D loss: 0.030290, acc:  99%] [G loss: 6.401410] time: 0:20:16.698871\n",
      "[Epoch 13/10000] [Batch 33/36] [D loss: 0.053435, acc:  99%] [G loss: 5.767465] time: 0:20:19.431456\n",
      "[Epoch 13/10000] [Batch 34/36] [D loss: 0.031037, acc:  99%] [G loss: 7.656099] time: 0:20:21.872474\n",
      "[Epoch 14/10000] [Batch 0/36] [D loss: 0.035396, acc: 100%] [G loss: 6.044165] time: 0:20:24.490427\n",
      "[Epoch 14/10000] [Batch 1/36] [D loss: 0.036887, acc:  99%] [G loss: 6.660104] time: 0:20:27.355053\n",
      "[Epoch 14/10000] [Batch 2/36] [D loss: 0.038226, acc:  99%] [G loss: 5.192746] time: 0:20:29.782977\n",
      "[Epoch 14/10000] [Batch 3/36] [D loss: 0.035726, acc:  99%] [G loss: 5.924169] time: 0:20:32.326832\n",
      "[Epoch 14/10000] [Batch 4/36] [D loss: 0.022267, acc:  99%] [G loss: 5.634038] time: 0:20:34.898037\n",
      "[Epoch 14/10000] [Batch 5/36] [D loss: 0.034365, acc:  99%] [G loss: 6.423098] time: 0:20:37.620532\n",
      "[Epoch 14/10000] [Batch 6/36] [D loss: 0.070889, acc:  95%] [G loss: 6.342957] time: 0:20:40.175589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14/10000] [Batch 7/36] [D loss: 0.040829, acc:  99%] [G loss: 6.827934] time: 0:20:42.726294\n",
      "[Epoch 14/10000] [Batch 8/36] [D loss: 0.050659, acc:  99%] [G loss: 6.116211] time: 0:20:45.588368\n",
      "[Epoch 14/10000] [Batch 9/36] [D loss: 0.040672, acc:  99%] [G loss: 6.052268] time: 0:20:48.101853\n",
      "[Epoch 14/10000] [Batch 10/36] [D loss: 0.037197, acc:  99%] [G loss: 6.557438] time: 0:20:50.647737\n",
      "[Epoch 14/10000] [Batch 11/36] [D loss: 0.051683, acc:  99%] [G loss: 6.409778] time: 0:20:53.065896\n",
      "[Epoch 14/10000] [Batch 12/36] [D loss: 0.052465, acc:  99%] [G loss: 5.673685] time: 0:20:55.598561\n",
      "[Epoch 14/10000] [Batch 13/36] [D loss: 0.022732, acc:  99%] [G loss: 7.316398] time: 0:20:58.032657\n",
      "[Epoch 14/10000] [Batch 14/36] [D loss: 0.049229, acc:  99%] [G loss: 6.294940] time: 0:21:00.515499\n",
      "[Epoch 14/10000] [Batch 15/36] [D loss: 0.029375, acc:  99%] [G loss: 6.326128] time: 0:21:02.919421\n",
      "[Epoch 14/10000] [Batch 16/36] [D loss: 0.076448, acc:  98%] [G loss: 6.611895] time: 0:21:05.377620\n",
      "[Epoch 14/10000] [Batch 17/36] [D loss: 0.043195, acc:  99%] [G loss: 6.166076] time: 0:21:07.825604\n",
      "[Epoch 14/10000] [Batch 18/36] [D loss: 0.032331, acc:  99%] [G loss: 6.318764] time: 0:21:10.216709\n",
      "[Epoch 14/10000] [Batch 19/36] [D loss: 0.041762, acc:  97%] [G loss: 7.080605] time: 0:21:12.661526\n",
      "[Epoch 14/10000] [Batch 20/36] [D loss: 0.056465, acc:  97%] [G loss: 5.780644] time: 0:21:15.109777\n",
      "[Epoch 14/10000] [Batch 21/36] [D loss: 0.054804, acc:  98%] [G loss: 5.964228] time: 0:21:17.500467\n",
      "[Epoch 14/10000] [Batch 22/36] [D loss: 0.024462, acc:  99%] [G loss: 6.386594] time: 0:21:19.914739\n",
      "[Epoch 14/10000] [Batch 23/36] [D loss: 0.036613, acc:  99%] [G loss: 6.265519] time: 0:21:22.373595\n",
      "[Epoch 14/10000] [Batch 24/36] [D loss: 0.046679, acc:  99%] [G loss: 6.799988] time: 0:21:24.800782\n",
      "[Epoch 14/10000] [Batch 25/36] [D loss: 0.052813, acc:  99%] [G loss: 6.510783] time: 0:21:27.392731\n",
      "[Epoch 14/10000] [Batch 26/36] [D loss: 0.069528, acc:  98%] [G loss: 6.472534] time: 0:21:29.893920\n",
      "[Epoch 14/10000] [Batch 27/36] [D loss: 0.118826, acc:  86%] [G loss: 5.943525] time: 0:21:32.349144\n",
      "[Epoch 14/10000] [Batch 28/36] [D loss: 0.047703, acc:  98%] [G loss: 5.893966] time: 0:21:34.777470\n",
      "[Epoch 14/10000] [Batch 29/36] [D loss: 0.021251, acc:  99%] [G loss: 6.371713] time: 0:21:37.225643\n",
      "[Epoch 14/10000] [Batch 30/36] [D loss: 0.032285, acc:  99%] [G loss: 5.736742] time: 0:21:39.616697\n",
      "[Epoch 14/10000] [Batch 31/36] [D loss: 0.033664, acc:  99%] [G loss: 5.996377] time: 0:21:42.071013\n",
      "[Epoch 14/10000] [Batch 32/36] [D loss: 0.029595, acc:  99%] [G loss: 6.275122] time: 0:21:44.513387\n",
      "[Epoch 14/10000] [Batch 33/36] [D loss: 0.078569, acc:  97%] [G loss: 5.768749] time: 0:21:47.246536\n",
      "[Epoch 14/10000] [Batch 34/36] [D loss: 0.020854, acc:  99%] [G loss: 6.890138] time: 0:21:49.744432\n",
      "[Epoch 15/10000] [Batch 0/36] [D loss: 0.056345, acc: 100%] [G loss: 6.281175] time: 0:21:52.646477\n",
      "[Epoch 15/10000] [Batch 1/36] [D loss: 0.035225, acc:  99%] [G loss: 6.663031] time: 0:21:57.069182\n",
      "[Epoch 15/10000] [Batch 2/36] [D loss: 0.030509, acc:  99%] [G loss: 5.259266] time: 0:21:59.543848\n",
      "[Epoch 15/10000] [Batch 3/36] [D loss: 0.066789, acc:  96%] [G loss: 5.894185] time: 0:22:02.052871\n",
      "[Epoch 15/10000] [Batch 4/36] [D loss: 0.049344, acc:  97%] [G loss: 5.534597] time: 0:22:04.464478\n",
      "[Epoch 15/10000] [Batch 5/36] [D loss: 0.070036, acc:  97%] [G loss: 6.097953] time: 0:22:06.943194\n",
      "[Epoch 15/10000] [Batch 6/36] [D loss: 0.072299, acc:  95%] [G loss: 6.076827] time: 0:22:09.408192\n",
      "[Epoch 15/10000] [Batch 7/36] [D loss: 0.044471, acc:  99%] [G loss: 6.344943] time: 0:22:11.836009\n",
      "[Epoch 15/10000] [Batch 8/36] [D loss: 0.044338, acc:  99%] [G loss: 5.649141] time: 0:22:14.345190\n",
      "[Epoch 15/10000] [Batch 9/36] [D loss: 0.049007, acc:  98%] [G loss: 6.002483] time: 0:22:16.834114\n",
      "[Epoch 15/10000] [Batch 10/36] [D loss: 0.023105, acc:  99%] [G loss: 6.483793] time: 0:22:19.255436\n",
      "[Epoch 15/10000] [Batch 11/36] [D loss: 0.047559, acc:  99%] [G loss: 6.376405] time: 0:22:21.720380\n",
      "[Epoch 15/10000] [Batch 12/36] [D loss: 0.046052, acc:  99%] [G loss: 6.049860] time: 0:22:24.137905\n",
      "[Epoch 15/10000] [Batch 13/36] [D loss: 0.027231, acc:  99%] [G loss: 7.170105] time: 0:22:26.580841\n",
      "[Epoch 15/10000] [Batch 14/36] [D loss: 0.062481, acc:  98%] [G loss: 5.876452] time: 0:22:29.008146\n",
      "[Epoch 15/10000] [Batch 15/36] [D loss: 0.031082, acc:  99%] [G loss: 6.636187] time: 0:22:31.452184\n",
      "[Epoch 15/10000] [Batch 16/36] [D loss: 0.055653, acc:  98%] [G loss: 6.619877] time: 0:22:33.870008\n",
      "[Epoch 15/10000] [Batch 17/36] [D loss: 0.061416, acc:  99%] [G loss: 6.288135] time: 0:22:36.318096\n",
      "[Epoch 15/10000] [Batch 18/36] [D loss: 0.044481, acc:  99%] [G loss: 6.473344] time: 0:22:38.796716\n",
      "[Epoch 15/10000] [Batch 19/36] [D loss: 0.042051, acc:  97%] [G loss: 7.300082] time: 0:22:41.245152\n",
      "[Epoch 15/10000] [Batch 20/36] [D loss: 0.056085, acc:  98%] [G loss: 5.806317] time: 0:22:43.703614\n",
      "[Epoch 15/10000] [Batch 21/36] [D loss: 0.061171, acc:  97%] [G loss: 5.761196] time: 0:22:46.192403\n",
      "[Epoch 15/10000] [Batch 22/36] [D loss: 0.031501, acc:  99%] [G loss: 6.381093] time: 0:22:48.671834\n",
      "[Epoch 15/10000] [Batch 23/36] [D loss: 0.057290, acc:  98%] [G loss: 6.281934] time: 0:22:51.301943\n",
      "[Epoch 15/10000] [Batch 24/36] [D loss: 0.074426, acc:  98%] [G loss: 6.717842] time: 0:22:53.776502\n",
      "[Epoch 15/10000] [Batch 25/36] [D loss: 0.036976, acc: 100%] [G loss: 6.633697] time: 0:22:56.272748\n",
      "[Epoch 15/10000] [Batch 26/36] [D loss: 0.088677, acc:  96%] [G loss: 5.605192] time: 0:22:58.799051\n",
      "[Epoch 15/10000] [Batch 27/36] [D loss: 0.072435, acc:  96%] [G loss: 5.776491] time: 0:23:01.298020\n",
      "[Epoch 15/10000] [Batch 28/36] [D loss: 0.039151, acc:  99%] [G loss: 6.180719] time: 0:23:04.325235\n",
      "[Epoch 15/10000] [Batch 29/36] [D loss: 0.023131, acc:  99%] [G loss: 6.003759] time: 0:23:06.773892\n",
      "[Epoch 15/10000] [Batch 30/36] [D loss: 0.018512, acc:  99%] [G loss: 5.610332] time: 0:23:09.262659\n",
      "[Epoch 15/10000] [Batch 31/36] [D loss: 0.074723, acc:  96%] [G loss: 6.026476] time: 0:23:11.761600\n",
      "[Epoch 15/10000] [Batch 32/36] [D loss: 0.054918, acc:  98%] [G loss: 5.870382] time: 0:23:14.264716\n",
      "[Epoch 15/10000] [Batch 33/36] [D loss: 0.101263, acc:  92%] [G loss: 5.639606] time: 0:23:17.033685\n",
      "[Epoch 15/10000] [Batch 34/36] [D loss: 0.023784, acc:  99%] [G loss: 6.967539] time: 0:23:19.512667\n",
      "[Epoch 16/10000] [Batch 0/36] [D loss: 0.046805, acc: 100%] [G loss: 5.850466] time: 0:23:22.031680\n",
      "[Epoch 16/10000] [Batch 1/36] [D loss: 0.034084, acc:  99%] [G loss: 6.626868] time: 0:23:25.526275\n",
      "[Epoch 16/10000] [Batch 2/36] [D loss: 0.043315, acc:  99%] [G loss: 5.412103] time: 0:23:28.066126\n",
      "[Epoch 16/10000] [Batch 3/36] [D loss: 0.095362, acc:  94%] [G loss: 6.451991] time: 0:23:30.626190\n",
      "[Epoch 16/10000] [Batch 4/36] [D loss: 0.028380, acc:  99%] [G loss: 5.871302] time: 0:23:33.155641\n",
      "[Epoch 16/10000] [Batch 5/36] [D loss: 0.046150, acc:  99%] [G loss: 6.348763] time: 0:23:35.657570\n",
      "[Epoch 16/10000] [Batch 6/36] [D loss: 0.062633, acc:  95%] [G loss: 6.062774] time: 0:23:38.092863\n",
      "[Epoch 16/10000] [Batch 7/36] [D loss: 0.044029, acc:  99%] [G loss: 6.731412] time: 0:23:40.503754\n",
      "[Epoch 16/10000] [Batch 8/36] [D loss: 0.063808, acc:  97%] [G loss: 5.426462] time: 0:23:43.009593\n",
      "[Epoch 16/10000] [Batch 9/36] [D loss: 0.069434, acc:  94%] [G loss: 6.064481] time: 0:23:45.518630\n",
      "[Epoch 16/10000] [Batch 10/36] [D loss: 0.020811, acc:  99%] [G loss: 6.563735] time: 0:23:48.011057\n",
      "[Epoch 16/10000] [Batch 11/36] [D loss: 0.054986, acc:  98%] [G loss: 6.519261] time: 0:23:50.445683\n",
      "[Epoch 16/10000] [Batch 12/36] [D loss: 0.055579, acc:  97%] [G loss: 5.835203] time: 0:23:52.903944\n",
      "[Epoch 16/10000] [Batch 13/36] [D loss: 0.021248, acc:  99%] [G loss: 7.427957] time: 0:23:55.397877\n",
      "[Epoch 16/10000] [Batch 14/36] [D loss: 0.048236, acc:  99%] [G loss: 6.052818] time: 0:23:57.912321\n",
      "[Epoch 16/10000] [Batch 15/36] [D loss: 0.043382, acc:  99%] [G loss: 6.079221] time: 0:24:00.475131\n",
      "[Epoch 16/10000] [Batch 16/36] [D loss: 0.039949, acc:  99%] [G loss: 6.606833] time: 0:24:02.924183\n",
      "[Epoch 16/10000] [Batch 17/36] [D loss: 0.036537, acc:  99%] [G loss: 6.081137] time: 0:24:05.490916\n",
      "[Epoch 16/10000] [Batch 18/36] [D loss: 0.068195, acc:  96%] [G loss: 6.572500] time: 0:24:07.867644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16/10000] [Batch 19/36] [D loss: 0.024286, acc:  99%] [G loss: 6.954523] time: 0:24:10.986428\n",
      "[Epoch 16/10000] [Batch 20/36] [D loss: 0.061207, acc:  98%] [G loss: 5.651855] time: 0:24:13.610860\n",
      "[Epoch 16/10000] [Batch 21/36] [D loss: 0.072225, acc:  95%] [G loss: 5.730158] time: 0:24:16.157025\n",
      "[Epoch 16/10000] [Batch 22/36] [D loss: 0.035235, acc:  99%] [G loss: 6.191455] time: 0:24:18.545058\n",
      "[Epoch 16/10000] [Batch 23/36] [D loss: 0.024928, acc:  99%] [G loss: 6.223861] time: 0:24:20.994767\n",
      "[Epoch 16/10000] [Batch 24/36] [D loss: 0.055505, acc:  99%] [G loss: 6.364295] time: 0:24:23.613877\n",
      "[Epoch 16/10000] [Batch 25/36] [D loss: 0.049348, acc:  99%] [G loss: 6.412890] time: 0:24:26.225559\n",
      "[Epoch 16/10000] [Batch 26/36] [D loss: 0.103454, acc:  94%] [G loss: 5.598829] time: 0:24:28.868700\n",
      "[Epoch 16/10000] [Batch 27/36] [D loss: 0.070966, acc:  95%] [G loss: 6.362854] time: 0:24:31.626645\n",
      "[Epoch 16/10000] [Batch 28/36] [D loss: 0.055651, acc:  96%] [G loss: 6.155929] time: 0:24:34.217511\n",
      "[Epoch 16/10000] [Batch 29/36] [D loss: 0.028201, acc:  99%] [G loss: 6.741499] time: 0:24:36.686388\n",
      "[Epoch 16/10000] [Batch 30/36] [D loss: 0.021746, acc:  99%] [G loss: 5.671340] time: 0:24:39.139230\n",
      "[Epoch 16/10000] [Batch 31/36] [D loss: 0.071861, acc:  95%] [G loss: 6.510084] time: 0:24:41.582330\n",
      "[Epoch 16/10000] [Batch 32/36] [D loss: 0.030343, acc:  99%] [G loss: 6.149166] time: 0:24:44.030336\n",
      "[Epoch 16/10000] [Batch 33/36] [D loss: 0.077851, acc:  98%] [G loss: 5.784223] time: 0:24:46.854923\n",
      "[Epoch 16/10000] [Batch 34/36] [D loss: 0.035262, acc:  99%] [G loss: 6.975290] time: 0:24:49.283457\n",
      "[Epoch 17/10000] [Batch 0/36] [D loss: 0.032395, acc: 100%] [G loss: 6.117770] time: 0:24:51.822075\n",
      "[Epoch 17/10000] [Batch 1/36] [D loss: 0.036321, acc:  99%] [G loss: 6.406051] time: 0:24:55.351311\n",
      "[Epoch 17/10000] [Batch 2/36] [D loss: 0.058035, acc:  99%] [G loss: 5.496637] time: 0:24:57.848927\n",
      "[Epoch 17/10000] [Batch 3/36] [D loss: 0.038626, acc:  99%] [G loss: 5.834500] time: 0:25:00.294416\n",
      "[Epoch 17/10000] [Batch 4/36] [D loss: 0.065415, acc:  97%] [G loss: 5.759395] time: 0:25:02.866372\n",
      "[Epoch 17/10000] [Batch 5/36] [D loss: 0.062993, acc:  97%] [G loss: 6.017943] time: 0:25:05.499157\n",
      "[Epoch 17/10000] [Batch 6/36] [D loss: 0.067187, acc:  94%] [G loss: 5.992372] time: 0:25:08.015139\n",
      "[Epoch 17/10000] [Batch 7/36] [D loss: 0.034096, acc:  99%] [G loss: 6.580523] time: 0:25:11.326678\n",
      "[Epoch 17/10000] [Batch 8/36] [D loss: 0.089126, acc:  95%] [G loss: 5.587841] time: 0:25:13.992109\n",
      "[Epoch 17/10000] [Batch 9/36] [D loss: 0.038973, acc:  99%] [G loss: 5.875609] time: 0:25:16.523561\n",
      "[Epoch 17/10000] [Batch 10/36] [D loss: 0.028933, acc:  99%] [G loss: 6.317310] time: 0:25:18.996328\n",
      "[Epoch 17/10000] [Batch 11/36] [D loss: 0.036429, acc:  99%] [G loss: 6.277547] time: 0:25:21.454847\n",
      "[Epoch 17/10000] [Batch 12/36] [D loss: 0.043699, acc:  99%] [G loss: 5.677951] time: 0:25:23.984129\n",
      "[Epoch 17/10000] [Batch 13/36] [D loss: 0.022368, acc:  99%] [G loss: 7.202383] time: 0:25:26.513831\n",
      "[Epoch 17/10000] [Batch 14/36] [D loss: 0.023198, acc:  99%] [G loss: 5.942623] time: 0:25:28.982181\n",
      "[Epoch 17/10000] [Batch 15/36] [D loss: 0.076616, acc:  95%] [G loss: 6.227188] time: 0:25:31.411035\n",
      "[Epoch 17/10000] [Batch 16/36] [D loss: 0.070431, acc:  96%] [G loss: 6.182408] time: 0:25:33.838180\n",
      "[Epoch 17/10000] [Batch 17/36] [D loss: 0.046821, acc:  99%] [G loss: 5.973624] time: 0:25:36.235545\n",
      "[Epoch 17/10000] [Batch 18/36] [D loss: 0.039729, acc:  99%] [G loss: 6.210192] time: 0:25:38.683914\n",
      "[Epoch 17/10000] [Batch 19/36] [D loss: 0.030835, acc:  99%] [G loss: 7.093174] time: 0:25:41.223709\n",
      "[Epoch 17/10000] [Batch 20/36] [D loss: 0.045478, acc:  99%] [G loss: 5.593225] time: 0:25:43.766774\n",
      "[Epoch 17/10000] [Batch 21/36] [D loss: 0.078948, acc:  96%] [G loss: 5.662982] time: 0:25:46.265559\n",
      "[Epoch 17/10000] [Batch 22/36] [D loss: 0.036280, acc:  99%] [G loss: 6.085444] time: 0:25:48.720534\n",
      "[Epoch 17/10000] [Batch 23/36] [D loss: 0.021908, acc:  99%] [G loss: 6.304212] time: 0:25:51.118033\n",
      "[Epoch 17/10000] [Batch 24/36] [D loss: 0.053273, acc:  99%] [G loss: 6.586982] time: 0:25:53.560104\n",
      "[Epoch 17/10000] [Batch 25/36] [D loss: 0.036071, acc:  99%] [G loss: 6.189223] time: 0:25:55.983936\n",
      "[Epoch 17/10000] [Batch 26/36] [D loss: 0.094109, acc:  90%] [G loss: 5.564728] time: 0:25:58.511502\n",
      "[Epoch 17/10000] [Batch 27/36] [D loss: 0.082282, acc:  95%] [G loss: 5.790524] time: 0:26:01.156213\n",
      "[Epoch 17/10000] [Batch 28/36] [D loss: 0.059258, acc:  97%] [G loss: 6.136067] time: 0:26:03.665171\n",
      "[Epoch 17/10000] [Batch 29/36] [D loss: 0.030232, acc:  99%] [G loss: 6.014881] time: 0:26:06.133471\n",
      "[Epoch 17/10000] [Batch 30/36] [D loss: 0.054449, acc:  99%] [G loss: 5.649778] time: 0:26:08.525940\n",
      "[Epoch 17/10000] [Batch 31/36] [D loss: 0.074803, acc:  94%] [G loss: 6.163612] time: 0:26:11.090301\n",
      "[Epoch 17/10000] [Batch 32/36] [D loss: 0.052135, acc:  98%] [G loss: 5.940991] time: 0:26:13.546204\n",
      "[Epoch 17/10000] [Batch 33/36] [D loss: 0.061468, acc:  98%] [G loss: 5.994278] time: 0:26:16.207717\n",
      "[Epoch 17/10000] [Batch 34/36] [D loss: 0.034189, acc:  99%] [G loss: 6.740841] time: 0:26:18.883463\n",
      "[Epoch 18/10000] [Batch 0/36] [D loss: 0.043584, acc: 100%] [G loss: 6.344834] time: 0:26:21.804442\n",
      "[Epoch 18/10000] [Batch 1/36] [D loss: 0.037856, acc:  99%] [G loss: 6.594712] time: 0:26:24.494399\n",
      "[Epoch 18/10000] [Batch 2/36] [D loss: 0.047225, acc:  99%] [G loss: 5.776776] time: 0:26:26.754426\n",
      "[Epoch 18/10000] [Batch 3/36] [D loss: 0.057416, acc:  98%] [G loss: 6.138193] time: 0:26:28.936112\n",
      "[Epoch 18/10000] [Batch 4/36] [D loss: 0.029055, acc:  99%] [G loss: 5.713041] time: 0:26:31.287196\n",
      "[Epoch 18/10000] [Batch 5/36] [D loss: 0.064840, acc:  98%] [G loss: 5.873789] time: 0:26:33.657874\n",
      "[Epoch 18/10000] [Batch 6/36] [D loss: 0.079724, acc:  92%] [G loss: 6.304742] time: 0:26:35.891109\n",
      "[Epoch 18/10000] [Batch 7/36] [D loss: 0.045233, acc:  99%] [G loss: 6.619898] time: 0:26:38.154214\n",
      "[Epoch 18/10000] [Batch 8/36] [D loss: 0.066815, acc:  97%] [G loss: 5.577953] time: 0:26:40.441915\n",
      "[Epoch 18/10000] [Batch 9/36] [D loss: 0.066135, acc:  96%] [G loss: 5.750544] time: 0:26:42.633847\n",
      "[Epoch 18/10000] [Batch 10/36] [D loss: 0.024684, acc:  99%] [G loss: 6.199103] time: 0:26:45.108766\n",
      "[Epoch 18/10000] [Batch 11/36] [D loss: 0.089399, acc:  94%] [G loss: 6.121586] time: 0:26:47.493892\n",
      "[Epoch 18/10000] [Batch 12/36] [D loss: 0.062384, acc:  96%] [G loss: 5.732068] time: 0:26:49.825675\n",
      "[Epoch 18/10000] [Batch 13/36] [D loss: 0.032417, acc:  99%] [G loss: 7.419703] time: 0:26:52.044887\n",
      "[Epoch 18/10000] [Batch 14/36] [D loss: 0.026519, acc:  99%] [G loss: 5.949502] time: 0:26:54.337647\n",
      "[Epoch 18/10000] [Batch 15/36] [D loss: 0.036492, acc:  99%] [G loss: 6.316335] time: 0:26:56.710468\n",
      "[Epoch 18/10000] [Batch 16/36] [D loss: 0.041498, acc:  99%] [G loss: 6.001209] time: 0:26:58.942683\n",
      "[Epoch 18/10000] [Batch 17/36] [D loss: 0.022279, acc:  99%] [G loss: 5.861424] time: 0:27:01.233690\n",
      "[Epoch 18/10000] [Batch 18/36] [D loss: 0.019143, acc:  99%] [G loss: 6.300500] time: 0:27:03.484509\n",
      "[Epoch 18/10000] [Batch 19/36] [D loss: 0.041381, acc:  98%] [G loss: 6.965277] time: 0:27:05.864036\n",
      "[Epoch 18/10000] [Batch 20/36] [D loss: 0.042166, acc:  99%] [G loss: 5.585003] time: 0:27:08.146564\n",
      "[Epoch 18/10000] [Batch 21/36] [D loss: 0.067092, acc:  96%] [G loss: 5.626593] time: 0:27:10.467124\n",
      "[Epoch 18/10000] [Batch 22/36] [D loss: 0.045263, acc:  99%] [G loss: 6.218102] time: 0:27:12.797611\n",
      "[Epoch 18/10000] [Batch 23/36] [D loss: 0.036877, acc:  99%] [G loss: 6.087571] time: 0:27:15.052511\n",
      "[Epoch 18/10000] [Batch 24/36] [D loss: 0.037119, acc:  99%] [G loss: 7.104150] time: 0:27:17.332761\n",
      "[Epoch 18/10000] [Batch 25/36] [D loss: 0.039366, acc:  99%] [G loss: 6.457072] time: 0:27:19.643202\n",
      "[Epoch 18/10000] [Batch 26/36] [D loss: 0.090016, acc:  93%] [G loss: 5.583464] time: 0:27:21.894788\n",
      "[Epoch 18/10000] [Batch 27/36] [D loss: 0.102325, acc:  93%] [G loss: 5.869157] time: 0:27:24.175936\n",
      "[Epoch 18/10000] [Batch 28/36] [D loss: 0.042794, acc:  99%] [G loss: 6.553166] time: 0:27:26.497299\n",
      "[Epoch 18/10000] [Batch 29/36] [D loss: 0.020946, acc:  99%] [G loss: 6.130075] time: 0:27:28.873167\n",
      "[Epoch 18/10000] [Batch 30/36] [D loss: 0.026252, acc: 100%] [G loss: 5.638705] time: 0:27:31.656466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18/10000] [Batch 31/36] [D loss: 0.054941, acc:  98%] [G loss: 6.155239] time: 0:27:34.061693\n",
      "[Epoch 18/10000] [Batch 32/36] [D loss: 0.026440, acc:  99%] [G loss: 6.009500] time: 0:27:36.429579\n",
      "[Epoch 18/10000] [Batch 33/36] [D loss: 0.086313, acc:  97%] [G loss: 5.435231] time: 0:27:39.160267\n",
      "[Epoch 18/10000] [Batch 34/36] [D loss: 0.048168, acc:  98%] [G loss: 7.043334] time: 0:27:41.571530\n",
      "[Epoch 19/10000] [Batch 0/36] [D loss: 0.042341, acc: 100%] [G loss: 6.095004] time: 0:27:43.863147\n",
      "[Epoch 19/10000] [Batch 1/36] [D loss: 0.039472, acc:  99%] [G loss: 6.809568] time: 0:27:46.469559\n",
      "[Epoch 19/10000] [Batch 2/36] [D loss: 0.033429, acc:  99%] [G loss: 5.558965] time: 0:27:48.803766\n",
      "[Epoch 19/10000] [Batch 3/36] [D loss: 0.067304, acc:  97%] [G loss: 6.202133] time: 0:27:51.104072\n",
      "[Epoch 19/10000] [Batch 4/36] [D loss: 0.034249, acc:  99%] [G loss: 5.908873] time: 0:27:53.415083\n",
      "[Epoch 19/10000] [Batch 5/36] [D loss: 0.041231, acc:  99%] [G loss: 5.854717] time: 0:27:55.685705\n",
      "[Epoch 19/10000] [Batch 6/36] [D loss: 0.042373, acc:  99%] [G loss: 5.918598] time: 0:27:57.969082\n",
      "[Epoch 19/10000] [Batch 7/36] [D loss: 0.028164, acc:  99%] [G loss: 6.199275] time: 0:28:00.327793\n",
      "[Epoch 19/10000] [Batch 8/36] [D loss: 0.049347, acc:  99%] [G loss: 5.639412] time: 0:28:02.671592\n",
      "[Epoch 19/10000] [Batch 9/36] [D loss: 0.044427, acc:  99%] [G loss: 5.840854] time: 0:28:05.034434\n",
      "[Epoch 19/10000] [Batch 10/36] [D loss: 0.024902, acc:  99%] [G loss: 6.814127] time: 0:28:07.336979\n",
      "[Epoch 19/10000] [Batch 11/36] [D loss: 0.047413, acc:  98%] [G loss: 5.935109] time: 0:28:09.545259\n",
      "[Epoch 19/10000] [Batch 12/36] [D loss: 0.051913, acc:  99%] [G loss: 5.853594] time: 0:28:11.915796\n",
      "[Epoch 19/10000] [Batch 13/36] [D loss: 0.023042, acc:  99%] [G loss: 6.976766] time: 0:28:14.237512\n",
      "[Epoch 19/10000] [Batch 14/36] [D loss: 0.035203, acc:  99%] [G loss: 6.025679] time: 0:28:16.541338\n",
      "[Epoch 19/10000] [Batch 15/36] [D loss: 0.036665, acc:  99%] [G loss: 6.326437] time: 0:28:18.784200\n",
      "[Epoch 19/10000] [Batch 16/36] [D loss: 0.076108, acc:  97%] [G loss: 6.737482] time: 0:28:21.005609\n",
      "[Epoch 19/10000] [Batch 17/36] [D loss: 0.033142, acc:  99%] [G loss: 5.859625] time: 0:28:23.365939\n",
      "[Epoch 19/10000] [Batch 18/36] [D loss: 0.041152, acc:  99%] [G loss: 6.607983] time: 0:28:25.735814\n",
      "[Epoch 19/10000] [Batch 19/36] [D loss: 0.031955, acc:  99%] [G loss: 7.275295] time: 0:28:28.069427\n",
      "[Epoch 19/10000] [Batch 20/36] [D loss: 0.027909, acc:  99%] [G loss: 5.683789] time: 0:28:30.339750\n",
      "[Epoch 19/10000] [Batch 21/36] [D loss: 0.059118, acc:  96%] [G loss: 5.619807] time: 0:28:32.608630\n",
      "[Epoch 19/10000] [Batch 22/36] [D loss: 0.030015, acc:  99%] [G loss: 6.030333] time: 0:28:34.831577\n",
      "[Epoch 19/10000] [Batch 23/36] [D loss: 0.070896, acc:  95%] [G loss: 6.565779] time: 0:28:37.094506\n",
      "[Epoch 19/10000] [Batch 24/36] [D loss: 0.056623, acc:  98%] [G loss: 6.705006] time: 0:28:39.530947\n",
      "[Epoch 19/10000] [Batch 25/36] [D loss: 0.041472, acc:  99%] [G loss: 6.908043] time: 0:28:42.092592\n",
      "[Epoch 19/10000] [Batch 26/36] [D loss: 0.085052, acc:  97%] [G loss: 5.849362] time: 0:28:44.553558\n",
      "[Epoch 19/10000] [Batch 27/36] [D loss: 0.076748, acc:  97%] [G loss: 5.934708] time: 0:28:47.040898\n",
      "[Epoch 19/10000] [Batch 28/36] [D loss: 0.059102, acc:  97%] [G loss: 5.928463] time: 0:28:49.353810\n",
      "[Epoch 19/10000] [Batch 29/36] [D loss: 0.021174, acc:  99%] [G loss: 6.746140] time: 0:28:51.996129\n",
      "[Epoch 19/10000] [Batch 30/36] [D loss: 0.040293, acc:  99%] [G loss: 5.519554] time: 0:28:54.515663\n",
      "[Epoch 19/10000] [Batch 31/36] [D loss: 0.070003, acc:  95%] [G loss: 6.123524] time: 0:28:56.977766\n",
      "[Epoch 19/10000] [Batch 32/36] [D loss: 0.041560, acc:  99%] [G loss: 6.086829] time: 0:28:59.381637\n",
      "[Epoch 19/10000] [Batch 33/36] [D loss: 0.056744, acc:  98%] [G loss: 5.543289] time: 0:29:02.254131\n",
      "[Epoch 19/10000] [Batch 34/36] [D loss: 0.031496, acc:  99%] [G loss: 6.938939] time: 0:29:04.703010\n",
      "[Epoch 20/10000] [Batch 0/36] [D loss: 0.024037, acc:  99%] [G loss: 6.310437] time: 0:29:07.207365\n",
      "[Epoch 20/10000] [Batch 1/36] [D loss: 0.032180, acc:  99%] [G loss: 6.357648] time: 0:29:10.070311\n",
      "[Epoch 20/10000] [Batch 2/36] [D loss: 0.024870, acc:  99%] [G loss: 5.157183] time: 0:29:12.537612\n",
      "[Epoch 20/10000] [Batch 3/36] [D loss: 0.042676, acc:  99%] [G loss: 6.470568] time: 0:29:14.968932\n",
      "[Epoch 20/10000] [Batch 4/36] [D loss: 0.065915, acc:  95%] [G loss: 5.507481] time: 0:29:17.494839\n",
      "[Epoch 20/10000] [Batch 5/36] [D loss: 0.048225, acc:  98%] [G loss: 6.074689] time: 0:29:20.019266\n",
      "[Epoch 20/10000] [Batch 6/36] [D loss: 0.070209, acc:  95%] [G loss: 5.930820] time: 0:29:22.547716\n",
      "[Epoch 20/10000] [Batch 7/36] [D loss: 0.044685, acc:  99%] [G loss: 6.660979] time: 0:29:25.033675\n",
      "[Epoch 20/10000] [Batch 8/36] [D loss: 0.056823, acc:  99%] [G loss: 5.473573] time: 0:29:27.379492\n",
      "[Epoch 20/10000] [Batch 9/36] [D loss: 0.040411, acc:  99%] [G loss: 6.054251] time: 0:29:29.766426\n",
      "[Epoch 20/10000] [Batch 10/36] [D loss: 0.028002, acc:  99%] [G loss: 6.271908] time: 0:29:32.286058\n",
      "[Epoch 20/10000] [Batch 11/36] [D loss: 0.073529, acc:  95%] [G loss: 6.788905] time: 0:29:34.667423\n",
      "[Epoch 20/10000] [Batch 12/36] [D loss: 0.060673, acc:  98%] [G loss: 5.553279] time: 0:29:37.028716\n",
      "[Epoch 20/10000] [Batch 13/36] [D loss: 0.047109, acc:  99%] [G loss: 7.365146] time: 0:29:39.462197\n",
      "[Epoch 20/10000] [Batch 14/36] [D loss: 0.057871, acc:  97%] [G loss: 6.089530] time: 0:29:41.780220\n",
      "[Epoch 20/10000] [Batch 15/36] [D loss: 0.021589, acc:  99%] [G loss: 6.199295] time: 0:29:44.202848\n",
      "[Epoch 20/10000] [Batch 16/36] [D loss: 0.049718, acc:  99%] [G loss: 6.991933] time: 0:29:46.582654\n",
      "[Epoch 20/10000] [Batch 17/36] [D loss: 0.032499, acc:  99%] [G loss: 6.178077] time: 0:29:48.995270\n",
      "[Epoch 20/10000] [Batch 18/36] [D loss: 0.049985, acc:  99%] [G loss: 6.527328] time: 0:29:51.438929\n",
      "[Epoch 20/10000] [Batch 19/36] [D loss: 0.025682, acc:  99%] [G loss: 7.178766] time: 0:29:54.014249\n",
      "[Epoch 20/10000] [Batch 20/36] [D loss: 0.062368, acc:  98%] [G loss: 5.712042] time: 0:29:56.604023\n",
      "[Epoch 20/10000] [Batch 21/36] [D loss: 0.052714, acc:  99%] [G loss: 5.657247] time: 0:29:59.466640\n",
      "[Epoch 20/10000] [Batch 22/36] [D loss: 0.044928, acc:  99%] [G loss: 6.205000] time: 0:30:01.929234\n",
      "[Epoch 20/10000] [Batch 23/36] [D loss: 0.036743, acc:  99%] [G loss: 6.249054] time: 0:30:04.362571\n",
      "[Epoch 20/10000] [Batch 24/36] [D loss: 0.072093, acc:  97%] [G loss: 6.660813] time: 0:30:06.922300\n",
      "[Epoch 20/10000] [Batch 25/36] [D loss: 0.043888, acc:  99%] [G loss: 7.190423] time: 0:30:09.441698\n",
      "[Epoch 20/10000] [Batch 26/36] [D loss: 0.081133, acc:  95%] [G loss: 6.225069] time: 0:30:11.920586\n",
      "[Epoch 20/10000] [Batch 27/36] [D loss: 0.084380, acc:  95%] [G loss: 6.397252] time: 0:30:14.582141\n",
      "[Epoch 20/10000] [Batch 28/36] [D loss: 0.065207, acc:  97%] [G loss: 6.503971] time: 0:30:17.751219\n",
      "[Epoch 20/10000] [Batch 29/36] [D loss: 0.021737, acc:  99%] [G loss: 6.632617] time: 0:30:20.255248\n",
      "[Epoch 20/10000] [Batch 30/36] [D loss: 0.040400, acc:  99%] [G loss: 6.030335] time: 0:30:22.673451\n",
      "[Epoch 20/10000] [Batch 31/36] [D loss: 0.056099, acc:  97%] [G loss: 6.100676] time: 0:30:25.096112\n",
      "[Epoch 20/10000] [Batch 32/36] [D loss: 0.041684, acc:  99%] [G loss: 6.148921] time: 0:30:27.544364\n",
      "[Epoch 20/10000] [Batch 33/36] [D loss: 0.091889, acc:  95%] [G loss: 6.329094] time: 0:30:30.327662\n",
      "[Epoch 20/10000] [Batch 34/36] [D loss: 0.055470, acc:  97%] [G loss: 7.443105] time: 0:30:32.846971\n",
      "[Epoch 21/10000] [Batch 0/36] [D loss: 0.062407, acc: 100%] [G loss: 5.947618] time: 0:30:35.356054\n",
      "[Epoch 21/10000] [Batch 1/36] [D loss: 0.045796, acc:  99%] [G loss: 6.544577] time: 0:30:38.078561\n",
      "[Epoch 21/10000] [Batch 2/36] [D loss: 0.042499, acc:  99%] [G loss: 5.215199] time: 0:30:40.506845\n",
      "[Epoch 21/10000] [Batch 3/36] [D loss: 0.072325, acc:  94%] [G loss: 6.018737] time: 0:30:42.954769\n",
      "[Epoch 21/10000] [Batch 4/36] [D loss: 0.052760, acc:  97%] [G loss: 5.767283] time: 0:30:45.423271\n",
      "[Epoch 21/10000] [Batch 5/36] [D loss: 0.075378, acc:  96%] [G loss: 5.887265] time: 0:30:47.963336\n",
      "[Epoch 21/10000] [Batch 6/36] [D loss: 0.113170, acc:  84%] [G loss: 6.348241] time: 0:30:50.455409\n",
      "[Epoch 21/10000] [Batch 7/36] [D loss: 0.046738, acc:  99%] [G loss: 6.263677] time: 0:30:52.943703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21/10000] [Batch 8/36] [D loss: 0.088251, acc:  94%] [G loss: 5.508676] time: 0:30:55.234317\n",
      "[Epoch 21/10000] [Batch 9/36] [D loss: 0.062076, acc:  97%] [G loss: 5.767185] time: 0:30:57.552836\n",
      "[Epoch 21/10000] [Batch 10/36] [D loss: 0.034876, acc:  99%] [G loss: 6.182635] time: 0:30:59.919673\n",
      "[Epoch 21/10000] [Batch 11/36] [D loss: 0.050094, acc:  98%] [G loss: 6.259566] time: 0:31:02.276434\n",
      "[Epoch 21/10000] [Batch 12/36] [D loss: 0.058849, acc:  98%] [G loss: 5.856182] time: 0:31:04.613099\n",
      "[Epoch 21/10000] [Batch 13/36] [D loss: 0.040024, acc:  99%] [G loss: 7.415048] time: 0:31:07.015446\n",
      "[Epoch 21/10000] [Batch 14/36] [D loss: 0.039196, acc:  99%] [G loss: 5.912665] time: 0:31:09.367507\n",
      "[Epoch 21/10000] [Batch 15/36] [D loss: 0.038789, acc:  99%] [G loss: 5.930072] time: 0:31:11.754615\n",
      "[Epoch 21/10000] [Batch 16/36] [D loss: 0.113538, acc:  90%] [G loss: 6.411419] time: 0:31:14.104219\n",
      "[Epoch 21/10000] [Batch 17/36] [D loss: 0.058600, acc:  99%] [G loss: 6.168998] time: 0:31:16.522244\n",
      "[Epoch 21/10000] [Batch 18/36] [D loss: 0.062814, acc:  97%] [G loss: 6.390064] time: 0:31:18.817591\n",
      "[Epoch 21/10000] [Batch 19/36] [D loss: 0.032349, acc:  99%] [G loss: 7.141659] time: 0:31:21.153812\n",
      "[Epoch 21/10000] [Batch 20/36] [D loss: 0.047155, acc:  98%] [G loss: 5.846043] time: 0:31:23.477877\n",
      "[Epoch 21/10000] [Batch 21/36] [D loss: 0.041497, acc:  99%] [G loss: 5.537122] time: 0:31:25.763654\n",
      "[Epoch 21/10000] [Batch 22/36] [D loss: 0.020752, acc:  99%] [G loss: 6.206126] time: 0:31:28.110172\n",
      "[Epoch 21/10000] [Batch 23/36] [D loss: 0.076661, acc:  94%] [G loss: 6.262706] time: 0:31:30.460065\n",
      "[Epoch 21/10000] [Batch 24/36] [D loss: 0.059432, acc:  98%] [G loss: 6.531798] time: 0:31:32.796240\n",
      "[Epoch 21/10000] [Batch 25/36] [D loss: 0.031308, acc: 100%] [G loss: 6.944245] time: 0:31:35.139550\n",
      "[Epoch 21/10000] [Batch 26/36] [D loss: 0.077213, acc:  97%] [G loss: 5.933098] time: 0:31:37.476201\n",
      "[Epoch 21/10000] [Batch 27/36] [D loss: 0.074744, acc:  95%] [G loss: 5.899134] time: 0:31:39.812866\n",
      "[Epoch 21/10000] [Batch 28/36] [D loss: 0.042285, acc:  99%] [G loss: 6.113363] time: 0:31:42.088095\n",
      "[Epoch 21/10000] [Batch 29/36] [D loss: 0.038100, acc:  99%] [G loss: 6.097471] time: 0:31:44.450163\n",
      "[Epoch 21/10000] [Batch 30/36] [D loss: 0.022966, acc: 100%] [G loss: 5.580229] time: 0:31:46.730809\n",
      "[Epoch 21/10000] [Batch 31/36] [D loss: 0.047894, acc:  99%] [G loss: 6.142780] time: 0:31:49.067422\n",
      "[Epoch 21/10000] [Batch 32/36] [D loss: 0.030393, acc:  99%] [G loss: 5.924118] time: 0:31:51.305213\n",
      "[Epoch 21/10000] [Batch 33/36] [D loss: 0.065040, acc:  98%] [G loss: 5.772306] time: 0:31:53.986459\n",
      "[Epoch 21/10000] [Batch 34/36] [D loss: 0.024239, acc:  99%] [G loss: 7.502001] time: 0:31:56.472788\n",
      "[Epoch 22/10000] [Batch 0/36] [D loss: 0.035616, acc:  99%] [G loss: 5.939426] time: 0:31:58.915668\n",
      "[Epoch 22/10000] [Batch 1/36] [D loss: 0.035174, acc:  99%] [G loss: 6.582999] time: 0:32:01.780422\n",
      "[Epoch 22/10000] [Batch 2/36] [D loss: 0.054394, acc:  98%] [G loss: 5.219689] time: 0:32:04.243968\n",
      "[Epoch 22/10000] [Batch 3/36] [D loss: 0.059096, acc:  96%] [G loss: 6.212994] time: 0:32:06.694719\n",
      "[Epoch 22/10000] [Batch 4/36] [D loss: 0.033017, acc:  99%] [G loss: 5.859545] time: 0:32:09.006601\n",
      "[Epoch 22/10000] [Batch 5/36] [D loss: 0.051260, acc:  99%] [G loss: 6.516627] time: 0:32:11.497300\n",
      "[Epoch 22/10000] [Batch 6/36] [D loss: 0.077150, acc:  92%] [G loss: 6.005014] time: 0:32:14.154110\n",
      "[Epoch 22/10000] [Batch 7/36] [D loss: 0.032409, acc:  99%] [G loss: 6.525129] time: 0:32:16.995069\n",
      "[Epoch 22/10000] [Batch 8/36] [D loss: 0.074057, acc:  97%] [G loss: 5.716119] time: 0:32:19.602179\n",
      "[Epoch 22/10000] [Batch 9/36] [D loss: 0.085735, acc:  94%] [G loss: 5.746746] time: 0:32:22.111368\n",
      "[Epoch 22/10000] [Batch 10/36] [D loss: 0.030233, acc:  99%] [G loss: 6.182558] time: 0:32:24.604208\n",
      "[Epoch 22/10000] [Batch 11/36] [D loss: 0.088638, acc:  90%] [G loss: 6.231194] time: 0:32:27.147612\n",
      "[Epoch 22/10000] [Batch 12/36] [D loss: 0.064160, acc:  98%] [G loss: 5.677618] time: 0:32:29.821821\n",
      "[Epoch 22/10000] [Batch 13/36] [D loss: 0.035231, acc:  99%] [G loss: 6.757432] time: 0:32:32.259725\n",
      "[Epoch 22/10000] [Batch 14/36] [D loss: 0.058199, acc:  98%] [G loss: 5.970108] time: 0:32:34.687436\n",
      "[Epoch 22/10000] [Batch 15/36] [D loss: 0.028903, acc:  99%] [G loss: 6.142964] time: 0:32:37.176249\n",
      "[Epoch 22/10000] [Batch 16/36] [D loss: 0.061704, acc:  98%] [G loss: 6.309399] time: 0:32:39.670177\n",
      "[Epoch 22/10000] [Batch 17/36] [D loss: 0.036082, acc: 100%] [G loss: 5.975735] time: 0:32:42.088688\n",
      "[Epoch 22/10000] [Batch 18/36] [D loss: 0.066297, acc:  96%] [G loss: 6.471102] time: 0:32:44.485727\n",
      "[Epoch 22/10000] [Batch 19/36] [D loss: 0.033793, acc:  99%] [G loss: 7.126327] time: 0:32:46.898252\n",
      "[Epoch 22/10000] [Batch 20/36] [D loss: 0.066119, acc:  97%] [G loss: 5.677597] time: 0:32:49.368262\n",
      "[Epoch 22/10000] [Batch 21/36] [D loss: 0.075384, acc:  95%] [G loss: 5.808913] time: 0:32:51.804889\n",
      "[Epoch 22/10000] [Batch 22/36] [D loss: 0.036703, acc:  99%] [G loss: 5.890780] time: 0:32:54.212354\n",
      "[Epoch 22/10000] [Batch 23/36] [D loss: 0.028845, acc:  99%] [G loss: 6.134312] time: 0:32:56.592916\n",
      "[Epoch 22/10000] [Batch 24/36] [D loss: 0.069156, acc:  97%] [G loss: 6.605064] time: 0:32:59.068320\n",
      "[Epoch 22/10000] [Batch 25/36] [D loss: 0.046571, acc: 100%] [G loss: 6.461494] time: 0:33:01.489843\n",
      "[Epoch 22/10000] [Batch 26/36] [D loss: 0.112043, acc:  91%] [G loss: 5.852446] time: 0:33:03.924327\n",
      "[Epoch 22/10000] [Batch 27/36] [D loss: 0.081427, acc:  94%] [G loss: 5.889478] time: 0:33:06.321507\n",
      "[Epoch 22/10000] [Batch 28/36] [D loss: 0.081816, acc:  90%] [G loss: 6.058894] time: 0:33:08.753052\n",
      "[Epoch 22/10000] [Batch 29/36] [D loss: 0.023930, acc:  99%] [G loss: 6.127217] time: 0:33:11.238220\n",
      "[Epoch 22/10000] [Batch 30/36] [D loss: 0.025409, acc:  99%] [G loss: 5.392156] time: 0:33:13.701790\n",
      "[Epoch 22/10000] [Batch 31/36] [D loss: 0.076884, acc:  96%] [G loss: 6.073482] time: 0:33:16.138663\n",
      "[Epoch 22/10000] [Batch 32/36] [D loss: 0.038144, acc:  99%] [G loss: 6.081457] time: 0:33:18.557709\n",
      "[Epoch 22/10000] [Batch 33/36] [D loss: 0.067412, acc:  98%] [G loss: 5.670190] time: 0:33:21.356547\n",
      "[Epoch 22/10000] [Batch 34/36] [D loss: 0.025153, acc:  99%] [G loss: 7.321141] time: 0:33:23.804761\n",
      "[Epoch 23/10000] [Batch 0/36] [D loss: 0.030387, acc: 100%] [G loss: 5.958148] time: 0:33:26.851930\n",
      "[Epoch 23/10000] [Batch 1/36] [D loss: 0.031493, acc:  99%] [G loss: 6.151013] time: 0:33:29.686225\n",
      "[Epoch 23/10000] [Batch 2/36] [D loss: 0.030572, acc:  99%] [G loss: 5.222252] time: 0:33:32.246480\n",
      "[Epoch 23/10000] [Batch 3/36] [D loss: 0.056428, acc:  98%] [G loss: 6.171823] time: 0:33:34.699536\n",
      "[Epoch 23/10000] [Batch 4/36] [D loss: 0.050982, acc:  97%] [G loss: 5.560680] time: 0:33:37.132529\n",
      "[Epoch 23/10000] [Batch 5/36] [D loss: 0.071882, acc:  97%] [G loss: 6.044593] time: 0:33:39.591077\n",
      "[Epoch 23/10000] [Batch 6/36] [D loss: 0.072956, acc:  95%] [G loss: 6.133461] time: 0:33:42.215763\n",
      "[Epoch 23/10000] [Batch 7/36] [D loss: 0.023313, acc:  99%] [G loss: 6.344914] time: 0:33:44.701005\n",
      "[Epoch 23/10000] [Batch 8/36] [D loss: 0.042370, acc:  99%] [G loss: 5.759697] time: 0:33:47.315466\n",
      "[Epoch 23/10000] [Batch 9/36] [D loss: 0.066557, acc:  96%] [G loss: 5.564842] time: 0:33:49.854647\n",
      "[Epoch 23/10000] [Batch 10/36] [D loss: 0.025572, acc:  99%] [G loss: 6.204746] time: 0:33:52.268917\n",
      "[Epoch 23/10000] [Batch 11/36] [D loss: 0.096518, acc:  92%] [G loss: 6.431337] time: 0:33:54.671702\n",
      "[Epoch 23/10000] [Batch 12/36] [D loss: 0.049735, acc:  98%] [G loss: 5.823362] time: 0:33:57.194569\n",
      "[Epoch 23/10000] [Batch 13/36] [D loss: 0.022938, acc:  99%] [G loss: 7.322030] time: 0:33:59.776005\n",
      "[Epoch 23/10000] [Batch 14/36] [D loss: 0.045533, acc:  99%] [G loss: 5.827589] time: 0:34:02.870632\n",
      "[Epoch 23/10000] [Batch 15/36] [D loss: 0.035780, acc:  99%] [G loss: 6.203216] time: 0:34:05.344198\n",
      "[Epoch 23/10000] [Batch 16/36] [D loss: 0.044098, acc:  99%] [G loss: 6.212951] time: 0:34:07.827844\n",
      "[Epoch 23/10000] [Batch 17/36] [D loss: 0.028077, acc:  99%] [G loss: 5.871332] time: 0:34:10.316780\n",
      "[Epoch 23/10000] [Batch 18/36] [D loss: 0.055305, acc:  98%] [G loss: 6.401541] time: 0:34:13.100408\n",
      "[Epoch 23/10000] [Batch 19/36] [D loss: 0.034015, acc:  99%] [G loss: 7.152528] time: 0:34:15.703560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/10000] [Batch 20/36] [D loss: 0.051246, acc:  99%] [G loss: 5.478739] time: 0:34:18.284721\n",
      "[Epoch 23/10000] [Batch 21/36] [D loss: 0.097762, acc:  94%] [G loss: 5.506697] time: 0:34:20.800470\n",
      "[Epoch 23/10000] [Batch 22/36] [D loss: 0.023591, acc:  99%] [G loss: 6.074990] time: 0:34:23.319999\n",
      "[Epoch 23/10000] [Batch 23/36] [D loss: 0.043079, acc:  99%] [G loss: 6.054330] time: 0:34:25.839190\n",
      "[Epoch 23/10000] [Batch 24/36] [D loss: 0.040359, acc:  99%] [G loss: 6.805949] time: 0:34:28.331877\n",
      "[Epoch 23/10000] [Batch 25/36] [D loss: 0.029640, acc:  99%] [G loss: 6.353472] time: 0:34:30.745683\n",
      "[Epoch 23/10000] [Batch 26/36] [D loss: 0.099876, acc:  93%] [G loss: 5.656888] time: 0:34:33.187567\n",
      "[Epoch 23/10000] [Batch 27/36] [D loss: 0.066565, acc:  97%] [G loss: 5.935952] time: 0:34:35.748630\n",
      "[Epoch 23/10000] [Batch 28/36] [D loss: 0.050930, acc:  97%] [G loss: 5.780408] time: 0:34:38.212223\n",
      "[Epoch 23/10000] [Batch 29/36] [D loss: 0.019433, acc:  99%] [G loss: 6.406806] time: 0:34:40.644007\n",
      "[Epoch 23/10000] [Batch 30/36] [D loss: 0.034000, acc:  99%] [G loss: 5.777555] time: 0:34:43.093579\n",
      "[Epoch 23/10000] [Batch 31/36] [D loss: 0.081731, acc:  93%] [G loss: 5.876869] time: 0:34:45.678917\n",
      "[Epoch 23/10000] [Batch 32/36] [D loss: 0.036626, acc:  99%] [G loss: 5.983090] time: 0:34:48.231674\n",
      "[Epoch 23/10000] [Batch 33/36] [D loss: 0.085531, acc:  95%] [G loss: 5.993791] time: 0:34:51.053449\n",
      "[Epoch 23/10000] [Batch 34/36] [D loss: 0.029307, acc:  99%] [G loss: 6.884091] time: 0:34:53.541463\n",
      "[Epoch 24/10000] [Batch 0/36] [D loss: 0.049286, acc:  99%] [G loss: 6.340217] time: 0:34:56.609765\n",
      "[Epoch 24/10000] [Batch 1/36] [D loss: 0.021858, acc:  99%] [G loss: 6.400182] time: 0:34:59.372940\n",
      "[Epoch 24/10000] [Batch 2/36] [D loss: 0.028084, acc:  99%] [G loss: 5.316027] time: 0:35:01.810796\n",
      "[Epoch 24/10000] [Batch 3/36] [D loss: 0.041367, acc:  99%] [G loss: 6.000471] time: 0:35:04.228574\n",
      "[Epoch 24/10000] [Batch 4/36] [D loss: 0.046432, acc:  98%] [G loss: 5.665420] time: 0:35:06.656366\n",
      "[Epoch 24/10000] [Batch 5/36] [D loss: 0.054209, acc:  98%] [G loss: 5.660833] time: 0:35:09.205272\n",
      "[Epoch 24/10000] [Batch 6/36] [D loss: 0.053624, acc:  97%] [G loss: 6.209274] time: 0:35:11.725465\n",
      "[Epoch 24/10000] [Batch 7/36] [D loss: 0.030798, acc:  99%] [G loss: 6.575677] time: 0:35:14.153798\n",
      "[Epoch 24/10000] [Batch 8/36] [D loss: 0.053760, acc:  99%] [G loss: 5.478336] time: 0:35:16.571228\n",
      "[Epoch 24/10000] [Batch 9/36] [D loss: 0.038380, acc:  99%] [G loss: 5.972859] time: 0:35:19.040065\n",
      "[Epoch 24/10000] [Batch 10/36] [D loss: 0.022361, acc:  99%] [G loss: 6.587695] time: 0:35:21.492190\n",
      "[Epoch 24/10000] [Batch 11/36] [D loss: 0.054827, acc:  98%] [G loss: 5.916306] time: 0:35:23.971945\n",
      "[Epoch 24/10000] [Batch 12/36] [D loss: 0.031738, acc:  99%] [G loss: 6.563633] time: 0:35:26.459332\n",
      "[Epoch 24/10000] [Batch 13/36] [D loss: 0.022314, acc:  99%] [G loss: 7.237224] time: 0:35:28.958293\n",
      "[Epoch 24/10000] [Batch 14/36] [D loss: 0.072882, acc:  96%] [G loss: 5.779674] time: 0:35:31.372557\n",
      "[Epoch 24/10000] [Batch 15/36] [D loss: 0.048755, acc:  98%] [G loss: 5.925307] time: 0:35:33.830862\n",
      "[Epoch 24/10000] [Batch 16/36] [D loss: 0.058257, acc:  97%] [G loss: 6.731669] time: 0:35:36.308795\n",
      "[Epoch 24/10000] [Batch 17/36] [D loss: 0.035487, acc:  99%] [G loss: 5.604227] time: 0:35:38.938838\n",
      "[Epoch 24/10000] [Batch 18/36] [D loss: 0.039771, acc:  99%] [G loss: 6.324642] time: 0:35:41.690165\n",
      "[Epoch 24/10000] [Batch 19/36] [D loss: 0.023482, acc:  99%] [G loss: 7.434120] time: 0:35:44.248560\n",
      "[Epoch 24/10000] [Batch 20/36] [D loss: 0.028680, acc:  99%] [G loss: 5.618587] time: 0:35:46.812649\n",
      "[Epoch 24/10000] [Batch 21/36] [D loss: 0.051887, acc:  98%] [G loss: 5.515618] time: 0:35:49.404864\n",
      "[Epoch 24/10000] [Batch 22/36] [D loss: 0.033218, acc:  99%] [G loss: 6.381281] time: 0:35:51.909228\n",
      "[Epoch 24/10000] [Batch 23/36] [D loss: 0.043517, acc:  99%] [G loss: 5.902569] time: 0:35:54.569875\n",
      "[Epoch 24/10000] [Batch 24/36] [D loss: 0.043146, acc:  99%] [G loss: 6.331752] time: 0:35:57.074926\n",
      "[Epoch 24/10000] [Batch 25/36] [D loss: 0.077067, acc: 100%] [G loss: 6.772002] time: 0:36:00.299048\n",
      "[Epoch 24/10000] [Batch 26/36] [D loss: 0.061971, acc:  98%] [G loss: 5.536838] time: 0:36:02.814317\n",
      "[Epoch 24/10000] [Batch 27/36] [D loss: 0.077686, acc:  96%] [G loss: 5.691979] time: 0:36:05.404911\n",
      "[Epoch 24/10000] [Batch 28/36] [D loss: 0.045626, acc:  97%] [G loss: 5.989077] time: 0:36:07.965037\n",
      "[Epoch 24/10000] [Batch 29/36] [D loss: 0.029761, acc:  99%] [G loss: 6.103941] time: 0:36:10.396970\n",
      "[Epoch 24/10000] [Batch 30/36] [D loss: 0.028838, acc:  99%] [G loss: 5.537271] time: 0:36:12.966845\n",
      "[Epoch 24/10000] [Batch 31/36] [D loss: 0.057767, acc:  97%] [G loss: 5.914258] time: 0:36:15.521441\n",
      "[Epoch 24/10000] [Batch 32/36] [D loss: 0.025414, acc:  99%] [G loss: 5.807899] time: 0:36:17.997772\n",
      "[Epoch 24/10000] [Batch 33/36] [D loss: 0.091257, acc:  94%] [G loss: 5.431178] time: 0:36:20.679642\n",
      "[Epoch 24/10000] [Batch 34/36] [D loss: 0.051442, acc:  98%] [G loss: 6.931878] time: 0:36:23.119072\n",
      "[Epoch 25/10000] [Batch 0/36] [D loss: 0.067959, acc: 100%] [G loss: 6.030187] time: 0:36:25.559726\n",
      "[Epoch 25/10000] [Batch 1/36] [D loss: 0.050388, acc:  98%] [G loss: 6.674397] time: 0:36:28.202129\n",
      "[Epoch 25/10000] [Batch 2/36] [D loss: 0.028011, acc:  99%] [G loss: 4.966587] time: 0:36:30.645418\n",
      "[Epoch 25/10000] [Batch 3/36] [D loss: 0.066591, acc:  98%] [G loss: 6.333994] time: 0:36:33.062926\n",
      "[Epoch 25/10000] [Batch 4/36] [D loss: 0.022580, acc:  99%] [G loss: 5.772229] time: 0:36:35.440365\n",
      "[Epoch 25/10000] [Batch 5/36] [D loss: 0.041561, acc:  99%] [G loss: 6.594473] time: 0:36:37.827566\n",
      "[Epoch 25/10000] [Batch 6/36] [D loss: 0.065810, acc:  96%] [G loss: 6.307338] time: 0:36:40.291831\n",
      "[Epoch 25/10000] [Batch 7/36] [D loss: 0.046807, acc:  99%] [G loss: 6.750445] time: 0:36:42.727845\n",
      "[Epoch 25/10000] [Batch 8/36] [D loss: 0.064915, acc:  97%] [G loss: 5.723912] time: 0:36:45.049733\n",
      "[Epoch 25/10000] [Batch 9/36] [D loss: 0.057022, acc:  98%] [G loss: 5.838316] time: 0:36:47.495045\n",
      "[Epoch 25/10000] [Batch 10/36] [D loss: 0.021481, acc:  99%] [G loss: 7.001682] time: 0:36:49.760589\n",
      "[Epoch 25/10000] [Batch 11/36] [D loss: 0.042492, acc:  99%] [G loss: 6.450228] time: 0:36:52.137952\n",
      "[Epoch 25/10000] [Batch 12/36] [D loss: 0.051500, acc:  99%] [G loss: 5.557299] time: 0:36:54.551267\n",
      "[Epoch 25/10000] [Batch 13/36] [D loss: 0.024097, acc:  99%] [G loss: 7.609979] time: 0:36:56.991057\n",
      "[Epoch 25/10000] [Batch 14/36] [D loss: 0.040191, acc:  99%] [G loss: 6.421769] time: 0:36:59.409383\n",
      "[Epoch 25/10000] [Batch 15/36] [D loss: 0.035450, acc:  99%] [G loss: 6.012493] time: 0:37:01.887585\n",
      "[Epoch 25/10000] [Batch 16/36] [D loss: 0.045062, acc:  99%] [G loss: 6.609100] time: 0:37:04.249109\n",
      "[Epoch 25/10000] [Batch 17/36] [D loss: 0.034779, acc:  99%] [G loss: 6.055634] time: 0:37:06.550156\n",
      "[Epoch 25/10000] [Batch 18/36] [D loss: 0.035642, acc:  99%] [G loss: 6.174927] time: 0:37:08.938063\n",
      "[Epoch 25/10000] [Batch 19/36] [D loss: 0.028947, acc:  99%] [G loss: 7.212826] time: 0:37:11.388182\n",
      "[Epoch 25/10000] [Batch 20/36] [D loss: 0.039663, acc:  99%] [G loss: 5.818184] time: 0:37:13.819865\n",
      "[Epoch 25/10000] [Batch 21/36] [D loss: 0.084268, acc:  92%] [G loss: 5.745143] time: 0:37:16.254583\n",
      "[Epoch 25/10000] [Batch 22/36] [D loss: 0.039946, acc:  99%] [G loss: 6.774152] time: 0:37:18.726686\n",
      "[Epoch 25/10000] [Batch 23/36] [D loss: 0.055304, acc:  96%] [G loss: 6.557650] time: 0:37:21.098294\n",
      "[Epoch 25/10000] [Batch 24/36] [D loss: 0.084747, acc:  95%] [G loss: 6.577795] time: 0:37:23.417996\n",
      "[Epoch 25/10000] [Batch 25/36] [D loss: 0.047576, acc: 100%] [G loss: 7.017160] time: 0:37:25.829367\n",
      "[Epoch 25/10000] [Batch 26/36] [D loss: 0.057220, acc:  98%] [G loss: 5.608100] time: 0:37:28.108464\n",
      "[Epoch 25/10000] [Batch 27/36] [D loss: 0.129405, acc:  86%] [G loss: 5.687783] time: 0:37:30.595618\n",
      "[Epoch 25/10000] [Batch 28/36] [D loss: 0.038168, acc:  99%] [G loss: 6.025508] time: 0:37:33.141323\n",
      "[Epoch 25/10000] [Batch 29/36] [D loss: 0.028052, acc:  99%] [G loss: 6.097141] time: 0:37:35.609751\n",
      "[Epoch 25/10000] [Batch 30/36] [D loss: 0.045171, acc:  99%] [G loss: 5.680423] time: 0:37:38.088649\n",
      "[Epoch 25/10000] [Batch 31/36] [D loss: 0.075566, acc:  94%] [G loss: 5.792215] time: 0:37:40.582402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25/10000] [Batch 32/36] [D loss: 0.035216, acc:  99%] [G loss: 5.839400] time: 0:37:43.122473\n",
      "[Epoch 25/10000] [Batch 33/36] [D loss: 0.101532, acc:  95%] [G loss: 5.465374] time: 0:37:46.134924\n",
      "[Epoch 25/10000] [Batch 34/36] [D loss: 0.020402, acc:  99%] [G loss: 7.414350] time: 0:37:48.735295\n",
      "[Epoch 26/10000] [Batch 0/36] [D loss: 0.037340, acc: 100%] [G loss: 5.918312] time: 0:37:51.339341\n",
      "[Epoch 26/10000] [Batch 1/36] [D loss: 0.026493, acc:  99%] [G loss: 6.291564] time: 0:37:54.041658\n",
      "[Epoch 26/10000] [Batch 2/36] [D loss: 0.043337, acc:  99%] [G loss: 4.983386] time: 0:37:56.567576\n",
      "[Epoch 26/10000] [Batch 3/36] [D loss: 0.061204, acc:  96%] [G loss: 5.645401] time: 0:37:59.148153\n",
      "[Epoch 26/10000] [Batch 4/36] [D loss: 0.028338, acc:  99%] [G loss: 5.541876] time: 0:38:01.657292\n",
      "[Epoch 26/10000] [Batch 5/36] [D loss: 0.049777, acc:  99%] [G loss: 6.188034] time: 0:38:04.806285\n",
      "[Epoch 26/10000] [Batch 6/36] [D loss: 0.049211, acc:  98%] [G loss: 6.193475] time: 0:38:07.254384\n",
      "[Epoch 26/10000] [Batch 7/36] [D loss: 0.031225, acc:  99%] [G loss: 5.963120] time: 0:38:09.733120\n",
      "[Epoch 26/10000] [Batch 8/36] [D loss: 0.063924, acc:  96%] [G loss: 5.468602] time: 0:38:12.181213\n",
      "[Epoch 26/10000] [Batch 9/36] [D loss: 0.065243, acc:  95%] [G loss: 5.542074] time: 0:38:14.771865\n",
      "[Epoch 26/10000] [Batch 10/36] [D loss: 0.031964, acc:  99%] [G loss: 6.075345] time: 0:38:17.345075\n",
      "[Epoch 26/10000] [Batch 11/36] [D loss: 0.088015, acc:  93%] [G loss: 6.107626] time: 0:38:19.895441\n",
      "[Epoch 26/10000] [Batch 12/36] [D loss: 0.041498, acc:  99%] [G loss: 5.438539] time: 0:38:22.411071\n",
      "[Epoch 26/10000] [Batch 13/36] [D loss: 0.028490, acc:  99%] [G loss: 6.911785] time: 0:38:25.021876\n",
      "[Epoch 26/10000] [Batch 14/36] [D loss: 0.056625, acc:  98%] [G loss: 5.694645] time: 0:38:27.585718\n",
      "[Epoch 26/10000] [Batch 15/36] [D loss: 0.042730, acc:  99%] [G loss: 6.333374] time: 0:38:30.060679\n",
      "[Epoch 26/10000] [Batch 16/36] [D loss: 0.042453, acc:  99%] [G loss: 5.969625] time: 0:38:32.483497\n",
      "[Epoch 26/10000] [Batch 17/36] [D loss: 0.042957, acc:  99%] [G loss: 5.667499] time: 0:38:35.040216\n",
      "[Epoch 26/10000] [Batch 18/36] [D loss: 0.039434, acc:  99%] [G loss: 6.464133] time: 0:38:37.687324\n",
      "[Epoch 26/10000] [Batch 19/36] [D loss: 0.063613, acc:  95%] [G loss: 7.180910] time: 0:38:40.269498\n",
      "[Epoch 26/10000] [Batch 20/36] [D loss: 0.059783, acc:  98%] [G loss: 5.481193] time: 0:38:42.788844\n",
      "[Epoch 26/10000] [Batch 21/36] [D loss: 0.077124, acc:  94%] [G loss: 5.817678] time: 0:38:45.206906\n",
      "[Epoch 26/10000] [Batch 22/36] [D loss: 0.045677, acc:  99%] [G loss: 6.240774] time: 0:38:47.624457\n",
      "[Epoch 26/10000] [Batch 23/36] [D loss: 0.021187, acc:  99%] [G loss: 6.432148] time: 0:38:50.070465\n",
      "[Epoch 26/10000] [Batch 24/36] [D loss: 0.047268, acc:  98%] [G loss: 6.770578] time: 0:38:52.474155\n",
      "[Epoch 26/10000] [Batch 25/36] [D loss: 0.039025, acc:  99%] [G loss: 6.669377] time: 0:38:54.911146\n",
      "[Epoch 26/10000] [Batch 26/36] [D loss: 0.082626, acc:  93%] [G loss: 6.072154] time: 0:38:57.407143\n",
      "[Epoch 26/10000] [Batch 27/36] [D loss: 0.085444, acc:  95%] [G loss: 5.882254] time: 0:38:59.830002\n",
      "[Epoch 26/10000] [Batch 28/36] [D loss: 0.046271, acc:  98%] [G loss: 5.970715] time: 0:39:02.324932\n",
      "[Epoch 26/10000] [Batch 29/36] [D loss: 0.021470, acc:  99%] [G loss: 6.033666] time: 0:39:04.741647\n",
      "[Epoch 26/10000] [Batch 30/36] [D loss: 0.033080, acc:  99%] [G loss: 5.198707] time: 0:39:07.159372\n",
      "[Epoch 26/10000] [Batch 31/36] [D loss: 0.050474, acc:  98%] [G loss: 6.114648] time: 0:39:09.623074\n",
      "[Epoch 26/10000] [Batch 32/36] [D loss: 0.025632, acc:  99%] [G loss: 5.754262] time: 0:39:12.078899\n",
      "[Epoch 26/10000] [Batch 33/36] [D loss: 0.064978, acc:  98%] [G loss: 5.696495] time: 0:39:14.819091\n",
      "[Epoch 26/10000] [Batch 34/36] [D loss: 0.045539, acc:  99%] [G loss: 6.626766] time: 0:39:17.241700\n",
      "[Epoch 27/10000] [Batch 0/36] [D loss: 0.024171, acc:  99%] [G loss: 5.789701] time: 0:39:19.695183\n",
      "[Epoch 27/10000] [Batch 1/36] [D loss: 0.056087, acc:  99%] [G loss: 6.640830] time: 0:39:22.377000\n",
      "[Epoch 27/10000] [Batch 2/36] [D loss: 0.021393, acc:  99%] [G loss: 5.396320] time: 0:39:24.836049\n",
      "[Epoch 27/10000] [Batch 3/36] [D loss: 0.070852, acc:  97%] [G loss: 6.617296] time: 0:39:27.304977\n",
      "[Epoch 27/10000] [Batch 4/36] [D loss: 0.061850, acc:  94%] [G loss: 5.653054] time: 0:39:29.776207\n",
      "[Epoch 27/10000] [Batch 5/36] [D loss: 0.079761, acc:  97%] [G loss: 5.774707] time: 0:39:32.281505\n",
      "[Epoch 27/10000] [Batch 6/36] [D loss: 0.088824, acc:  91%] [G loss: 5.965922] time: 0:39:34.682590\n",
      "[Epoch 27/10000] [Batch 7/36] [D loss: 0.045807, acc:  99%] [G loss: 6.168306] time: 0:39:37.147623\n",
      "[Epoch 27/10000] [Batch 8/36] [D loss: 0.049208, acc:  98%] [G loss: 5.212704] time: 0:39:39.606330\n",
      "[Epoch 27/10000] [Batch 9/36] [D loss: 0.083555, acc:  91%] [G loss: 5.745426] time: 0:39:42.188802\n",
      "[Epoch 27/10000] [Batch 10/36] [D loss: 0.031964, acc:  99%] [G loss: 6.219482] time: 0:39:44.807527\n",
      "[Epoch 27/10000] [Batch 11/36] [D loss: 0.075789, acc:  97%] [G loss: 5.835759] time: 0:39:47.326665\n",
      "[Epoch 27/10000] [Batch 12/36] [D loss: 0.088941, acc:  90%] [G loss: 5.604804] time: 0:39:49.805407\n",
      "[Epoch 27/10000] [Batch 13/36] [D loss: 0.040601, acc:  99%] [G loss: 7.079443] time: 0:39:52.273790\n",
      "[Epoch 27/10000] [Batch 14/36] [D loss: 0.031721, acc:  99%] [G loss: 5.758926] time: 0:39:54.854227\n",
      "[Epoch 27/10000] [Batch 15/36] [D loss: 0.036960, acc:  99%] [G loss: 5.774543] time: 0:39:57.439369\n",
      "[Epoch 27/10000] [Batch 16/36] [D loss: 0.049342, acc:  99%] [G loss: 6.398201] time: 0:39:59.892840\n",
      "[Epoch 27/10000] [Batch 17/36] [D loss: 0.040827, acc:  99%] [G loss: 5.661442] time: 0:40:02.351058\n",
      "[Epoch 27/10000] [Batch 18/36] [D loss: 0.048895, acc:  98%] [G loss: 6.072346] time: 0:40:05.417188\n",
      "[Epoch 27/10000] [Batch 19/36] [D loss: 0.041013, acc:  98%] [G loss: 7.049894] time: 0:40:07.907800\n",
      "[Epoch 27/10000] [Batch 20/36] [D loss: 0.032505, acc:  99%] [G loss: 5.736209] time: 0:40:10.388433\n",
      "[Epoch 27/10000] [Batch 21/36] [D loss: 0.037756, acc:  99%] [G loss: 5.612005] time: 0:40:13.269469\n",
      "[Epoch 27/10000] [Batch 22/36] [D loss: 0.037285, acc:  99%] [G loss: 6.373035] time: 0:40:15.883313\n",
      "[Epoch 27/10000] [Batch 23/36] [D loss: 0.040824, acc:  99%] [G loss: 6.026615] time: 0:40:18.532352\n",
      "[Epoch 27/10000] [Batch 24/36] [D loss: 0.035164, acc:  99%] [G loss: 6.584536] time: 0:40:20.895662\n",
      "[Epoch 27/10000] [Batch 25/36] [D loss: 0.047880, acc: 100%] [G loss: 6.782881] time: 0:40:23.311524\n",
      "[Epoch 27/10000] [Batch 26/36] [D loss: 0.052750, acc:  99%] [G loss: 5.582186] time: 0:40:25.596050\n",
      "[Epoch 27/10000] [Batch 27/36] [D loss: 0.073341, acc:  96%] [G loss: 5.960760] time: 0:40:27.912337\n",
      "[Epoch 27/10000] [Batch 28/36] [D loss: 0.028958, acc:  99%] [G loss: 6.290593] time: 0:40:30.369287\n",
      "[Epoch 27/10000] [Batch 29/36] [D loss: 0.056520, acc:  98%] [G loss: 5.913113] time: 0:40:32.763992\n",
      "[Epoch 27/10000] [Batch 30/36] [D loss: 0.048590, acc:  98%] [G loss: 5.418134] time: 0:40:35.143485\n",
      "[Epoch 27/10000] [Batch 31/36] [D loss: 0.058027, acc:  98%] [G loss: 5.810463] time: 0:40:37.633133\n",
      "[Epoch 27/10000] [Batch 32/36] [D loss: 0.033740, acc:  99%] [G loss: 5.707860] time: 0:40:40.216697\n",
      "[Epoch 27/10000] [Batch 33/36] [D loss: 0.074231, acc:  97%] [G loss: 5.517363] time: 0:40:43.067447\n",
      "[Epoch 27/10000] [Batch 34/36] [D loss: 0.056231, acc:  96%] [G loss: 7.029911] time: 0:40:45.607442\n",
      "[Epoch 28/10000] [Batch 0/36] [D loss: 0.039922, acc: 100%] [G loss: 5.970519] time: 0:40:48.107079\n",
      "[Epoch 28/10000] [Batch 1/36] [D loss: 0.058828, acc:  98%] [G loss: 6.160825] time: 0:40:50.842598\n",
      "[Epoch 28/10000] [Batch 2/36] [D loss: 0.043810, acc:  99%] [G loss: 4.767804] time: 0:40:53.267048\n",
      "[Epoch 28/10000] [Batch 3/36] [D loss: 0.071594, acc:  96%] [G loss: 5.775430] time: 0:40:55.775819\n",
      "[Epoch 28/10000] [Batch 4/36] [D loss: 0.046984, acc:  99%] [G loss: 5.310755] time: 0:40:58.183743\n",
      "[Epoch 28/10000] [Batch 5/36] [D loss: 0.081412, acc:  97%] [G loss: 6.038015] time: 0:41:00.637516\n",
      "[Epoch 28/10000] [Batch 6/36] [D loss: 0.109633, acc:  87%] [G loss: 5.889269] time: 0:41:03.104502\n",
      "[Epoch 28/10000] [Batch 7/36] [D loss: 0.051506, acc:  98%] [G loss: 6.858841] time: 0:41:05.518024\n",
      "[Epoch 28/10000] [Batch 8/36] [D loss: 0.047261, acc:  99%] [G loss: 5.647810] time: 0:41:07.966343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28/10000] [Batch 9/36] [D loss: 0.067088, acc:  97%] [G loss: 5.629587] time: 0:41:10.435915\n",
      "[Epoch 28/10000] [Batch 10/36] [D loss: 0.026518, acc:  99%] [G loss: 6.062657] time: 0:41:12.801733\n",
      "[Epoch 28/10000] [Batch 11/36] [D loss: 0.061139, acc:  98%] [G loss: 6.087672] time: 0:41:15.209801\n",
      "[Epoch 28/10000] [Batch 12/36] [D loss: 0.047238, acc:  99%] [G loss: 5.368460] time: 0:41:17.661255\n",
      "[Epoch 28/10000] [Batch 13/36] [D loss: 0.023676, acc:  99%] [G loss: 7.298233] time: 0:41:20.095563\n",
      "[Epoch 28/10000] [Batch 14/36] [D loss: 0.027306, acc:  99%] [G loss: 5.552992] time: 0:41:22.554069\n",
      "[Epoch 28/10000] [Batch 15/36] [D loss: 0.031042, acc:  99%] [G loss: 5.787525] time: 0:41:24.985495\n",
      "[Epoch 28/10000] [Batch 16/36] [D loss: 0.045126, acc:  99%] [G loss: 6.214814] time: 0:41:27.413700\n",
      "[Epoch 28/10000] [Batch 17/36] [D loss: 0.030363, acc:  99%] [G loss: 5.659244] time: 0:41:29.828221\n",
      "[Epoch 28/10000] [Batch 18/36] [D loss: 0.046275, acc:  99%] [G loss: 6.160319] time: 0:41:32.171292\n",
      "[Epoch 28/10000] [Batch 19/36] [D loss: 0.031992, acc:  99%] [G loss: 6.643559] time: 0:41:34.485052\n",
      "[Epoch 28/10000] [Batch 20/36] [D loss: 0.043547, acc:  99%] [G loss: 5.294547] time: 0:41:36.998971\n",
      "[Epoch 28/10000] [Batch 21/36] [D loss: 0.045567, acc:  99%] [G loss: 5.702263] time: 0:41:39.507791\n",
      "[Epoch 28/10000] [Batch 22/36] [D loss: 0.033754, acc:  99%] [G loss: 6.414084] time: 0:41:42.080457\n",
      "[Epoch 28/10000] [Batch 23/36] [D loss: 0.091628, acc:  88%] [G loss: 6.131039] time: 0:41:44.681029\n",
      "[Epoch 28/10000] [Batch 24/36] [D loss: 0.050209, acc:  99%] [G loss: 6.698975] time: 0:41:47.133152\n",
      "[Epoch 28/10000] [Batch 25/36] [D loss: 0.039451, acc:  99%] [G loss: 6.112520] time: 0:41:49.662573\n",
      "[Epoch 28/10000] [Batch 26/36] [D loss: 0.089978, acc:  95%] [G loss: 5.656855] time: 0:41:52.142594\n",
      "[Epoch 28/10000] [Batch 27/36] [D loss: 0.067392, acc:  97%] [G loss: 5.548275] time: 0:41:55.063488\n",
      "[Epoch 28/10000] [Batch 28/36] [D loss: 0.031057, acc:  99%] [G loss: 5.987261] time: 0:41:57.704639\n",
      "[Epoch 28/10000] [Batch 29/36] [D loss: 0.028186, acc:  99%] [G loss: 6.198278] time: 0:42:00.254407\n",
      "[Epoch 28/10000] [Batch 30/36] [D loss: 0.021061, acc:  99%] [G loss: 5.094889] time: 0:42:02.844810\n",
      "[Epoch 28/10000] [Batch 31/36] [D loss: 0.057542, acc:  97%] [G loss: 6.109884] time: 0:42:05.272548\n",
      "[Epoch 28/10000] [Batch 32/36] [D loss: 0.047881, acc:  98%] [G loss: 5.586302] time: 0:42:07.771709\n",
      "[Epoch 28/10000] [Batch 33/36] [D loss: 0.115976, acc:  93%] [G loss: 5.446098] time: 0:42:10.552645\n",
      "[Epoch 28/10000] [Batch 34/36] [D loss: 0.021554, acc:  99%] [G loss: 7.237503] time: 0:42:12.942689\n",
      "[Epoch 29/10000] [Batch 0/36] [D loss: 0.048767, acc: 100%] [G loss: 5.618729] time: 0:42:15.350409\n",
      "[Epoch 29/10000] [Batch 1/36] [D loss: 0.045816, acc:  99%] [G loss: 6.483978] time: 0:42:17.952716\n",
      "[Epoch 29/10000] [Batch 2/36] [D loss: 0.041261, acc:  99%] [G loss: 5.107643] time: 0:42:20.171483\n",
      "[Epoch 29/10000] [Batch 3/36] [D loss: 0.038884, acc:  99%] [G loss: 5.472631] time: 0:42:22.470766\n",
      "[Epoch 29/10000] [Batch 4/36] [D loss: 0.032959, acc:  99%] [G loss: 5.219806] time: 0:42:24.761151\n",
      "[Epoch 29/10000] [Batch 5/36] [D loss: 0.096347, acc:  94%] [G loss: 5.565617] time: 0:42:27.095828\n",
      "[Epoch 29/10000] [Batch 6/36] [D loss: 0.081042, acc:  93%] [G loss: 5.944149] time: 0:42:29.327121\n",
      "[Epoch 29/10000] [Batch 7/36] [D loss: 0.029931, acc:  99%] [G loss: 6.217203] time: 0:42:31.670793\n",
      "[Epoch 29/10000] [Batch 8/36] [D loss: 0.066782, acc:  98%] [G loss: 5.324329] time: 0:42:34.030425\n",
      "[Epoch 29/10000] [Batch 9/36] [D loss: 0.054336, acc:  99%] [G loss: 5.688497] time: 0:42:36.381276\n",
      "[Epoch 29/10000] [Batch 10/36] [D loss: 0.038374, acc:  99%] [G loss: 6.253248] time: 0:42:38.773462\n",
      "[Epoch 29/10000] [Batch 11/36] [D loss: 0.047511, acc:  98%] [G loss: 5.976743] time: 0:42:41.066828\n",
      "[Epoch 29/10000] [Batch 12/36] [D loss: 0.050510, acc:  97%] [G loss: 5.433062] time: 0:42:43.296479\n",
      "[Epoch 29/10000] [Batch 13/36] [D loss: 0.026752, acc:  99%] [G loss: 6.889556] time: 0:42:45.689113\n",
      "[Epoch 29/10000] [Batch 14/36] [D loss: 0.033052, acc:  99%] [G loss: 5.797585] time: 0:42:48.100916\n",
      "[Epoch 29/10000] [Batch 15/36] [D loss: 0.039701, acc:  99%] [G loss: 5.735782] time: 0:42:50.428198\n",
      "[Epoch 29/10000] [Batch 16/36] [D loss: 0.069174, acc:  97%] [G loss: 6.209821] time: 0:42:52.811583\n",
      "[Epoch 29/10000] [Batch 17/36] [D loss: 0.026015, acc: 100%] [G loss: 5.426761] time: 0:42:55.040551\n",
      "[Epoch 29/10000] [Batch 18/36] [D loss: 0.063769, acc:  98%] [G loss: 5.977859] time: 0:42:57.478791\n",
      "[Epoch 29/10000] [Batch 19/36] [D loss: 0.025951, acc:  99%] [G loss: 6.896024] time: 0:43:00.010073\n",
      "[Epoch 29/10000] [Batch 20/36] [D loss: 0.047718, acc:  99%] [G loss: 5.273805] time: 0:43:02.451802\n",
      "[Epoch 29/10000] [Batch 21/36] [D loss: 0.055675, acc:  98%] [G loss: 5.285166] time: 0:43:04.741662\n",
      "[Epoch 29/10000] [Batch 22/36] [D loss: 0.017589, acc:  99%] [G loss: 5.929336] time: 0:43:07.022975\n",
      "[Epoch 29/10000] [Batch 23/36] [D loss: 0.051737, acc:  97%] [G loss: 5.862989] time: 0:43:09.373016\n",
      "[Epoch 29/10000] [Batch 24/36] [D loss: 0.039079, acc:  99%] [G loss: 6.385038] time: 0:43:12.006670\n",
      "[Epoch 29/10000] [Batch 25/36] [D loss: 0.055639, acc:  99%] [G loss: 6.379634] time: 0:43:14.442369\n",
      "[Epoch 29/10000] [Batch 26/36] [D loss: 0.089409, acc:  96%] [G loss: 5.641865] time: 0:43:17.003179\n",
      "[Epoch 29/10000] [Batch 27/36] [D loss: 0.051326, acc:  98%] [G loss: 5.784881] time: 0:43:19.472802\n",
      "[Epoch 29/10000] [Batch 28/36] [D loss: 0.033578, acc:  99%] [G loss: 5.852572] time: 0:43:21.960668\n",
      "[Epoch 29/10000] [Batch 29/36] [D loss: 0.030762, acc:  99%] [G loss: 5.874623] time: 0:43:24.468296\n",
      "[Epoch 29/10000] [Batch 30/36] [D loss: 0.019971, acc:  99%] [G loss: 5.390049] time: 0:43:27.181432\n",
      "[Epoch 29/10000] [Batch 31/36] [D loss: 0.037043, acc:  99%] [G loss: 5.802504] time: 0:43:29.551182\n",
      "[Epoch 29/10000] [Batch 32/36] [D loss: 0.025833, acc:  99%] [G loss: 5.566979] time: 0:43:31.781864\n",
      "[Epoch 29/10000] [Batch 33/36] [D loss: 0.058058, acc:  98%] [G loss: 5.276197] time: 0:43:34.335170\n",
      "[Epoch 29/10000] [Batch 34/36] [D loss: 0.047311, acc:  98%] [G loss: 6.323926] time: 0:43:36.652294\n",
      "[Epoch 30/10000] [Batch 0/36] [D loss: 0.047141, acc: 100%] [G loss: 5.564101] time: 0:43:39.102987\n",
      "[Epoch 30/10000] [Batch 1/36] [D loss: 0.052580, acc:  97%] [G loss: 5.912702] time: 0:43:42.144228\n",
      "[Epoch 30/10000] [Batch 2/36] [D loss: 0.037207, acc:  99%] [G loss: 4.997671] time: 0:43:44.560497\n",
      "[Epoch 30/10000] [Batch 3/36] [D loss: 0.066033, acc:  96%] [G loss: 5.842319] time: 0:43:46.903807\n",
      "[Epoch 30/10000] [Batch 4/36] [D loss: 0.046468, acc:  97%] [G loss: 5.252876] time: 0:43:49.242018\n",
      "[Epoch 30/10000] [Batch 5/36] [D loss: 0.090390, acc:  95%] [G loss: 5.798559] time: 0:43:51.582401\n",
      "[Epoch 30/10000] [Batch 6/36] [D loss: 0.073834, acc:  94%] [G loss: 5.641501] time: 0:43:53.933140\n",
      "[Epoch 30/10000] [Batch 7/36] [D loss: 0.044830, acc:  99%] [G loss: 5.963204] time: 0:43:56.263189\n",
      "[Epoch 30/10000] [Batch 8/36] [D loss: 0.083988, acc:  93%] [G loss: 5.099967] time: 0:43:58.580834\n",
      "[Epoch 30/10000] [Batch 9/36] [D loss: 0.055288, acc:  97%] [G loss: 5.584333] time: 0:44:00.854308\n",
      "[Epoch 30/10000] [Batch 10/36] [D loss: 0.024539, acc:  99%] [G loss: 5.970622] time: 0:44:03.207382\n",
      "[Epoch 30/10000] [Batch 11/36] [D loss: 0.066112, acc:  98%] [G loss: 5.632572] time: 0:44:05.589476\n",
      "[Epoch 30/10000] [Batch 12/36] [D loss: 0.036863, acc:  99%] [G loss: 5.502453] time: 0:44:07.844330\n",
      "[Epoch 30/10000] [Batch 13/36] [D loss: 0.019583, acc:  99%] [G loss: 6.916251] time: 0:44:10.224512\n",
      "[Epoch 30/10000] [Batch 14/36] [D loss: 0.053888, acc:  98%] [G loss: 5.385914] time: 0:44:12.576949\n",
      "[Epoch 30/10000] [Batch 15/36] [D loss: 0.030680, acc:  99%] [G loss: 5.738240] time: 0:44:14.879649\n",
      "[Epoch 30/10000] [Batch 16/36] [D loss: 0.070795, acc:  98%] [G loss: 6.039312] time: 0:44:17.178930\n",
      "[Epoch 30/10000] [Batch 17/36] [D loss: 0.024467, acc:  99%] [G loss: 5.343268] time: 0:44:19.602733\n",
      "[Epoch 30/10000] [Batch 18/36] [D loss: 0.050134, acc:  99%] [G loss: 5.955658] time: 0:44:21.887622\n",
      "[Epoch 30/10000] [Batch 19/36] [D loss: 0.032949, acc:  99%] [G loss: 6.647209] time: 0:44:24.186470\n",
      "[Epoch 30/10000] [Batch 20/36] [D loss: 0.078029, acc:  97%] [G loss: 5.460019] time: 0:44:26.503855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30/10000] [Batch 21/36] [D loss: 0.061879, acc:  97%] [G loss: 5.091615] time: 0:44:28.898031\n",
      "[Epoch 30/10000] [Batch 22/36] [D loss: 0.028585, acc:  99%] [G loss: 6.264400] time: 0:44:31.281553\n",
      "[Epoch 30/10000] [Batch 23/36] [D loss: 0.034030, acc:  99%] [G loss: 5.761091] time: 0:44:33.660480\n",
      "[Epoch 30/10000] [Batch 24/36] [D loss: 0.060558, acc:  98%] [G loss: 6.385343] time: 0:44:36.003589\n",
      "[Epoch 30/10000] [Batch 25/36] [D loss: 0.046757, acc: 100%] [G loss: 6.304284] time: 0:44:38.339585\n",
      "[Epoch 30/10000] [Batch 26/36] [D loss: 0.088098, acc:  94%] [G loss: 5.320852] time: 0:44:40.730683\n",
      "[Epoch 30/10000] [Batch 27/36] [D loss: 0.076618, acc:  97%] [G loss: 5.474159] time: 0:44:43.003368\n",
      "[Epoch 30/10000] [Batch 28/36] [D loss: 0.057399, acc:  97%] [G loss: 6.030283] time: 0:44:45.381115\n",
      "[Epoch 30/10000] [Batch 29/36] [D loss: 0.020454, acc:  99%] [G loss: 5.554451] time: 0:44:47.749284\n",
      "[Epoch 30/10000] [Batch 30/36] [D loss: 0.022414, acc:  99%] [G loss: 5.589322] time: 0:44:50.028407\n",
      "[Epoch 30/10000] [Batch 31/36] [D loss: 0.051794, acc:  98%] [G loss: 5.627484] time: 0:44:52.426067\n",
      "[Epoch 30/10000] [Batch 32/36] [D loss: 0.039427, acc:  99%] [G loss: 5.536987] time: 0:44:54.828733\n",
      "[Epoch 30/10000] [Batch 33/36] [D loss: 0.095853, acc:  91%] [G loss: 5.449317] time: 0:44:57.459307\n",
      "[Epoch 30/10000] [Batch 34/36] [D loss: 0.027736, acc:  99%] [G loss: 6.465057] time: 0:44:59.861980\n",
      "[Epoch 31/10000] [Batch 0/36] [D loss: 0.040928, acc: 100%] [G loss: 5.693590] time: 0:45:02.194519\n",
      "[Epoch 31/10000] [Batch 1/36] [D loss: 0.020447, acc:  99%] [G loss: 6.535714] time: 0:45:04.813322\n",
      "[Epoch 31/10000] [Batch 2/36] [D loss: 0.022885, acc:  99%] [G loss: 4.841147] time: 0:45:07.193077\n",
      "[Epoch 31/10000] [Batch 3/36] [D loss: 0.043843, acc:  99%] [G loss: 5.603973] time: 0:45:09.561535\n",
      "[Epoch 31/10000] [Batch 4/36] [D loss: 0.043907, acc:  98%] [G loss: 5.404487] time: 0:45:11.962659\n",
      "[Epoch 31/10000] [Batch 5/36] [D loss: 0.045523, acc:  99%] [G loss: 5.554542] time: 0:45:14.604929\n",
      "[Epoch 31/10000] [Batch 6/36] [D loss: 0.098663, acc:  87%] [G loss: 5.791215] time: 0:45:17.130624\n",
      "[Epoch 31/10000] [Batch 7/36] [D loss: 0.044314, acc:  98%] [G loss: 5.967040] time: 0:45:19.455344\n",
      "[Epoch 31/10000] [Batch 8/36] [D loss: 0.066234, acc:  98%] [G loss: 5.241188] time: 0:45:21.965633\n",
      "[Epoch 31/10000] [Batch 9/36] [D loss: 0.080967, acc:  93%] [G loss: 5.362765] time: 0:45:24.336936\n",
      "[Epoch 31/10000] [Batch 10/36] [D loss: 0.024440, acc:  99%] [G loss: 6.367486] time: 0:45:26.646471\n",
      "[Epoch 31/10000] [Batch 11/36] [D loss: 0.058375, acc:  98%] [G loss: 5.750335] time: 0:45:28.931302\n",
      "[Epoch 31/10000] [Batch 12/36] [D loss: 0.061735, acc:  95%] [G loss: 5.171818] time: 0:45:31.245070\n",
      "[Epoch 31/10000] [Batch 13/36] [D loss: 0.024023, acc:  99%] [G loss: 6.008471] time: 0:45:33.685448\n",
      "[Epoch 31/10000] [Batch 14/36] [D loss: 0.051229, acc:  99%] [G loss: 5.502068] time: 0:45:36.017293\n",
      "[Epoch 31/10000] [Batch 15/36] [D loss: 0.029742, acc:  99%] [G loss: 5.561556] time: 0:45:38.313374\n",
      "[Epoch 31/10000] [Batch 16/36] [D loss: 0.039854, acc:  99%] [G loss: 5.688480] time: 0:45:40.726102\n",
      "[Epoch 31/10000] [Batch 17/36] [D loss: 0.034258, acc:  99%] [G loss: 5.381682] time: 0:45:43.120910\n",
      "[Epoch 31/10000] [Batch 18/36] [D loss: 0.042870, acc:  99%] [G loss: 5.833279] time: 0:45:45.497184\n",
      "[Epoch 31/10000] [Batch 19/36] [D loss: 0.028626, acc:  99%] [G loss: 6.894696] time: 0:45:47.833322\n",
      "[Epoch 31/10000] [Batch 20/36] [D loss: 0.042246, acc:  99%] [G loss: 5.063687] time: 0:45:50.288764\n",
      "[Epoch 31/10000] [Batch 21/36] [D loss: 0.044036, acc:  98%] [G loss: 5.250202] time: 0:45:52.621408\n",
      "[Epoch 31/10000] [Batch 22/36] [D loss: 0.024425, acc:  99%] [G loss: 5.664336] time: 0:45:54.989888\n",
      "[Epoch 31/10000] [Batch 23/36] [D loss: 0.032303, acc:  99%] [G loss: 5.897104] time: 0:45:57.404885\n",
      "[Epoch 31/10000] [Batch 24/36] [D loss: 0.082320, acc:  96%] [G loss: 6.036686] time: 0:45:59.758268\n",
      "[Epoch 31/10000] [Batch 25/36] [D loss: 0.039341, acc: 100%] [G loss: 6.262304] time: 0:46:02.034137\n",
      "[Epoch 31/10000] [Batch 26/36] [D loss: 0.088426, acc:  96%] [G loss: 5.502625] time: 0:46:04.595783\n",
      "[Epoch 31/10000] [Batch 27/36] [D loss: 0.069611, acc:  96%] [G loss: 5.573412] time: 0:46:07.230531\n",
      "[Epoch 31/10000] [Batch 28/36] [D loss: 0.054561, acc:  97%] [G loss: 5.816474] time: 0:46:09.566769\n",
      "[Epoch 31/10000] [Batch 29/36] [D loss: 0.020351, acc:  99%] [G loss: 5.962660] time: 0:46:11.994580\n",
      "[Epoch 31/10000] [Batch 30/36] [D loss: 0.034135, acc:  99%] [G loss: 5.328659] time: 0:46:14.397046\n",
      "[Epoch 31/10000] [Batch 31/36] [D loss: 0.096852, acc:  89%] [G loss: 5.812592] time: 0:46:17.098205\n",
      "[Epoch 31/10000] [Batch 32/36] [D loss: 0.043365, acc:  98%] [G loss: 5.897950] time: 0:46:19.647866\n",
      "[Epoch 31/10000] [Batch 33/36] [D loss: 0.077949, acc:  96%] [G loss: 5.257350] time: 0:46:23.071526\n",
      "[Epoch 31/10000] [Batch 34/36] [D loss: 0.052102, acc:  98%] [G loss: 7.525042] time: 0:46:26.199971\n",
      "[Epoch 32/10000] [Batch 0/36] [D loss: 0.067277, acc: 100%] [G loss: 6.109515] time: 0:46:28.760354\n",
      "[Epoch 32/10000] [Batch 1/36] [D loss: 0.043320, acc:  99%] [G loss: 6.453939] time: 0:46:31.565392\n",
      "[Epoch 32/10000] [Batch 2/36] [D loss: 0.047986, acc:  99%] [G loss: 4.800894] time: 0:46:34.713214\n",
      "[Epoch 32/10000] [Batch 3/36] [D loss: 0.067029, acc:  96%] [G loss: 6.007451] time: 0:46:37.362010\n",
      "[Epoch 32/10000] [Batch 4/36] [D loss: 0.045113, acc:  98%] [G loss: 5.355840] time: 0:46:40.269596\n",
      "[Epoch 32/10000] [Batch 5/36] [D loss: 0.046390, acc:  99%] [G loss: 5.955476] time: 0:46:42.684624\n",
      "[Epoch 32/10000] [Batch 6/36] [D loss: 0.037244, acc:  99%] [G loss: 5.914639] time: 0:46:45.139072\n",
      "[Epoch 32/10000] [Batch 7/36] [D loss: 0.036425, acc:  99%] [G loss: 6.046071] time: 0:46:47.588017\n",
      "[Epoch 32/10000] [Batch 8/36] [D loss: 0.081762, acc:  97%] [G loss: 5.411749] time: 0:46:50.001579\n",
      "[Epoch 32/10000] [Batch 9/36] [D loss: 0.066143, acc:  97%] [G loss: 5.417253] time: 0:46:52.453101\n",
      "[Epoch 32/10000] [Batch 10/36] [D loss: 0.023993, acc:  99%] [G loss: 6.038052] time: 0:46:54.908206\n",
      "[Epoch 32/10000] [Batch 11/36] [D loss: 0.078356, acc:  96%] [G loss: 5.842974] time: 0:46:57.316541\n",
      "[Epoch 32/10000] [Batch 12/36] [D loss: 0.055028, acc:  99%] [G loss: 5.130041] time: 0:46:59.749097\n",
      "[Epoch 32/10000] [Batch 13/36] [D loss: 0.032420, acc:  99%] [G loss: 6.263067] time: 0:47:02.185926\n",
      "[Epoch 32/10000] [Batch 14/36] [D loss: 0.046326, acc:  99%] [G loss: 5.706817] time: 0:47:04.670618\n",
      "[Epoch 32/10000] [Batch 15/36] [D loss: 0.039030, acc:  99%] [G loss: 5.527020] time: 0:47:07.159567\n",
      "[Epoch 32/10000] [Batch 16/36] [D loss: 0.042548, acc:  99%] [G loss: 5.936077] time: 0:47:09.638244\n",
      "[Epoch 32/10000] [Batch 17/36] [D loss: 0.035418, acc:  99%] [G loss: 5.363052] time: 0:47:12.125226\n",
      "[Epoch 32/10000] [Batch 18/36] [D loss: 0.064468, acc:  97%] [G loss: 5.713526] time: 0:47:14.575982\n",
      "[Epoch 32/10000] [Batch 19/36] [D loss: 0.027722, acc:  99%] [G loss: 6.947370] time: 0:47:17.053958\n",
      "[Epoch 32/10000] [Batch 20/36] [D loss: 0.067435, acc:  97%] [G loss: 5.022871] time: 0:47:19.543282\n",
      "[Epoch 32/10000] [Batch 21/36] [D loss: 0.057610, acc:  97%] [G loss: 5.269637] time: 0:47:21.976004\n",
      "[Epoch 32/10000] [Batch 22/36] [D loss: 0.038907, acc:  99%] [G loss: 5.868290] time: 0:47:24.419147\n",
      "[Epoch 32/10000] [Batch 23/36] [D loss: 0.018921, acc:  99%] [G loss: 5.710512] time: 0:47:26.938479\n",
      "[Epoch 32/10000] [Batch 24/36] [D loss: 0.065283, acc:  98%] [G loss: 6.117203] time: 0:47:29.803401\n",
      "[Epoch 32/10000] [Batch 25/36] [D loss: 0.023663, acc:  99%] [G loss: 6.491520] time: 0:47:32.251607\n",
      "[Epoch 32/10000] [Batch 26/36] [D loss: 0.081210, acc:  97%] [G loss: 5.863521] time: 0:47:34.829776\n",
      "[Epoch 32/10000] [Batch 27/36] [D loss: 0.054829, acc:  98%] [G loss: 5.800350] time: 0:47:37.376171\n",
      "[Epoch 32/10000] [Batch 28/36] [D loss: 0.037010, acc:  99%] [G loss: 5.946124] time: 0:47:39.860156\n",
      "[Epoch 32/10000] [Batch 29/36] [D loss: 0.021333, acc:  99%] [G loss: 6.239061] time: 0:47:42.240277\n",
      "[Epoch 32/10000] [Batch 30/36] [D loss: 0.034865, acc:  99%] [G loss: 5.379045] time: 0:47:44.675398\n",
      "[Epoch 32/10000] [Batch 31/36] [D loss: 0.053916, acc:  97%] [G loss: 5.802565] time: 0:47:47.229408\n",
      "[Epoch 32/10000] [Batch 32/36] [D loss: 0.031689, acc:  99%] [G loss: 5.473685] time: 0:47:49.657759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 32/10000] [Batch 33/36] [D loss: 0.073788, acc:  97%] [G loss: 5.505059] time: 0:47:52.427085\n",
      "[Epoch 32/10000] [Batch 34/36] [D loss: 0.019674, acc:  99%] [G loss: 6.594120] time: 0:47:54.925378\n",
      "[Epoch 33/10000] [Batch 0/36] [D loss: 0.052466, acc: 100%] [G loss: 5.767337] time: 0:47:57.373803\n",
      "[Epoch 33/10000] [Batch 1/36] [D loss: 0.025053, acc:  99%] [G loss: 6.350683] time: 0:48:00.089973\n",
      "[Epoch 33/10000] [Batch 2/36] [D loss: 0.022446, acc:  99%] [G loss: 4.857995] time: 0:48:02.514117\n",
      "[Epoch 33/10000] [Batch 3/36] [D loss: 0.040272, acc:  99%] [G loss: 5.996594] time: 0:48:05.016523\n",
      "[Epoch 33/10000] [Batch 4/36] [D loss: 0.022239, acc:  99%] [G loss: 5.524319] time: 0:48:07.425837\n",
      "[Epoch 33/10000] [Batch 5/36] [D loss: 0.047262, acc:  99%] [G loss: 5.707425] time: 0:48:09.806366\n",
      "[Epoch 33/10000] [Batch 6/36] [D loss: 0.067002, acc:  94%] [G loss: 5.841396] time: 0:48:12.373076\n",
      "[Epoch 33/10000] [Batch 7/36] [D loss: 0.028735, acc:  99%] [G loss: 5.905837] time: 0:48:14.735226\n",
      "[Epoch 33/10000] [Batch 8/36] [D loss: 0.091581, acc:  93%] [G loss: 5.204444] time: 0:48:17.227519\n",
      "[Epoch 33/10000] [Batch 9/36] [D loss: 0.078877, acc:  94%] [G loss: 5.512817] time: 0:48:19.775036\n",
      "[Epoch 33/10000] [Batch 10/36] [D loss: 0.028532, acc:  99%] [G loss: 6.157303] time: 0:48:22.167176\n",
      "[Epoch 33/10000] [Batch 11/36] [D loss: 0.034044, acc:  99%] [G loss: 5.755329] time: 0:48:24.681279\n",
      "[Epoch 33/10000] [Batch 12/36] [D loss: 0.025574, acc:  99%] [G loss: 5.260400] time: 0:48:27.221100\n",
      "[Epoch 33/10000] [Batch 13/36] [D loss: 0.029547, acc:  99%] [G loss: 6.702160] time: 0:48:29.847951\n",
      "[Epoch 33/10000] [Batch 14/36] [D loss: 0.030939, acc:  99%] [G loss: 5.478982] time: 0:48:32.365685\n",
      "[Epoch 33/10000] [Batch 15/36] [D loss: 0.044339, acc:  99%] [G loss: 5.574885] time: 0:48:34.845036\n",
      "[Epoch 33/10000] [Batch 16/36] [D loss: 0.044003, acc:  99%] [G loss: 5.983833] time: 0:48:37.271825\n",
      "[Epoch 33/10000] [Batch 17/36] [D loss: 0.038486, acc: 100%] [G loss: 5.405795] time: 0:48:39.689636\n",
      "[Epoch 33/10000] [Batch 18/36] [D loss: 0.050584, acc:  99%] [G loss: 5.852356] time: 0:48:42.124120\n",
      "[Epoch 33/10000] [Batch 19/36] [D loss: 0.043651, acc:  99%] [G loss: 6.872086] time: 0:48:44.600137\n",
      "[Epoch 33/10000] [Batch 20/36] [D loss: 0.074656, acc:  94%] [G loss: 5.145740] time: 0:48:47.127694\n",
      "[Epoch 33/10000] [Batch 21/36] [D loss: 0.061078, acc:  97%] [G loss: 5.058989] time: 0:48:49.635772\n",
      "[Epoch 33/10000] [Batch 22/36] [D loss: 0.042131, acc:  99%] [G loss: 6.044200] time: 0:48:52.011712\n",
      "[Epoch 33/10000] [Batch 23/36] [D loss: 0.053848, acc:  98%] [G loss: 5.849186] time: 0:48:54.430086\n",
      "[Epoch 33/10000] [Batch 24/36] [D loss: 0.065693, acc:  96%] [G loss: 6.413212] time: 0:48:56.888523\n",
      "[Epoch 33/10000] [Batch 25/36] [D loss: 0.073792, acc:  99%] [G loss: 5.993066] time: 0:48:59.429534\n",
      "[Epoch 33/10000] [Batch 26/36] [D loss: 0.074001, acc:  97%] [G loss: 5.543011] time: 0:49:01.818381\n",
      "[Epoch 33/10000] [Batch 27/36] [D loss: 0.075795, acc:  96%] [G loss: 5.514525] time: 0:49:04.136789\n",
      "[Epoch 33/10000] [Batch 28/36] [D loss: 0.056490, acc:  97%] [G loss: 6.048010] time: 0:49:06.480501\n",
      "[Epoch 33/10000] [Batch 29/36] [D loss: 0.026387, acc:  99%] [G loss: 6.237671] time: 0:49:08.895479\n",
      "[Epoch 33/10000] [Batch 30/36] [D loss: 0.039665, acc:  99%] [G loss: 5.373590] time: 0:49:11.412990\n",
      "[Epoch 33/10000] [Batch 31/36] [D loss: 0.072146, acc:  95%] [G loss: 6.095944] time: 0:49:13.742858\n",
      "[Epoch 33/10000] [Batch 32/36] [D loss: 0.027887, acc:  99%] [G loss: 5.974510] time: 0:49:16.248956\n",
      "[Epoch 33/10000] [Batch 33/36] [D loss: 0.131936, acc:  88%] [G loss: 5.503272] time: 0:49:18.976542\n",
      "[Epoch 33/10000] [Batch 34/36] [D loss: 0.025133, acc:  99%] [G loss: 7.006065] time: 0:49:21.366667\n",
      "[Epoch 34/10000] [Batch 0/36] [D loss: 0.035513, acc: 100%] [G loss: 5.875097] time: 0:49:23.688105\n",
      "[Epoch 34/10000] [Batch 1/36] [D loss: 0.030139, acc:  99%] [G loss: 6.368671] time: 0:49:26.247632\n",
      "[Epoch 34/10000] [Batch 2/36] [D loss: 0.069520, acc:  97%] [G loss: 5.126126] time: 0:49:28.681015\n",
      "[Epoch 34/10000] [Batch 3/36] [D loss: 0.050446, acc:  97%] [G loss: 6.183599] time: 0:49:31.100101\n",
      "[Epoch 34/10000] [Batch 4/36] [D loss: 0.039880, acc:  98%] [G loss: 5.662474] time: 0:49:33.529314\n",
      "[Epoch 34/10000] [Batch 5/36] [D loss: 0.056855, acc:  98%] [G loss: 6.298072] time: 0:49:36.079181\n",
      "[Epoch 34/10000] [Batch 6/36] [D loss: 0.053515, acc:  98%] [G loss: 5.761445] time: 0:49:38.496995\n",
      "[Epoch 34/10000] [Batch 7/36] [D loss: 0.038360, acc:  99%] [G loss: 6.234154] time: 0:49:40.929316\n",
      "[Epoch 34/10000] [Batch 8/36] [D loss: 0.072586, acc:  97%] [G loss: 5.133491] time: 0:49:43.200130\n",
      "[Epoch 34/10000] [Batch 9/36] [D loss: 0.079704, acc:  95%] [G loss: 5.699101] time: 0:49:45.594064\n",
      "[Epoch 34/10000] [Batch 10/36] [D loss: 0.032962, acc:  99%] [G loss: 6.180899] time: 0:49:47.877729\n",
      "[Epoch 34/10000] [Batch 11/36] [D loss: 0.081947, acc:  93%] [G loss: 6.001993] time: 0:49:50.200863\n",
      "[Epoch 34/10000] [Batch 12/36] [D loss: 0.070120, acc:  94%] [G loss: 5.440304] time: 0:49:52.624010\n",
      "[Epoch 34/10000] [Batch 13/36] [D loss: 0.031002, acc:  99%] [G loss: 6.679334] time: 0:49:55.282940\n",
      "[Epoch 34/10000] [Batch 14/36] [D loss: 0.044293, acc:  99%] [G loss: 5.548395] time: 0:49:57.837138\n",
      "[Epoch 34/10000] [Batch 15/36] [D loss: 0.052178, acc:  98%] [G loss: 5.654471] time: 0:50:00.526548\n",
      "[Epoch 34/10000] [Batch 16/36] [D loss: 0.033784, acc:  99%] [G loss: 5.918621] time: 0:50:03.012306\n",
      "[Epoch 34/10000] [Batch 17/36] [D loss: 0.049682, acc: 100%] [G loss: 5.661562] time: 0:50:05.673960\n",
      "[Epoch 34/10000] [Batch 18/36] [D loss: 0.046431, acc:  99%] [G loss: 5.920330] time: 0:50:08.163682\n",
      "[Epoch 34/10000] [Batch 19/36] [D loss: 0.039697, acc:  98%] [G loss: 6.745311] time: 0:50:10.562079\n",
      "[Epoch 34/10000] [Batch 20/36] [D loss: 0.035328, acc:  99%] [G loss: 5.187358] time: 0:50:12.977329\n",
      "[Epoch 34/10000] [Batch 21/36] [D loss: 0.083079, acc:  93%] [G loss: 5.117505] time: 0:50:15.512063\n",
      "[Epoch 34/10000] [Batch 22/36] [D loss: 0.046726, acc:  98%] [G loss: 5.727754] time: 0:50:17.917089\n",
      "[Epoch 34/10000] [Batch 23/36] [D loss: 0.039720, acc:  99%] [G loss: 5.783992] time: 0:50:20.432996\n",
      "[Epoch 34/10000] [Batch 24/36] [D loss: 0.044368, acc:  99%] [G loss: 6.273666] time: 0:50:22.782203\n",
      "[Epoch 34/10000] [Batch 25/36] [D loss: 0.036919, acc:  99%] [G loss: 6.000674] time: 0:50:25.208478\n",
      "[Epoch 34/10000] [Batch 26/36] [D loss: 0.084199, acc:  96%] [G loss: 5.416940] time: 0:50:27.687085\n",
      "[Epoch 34/10000] [Batch 27/36] [D loss: 0.083289, acc:  94%] [G loss: 5.751156] time: 0:50:30.136173\n",
      "[Epoch 34/10000] [Batch 28/36] [D loss: 0.024444, acc:  99%] [G loss: 5.793439] time: 0:50:32.764990\n",
      "[Epoch 34/10000] [Batch 29/36] [D loss: 0.050273, acc:  99%] [G loss: 6.100618] time: 0:50:35.288152\n",
      "[Epoch 34/10000] [Batch 30/36] [D loss: 0.024429, acc:  99%] [G loss: 5.220757] time: 0:50:37.804047\n",
      "[Epoch 34/10000] [Batch 31/36] [D loss: 0.069982, acc:  95%] [G loss: 5.731342] time: 0:50:40.431894\n",
      "[Epoch 34/10000] [Batch 32/36] [D loss: 0.030322, acc:  99%] [G loss: 6.140177] time: 0:50:43.151415\n",
      "[Epoch 34/10000] [Batch 33/36] [D loss: 0.085969, acc:  95%] [G loss: 5.412108] time: 0:50:46.135851\n",
      "[Epoch 34/10000] [Batch 34/36] [D loss: 0.026621, acc:  99%] [G loss: 6.572588] time: 0:50:48.830792\n",
      "[Epoch 35/10000] [Batch 0/36] [D loss: 0.030330, acc: 100%] [G loss: 6.008738] time: 0:50:51.587580\n",
      "[Epoch 35/10000] [Batch 1/36] [D loss: 0.045124, acc:  99%] [G loss: 6.058764] time: 0:50:54.231251\n",
      "[Epoch 35/10000] [Batch 2/36] [D loss: 0.025467, acc:  99%] [G loss: 5.211053] time: 0:50:56.448237\n",
      "[Epoch 35/10000] [Batch 3/36] [D loss: 0.108110, acc:  87%] [G loss: 5.769799] time: 0:50:58.998319\n",
      "[Epoch 35/10000] [Batch 4/36] [D loss: 0.025182, acc:  99%] [G loss: 5.251501] time: 0:51:01.488762\n",
      "[Epoch 35/10000] [Batch 5/36] [D loss: 0.073450, acc:  97%] [G loss: 5.724038] time: 0:51:04.809127\n",
      "[Epoch 35/10000] [Batch 6/36] [D loss: 0.056140, acc:  97%] [G loss: 5.967473] time: 0:51:07.314685\n",
      "[Epoch 35/10000] [Batch 7/36] [D loss: 0.043584, acc:  99%] [G loss: 6.086813] time: 0:51:10.039087\n",
      "[Epoch 35/10000] [Batch 8/36] [D loss: 0.069520, acc:  96%] [G loss: 5.278223] time: 0:51:12.279930\n",
      "[Epoch 35/10000] [Batch 9/36] [D loss: 0.064316, acc:  97%] [G loss: 5.600646] time: 0:51:14.551599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35/10000] [Batch 10/36] [D loss: 0.029650, acc:  99%] [G loss: 6.163313] time: 0:51:17.001785\n",
      "[Epoch 35/10000] [Batch 11/36] [D loss: 0.041019, acc:  99%] [G loss: 5.851412] time: 0:51:19.678005\n",
      "[Epoch 35/10000] [Batch 12/36] [D loss: 0.049354, acc:  98%] [G loss: 5.662390] time: 0:51:22.329209\n",
      "[Epoch 35/10000] [Batch 13/36] [D loss: 0.021973, acc:  99%] [G loss: 7.003219] time: 0:51:25.105841\n",
      "[Epoch 35/10000] [Batch 14/36] [D loss: 0.046236, acc:  99%] [G loss: 5.809279] time: 0:51:27.581015\n",
      "[Epoch 35/10000] [Batch 15/36] [D loss: 0.031232, acc:  99%] [G loss: 5.714329] time: 0:51:30.007020\n",
      "[Epoch 35/10000] [Batch 16/36] [D loss: 0.060219, acc:  98%] [G loss: 5.795777] time: 0:51:32.379984\n",
      "[Epoch 35/10000] [Batch 17/36] [D loss: 0.042764, acc:  99%] [G loss: 5.575788] time: 0:51:34.771940\n",
      "[Epoch 35/10000] [Batch 18/36] [D loss: 0.037572, acc:  99%] [G loss: 5.742100] time: 0:51:37.094624\n",
      "[Epoch 35/10000] [Batch 19/36] [D loss: 0.033866, acc:  99%] [G loss: 6.652607] time: 0:51:39.419051\n",
      "[Epoch 35/10000] [Batch 20/36] [D loss: 0.035945, acc:  99%] [G loss: 5.409266] time: 0:51:41.869512\n",
      "[Epoch 35/10000] [Batch 21/36] [D loss: 0.077450, acc:  95%] [G loss: 5.219048] time: 0:51:44.431186\n",
      "[Epoch 35/10000] [Batch 22/36] [D loss: 0.033575, acc:  99%] [G loss: 5.846751] time: 0:51:47.153069\n",
      "[Epoch 35/10000] [Batch 23/36] [D loss: 0.022521, acc:  99%] [G loss: 5.573433] time: 0:51:49.777589\n",
      "[Epoch 35/10000] [Batch 24/36] [D loss: 0.045329, acc:  99%] [G loss: 6.383346] time: 0:51:52.312193\n",
      "[Epoch 35/10000] [Batch 25/36] [D loss: 0.041975, acc:  99%] [G loss: 6.085247] time: 0:51:54.780837\n",
      "[Epoch 35/10000] [Batch 26/36] [D loss: 0.079056, acc:  96%] [G loss: 5.277947] time: 0:51:57.219035\n",
      "[Epoch 35/10000] [Batch 27/36] [D loss: 0.074170, acc:  97%] [G loss: 5.464429] time: 0:51:59.727848\n",
      "[Epoch 35/10000] [Batch 28/36] [D loss: 0.054140, acc:  97%] [G loss: 5.859624] time: 0:52:02.179815\n",
      "[Epoch 35/10000] [Batch 29/36] [D loss: 0.020387, acc:  99%] [G loss: 6.046244] time: 0:52:04.685709\n",
      "[Epoch 35/10000] [Batch 30/36] [D loss: 0.027604, acc:  99%] [G loss: 5.597802] time: 0:52:07.123461\n",
      "[Epoch 35/10000] [Batch 31/36] [D loss: 0.079358, acc:  93%] [G loss: 5.707174] time: 0:52:09.575693\n",
      "[Epoch 35/10000] [Batch 32/36] [D loss: 0.032756, acc:  99%] [G loss: 5.768385] time: 0:52:12.141816\n",
      "[Epoch 35/10000] [Batch 33/36] [D loss: 0.096476, acc:  96%] [G loss: 5.262079] time: 0:52:14.859442\n",
      "[Epoch 35/10000] [Batch 34/36] [D loss: 0.025454, acc:  99%] [G loss: 6.605613] time: 0:52:17.927879\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-805c1c661593>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# also need more data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# and need to split into test and validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-466315673c46>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                 \u001b[1;31m# Train the generators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                 \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimgs_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs_B\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs_A\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1186\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Increase batch_size gradually\n",
    "# Start with batch of size 1\n",
    "# then 10 after like 500 epochs\n",
    "# then 100 after 500 more epochs\n",
    "# also need more data\n",
    "# and need to split into test and validation set\n",
    "gan.train(epochs=10000, batch_size=70, sample_interval=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: 'D:\\Users\\amm65\\Desktop\\color palette project\\testresized1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-dc2be77e19aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimageio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"testresized1.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m127.5\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimageio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generate.jpg\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\imageio\\core\\functions.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;31m# Get reader and read first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\imageio\\core\\functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[1;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;31m# Create request object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;31m# Get format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\imageio\\core\\request.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# Parse what was given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;31m# Set extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\imageio\\core\\request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[1;34m(self, uri)\u001b[0m\n\u001b[0;32m    276\u001b[0m                 \u001b[1;31m# Reading: check that the file exists (but is allowed a dir)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file: '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m                 \u001b[1;31m# Writing: check that the directory to write to does exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file: 'D:\\Users\\amm65\\Desktop\\color palette project\\testresized1.jpg'"
     ]
    }
   ],
   "source": [
    "img = np.array([imageio.imread(\"testresized1.jpg\").astype(np.float32)])/127.5 - 1.\n",
    "print(img)\n",
    "nu = gan.generator.predict(img)\n",
    "imageio.imwrite(\"generate.jpg\",nu[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpecGan",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
